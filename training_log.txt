nohup: ignoring input
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.94s/it]
/home/dev25-01/resume-parser-mistral/train_mistral_model.py:89: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/1868 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/dev25-01/mistral-env/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
  0%|          | 1/1868 [03:05<96:08:35, 185.39s/it]? Step 1  Epoch: 0.00  Loss: 1.4505
                                                      0%|          | 1/1868 [03:05<96:08:35, 185.39s/it]  0%|          | 2/1868 [06:12<96:28:04, 186.11s/it]{'loss': 1.4505, 'grad_norm': 0.0, 'learning_rate': 2e-05, 'epoch': 0.0}
? Step 2  Epoch: 0.00  Loss: 1.6336
                                                      0%|          | 2/1868 [06:12<96:28:04, 186.11s/it]  0%|          | 3/1868 [09:18<96:34:34, 186.42s/it]{'loss': 1.6336, 'grad_norm': 0.0, 'learning_rate': 1.998929336188437e-05, 'epoch': 0.0}
? Step 3  Epoch: 0.00  Loss: 1.5280
                                                      0%|          | 3/1868 [09:18<96:34:34, 186.42s/it]  0%|          | 4/1868 [12:25<96:34:02, 186.50s/it]{'loss': 1.528, 'grad_norm': 0.0, 'learning_rate': 1.997858672376874e-05, 'epoch': 0.0}
? Step 4  Epoch: 0.00  Loss: 1.6774
                                                      0%|          | 4/1868 [12:25<96:34:02, 186.50s/it]  0%|          | 5/1868 [15:32<96:33:06, 186.57s/it]{'loss': 1.6774, 'grad_norm': 0.0, 'learning_rate': 1.9967880085653108e-05, 'epoch': 0.0}
? Step 5  Epoch: 0.00  Loss: 1.4224
                                                      0%|          | 5/1868 [15:32<96:33:06, 186.57s/it]  0%|          | 6/1868 [18:38<96:27:35, 186.50s/it]{'loss': 1.4224, 'grad_norm': 0.0, 'learning_rate': 1.9957173447537473e-05, 'epoch': 0.0}
? Step 6  Epoch: 0.00  Loss: 2.0211
                                                      0%|          | 6/1868 [18:38<96:27:35, 186.50s/it]  0%|          | 7/1868 [21:45<96:26:02, 186.55s/it]{'loss': 2.0211, 'grad_norm': 0.0, 'learning_rate': 1.994646680942184e-05, 'epoch': 0.0}
? Step 7  Epoch: 0.00  Loss: 1.4537
                                                      0%|          | 7/1868 [21:45<96:26:02, 186.55s/it]  0%|          | 8/1868 [24:51<96:23:04, 186.55s/it]{'loss': 1.4537, 'grad_norm': 0.0, 'learning_rate': 1.993576017130621e-05, 'epoch': 0.0}
? Step 8  Epoch: 0.00  Loss: 1.3645
                                                      0%|          | 8/1868 [24:51<96:23:04, 186.55s/it]  0%|          | 9/1868 [27:57<96:16:16, 186.43s/it]{'loss': 1.3645, 'grad_norm': 0.0, 'learning_rate': 1.992505353319058e-05, 'epoch': 0.0}
? Step 9  Epoch: 0.00  Loss: 1.6942
                                                      0%|          | 9/1868 [27:57<96:16:16, 186.43s/it]  1%|          | 10/1868 [31:04<96:13:29, 186.44s/it]{'loss': 1.6942, 'grad_norm': 0.0, 'learning_rate': 1.9914346895074948e-05, 'epoch': 0.0}
? Step 10  Epoch: 0.01  Loss: 1.5919
                                                       1%|          | 10/1868 [31:04<96:13:29, 186.44s/it]  1%|          | 11/1868 [34:11<96:13:17, 186.54s/it]{'loss': 1.5919, 'grad_norm': 0.0, 'learning_rate': 1.9903640256959316e-05, 'epoch': 0.01}
? Step 11  Epoch: 0.01  Loss: 1.4134
                                                       1%|          | 11/1868 [34:11<96:13:17, 186.54s/it]  1%|          | 12/1868 [37:17<96:13:12, 186.63s/it]{'loss': 1.4134, 'grad_norm': 0.0, 'learning_rate': 1.9892933618843685e-05, 'epoch': 0.01}
? Step 12  Epoch: 0.01  Loss: 1.3787
                                                       1%|          | 12/1868 [37:17<96:13:12, 186.63s/it]  1%|          | 13/1868 [40:24<96:07:31, 186.55s/it]{'loss': 1.3787, 'grad_norm': 0.0, 'learning_rate': 1.9882226980728054e-05, 'epoch': 0.01}
? Step 13  Epoch: 0.01  Loss: 1.7001
                                                       1%|          | 13/1868 [40:24<96:07:31, 186.55s/it]  1%|          | 14/1868 [43:30<96:03:29, 186.52s/it]{'loss': 1.7001, 'grad_norm': 0.0, 'learning_rate': 1.987152034261242e-05, 'epoch': 0.01}
? Step 14  Epoch: 0.01  Loss: 1.5330
                                                       1%|          | 14/1868 [43:30<96:03:29, 186.52s/it]  1%|          | 15/1868 [46:37<96:00:01, 186.51s/it]{'loss': 1.533, 'grad_norm': 0.0, 'learning_rate': 1.9860813704496788e-05, 'epoch': 0.01}
? Step 15  Epoch: 0.01  Loss: 1.6334
                                                       1%|          | 15/1868 [46:37<96:00:01, 186.51s/it]  1%|          | 16/1868 [49:43<95:58:26, 186.56s/it]{'loss': 1.6334, 'grad_norm': 0.0, 'learning_rate': 1.985010706638116e-05, 'epoch': 0.01}
? Step 16  Epoch: 0.01  Loss: 1.5894
                                                       1%|          | 16/1868 [49:43<95:58:26, 186.56s/it]  1%|          | 17/1868 [52:50<95:54:12, 186.52s/it]{'loss': 1.5894, 'grad_norm': 0.0, 'learning_rate': 1.983940042826553e-05, 'epoch': 0.01}
? Step 17  Epoch: 0.01  Loss: 1.7353
                                                       1%|          | 17/1868 [52:50<95:54:12, 186.52s/it]  1%|          | 18/1868 [55:56<95:52:08, 186.56s/it]{'loss': 1.7353, 'grad_norm': 0.0, 'learning_rate': 1.9828693790149897e-05, 'epoch': 0.01}
? Step 18  Epoch: 0.01  Loss: 1.6836
                                                       1%|          | 18/1868 [55:56<95:52:08, 186.56s/it]  1%|          | 19/1868 [59:03<95:47:16, 186.50s/it]{'loss': 1.6836, 'grad_norm': 0.0, 'learning_rate': 1.9817987152034262e-05, 'epoch': 0.01}
? Step 19  Epoch: 0.01  Loss: 1.7192
                                                       1%|          | 19/1868 [59:03<95:47:16, 186.50s/it]  1%|          | 20/1868 [1:02:09<95:45:20, 186.54s/it]{'loss': 1.7192, 'grad_norm': 0.0, 'learning_rate': 1.980728051391863e-05, 'epoch': 0.01}
? Step 20  Epoch: 0.01  Loss: 1.4263
                                                         1%|          | 20/1868 [1:02:09<95:45:20, 186.54s/it]  1%|          | 21/1868 [1:05:16<95:43:24, 186.58s/it]{'loss': 1.4263, 'grad_norm': 0.0, 'learning_rate': 1.9796573875803e-05, 'epoch': 0.01}
? Step 21  Epoch: 0.01  Loss: 1.6408
                                                         1%|          | 21/1868 [1:05:16<95:43:24, 186.58s/it]  1%|          | 22/1868 [1:08:23<95:39:25, 186.55s/it]{'loss': 1.6408, 'grad_norm': 0.0, 'learning_rate': 1.9785867237687368e-05, 'epoch': 0.01}
? Step 22  Epoch: 0.01  Loss: 1.5708
                                                         1%|          | 22/1868 [1:08:23<95:39:25, 186.55s/it]  1%|          | 23/1868 [1:11:29<95:36:37, 186.56s/it]{'loss': 1.5708, 'grad_norm': 0.0, 'learning_rate': 1.9775160599571737e-05, 'epoch': 0.01}
? Step 23  Epoch: 0.01  Loss: 1.5017
                                                         1%|          | 23/1868 [1:11:29<95:36:37, 186.56s/it]  1%|▏         | 24/1868 [1:14:36<95:33:55, 186.57s/it]{'loss': 1.5017, 'grad_norm': 0.0, 'learning_rate': 1.9764453961456106e-05, 'epoch': 0.01}
? Step 24  Epoch: 0.01  Loss: 1.6632
                                                         1%|▏         | 24/1868 [1:14:36<95:33:55, 186.57s/it]  1%|▏         | 25/1868 [1:17:42<95:31:34, 186.59s/it]{'loss': 1.6632, 'grad_norm': 0.0, 'learning_rate': 1.9753747323340474e-05, 'epoch': 0.01}
? Step 25  Epoch: 0.01  Loss: 1.7259
                                                         1%|▏         | 25/1868 [1:17:42<95:31:34, 186.59s/it]  1%|▏         | 26/1868 [1:20:49<95:27:21, 186.56s/it]{'loss': 1.7259, 'grad_norm': 0.0, 'learning_rate': 1.9743040685224843e-05, 'epoch': 0.01}
? Step 26  Epoch: 0.01  Loss: 1.5460
                                                         1%|▏         | 26/1868 [1:20:49<95:27:21, 186.56s/it]  1%|▏         | 27/1868 [1:23:56<95:24:39, 186.57s/it]{'loss': 1.546, 'grad_norm': 0.0, 'learning_rate': 1.9732334047109208e-05, 'epoch': 0.01}
? Step 27  Epoch: 0.01  Loss: 1.9092
                                                         1%|▏         | 27/1868 [1:23:56<95:24:39, 186.57s/it]  1%|▏         | 28/1868 [1:27:02<95:20:59, 186.55s/it]{'loss': 1.9092, 'grad_norm': 0.0, 'learning_rate': 1.9721627408993577e-05, 'epoch': 0.01}
? Step 28  Epoch: 0.01  Loss: 1.6299
                                                         1%|▏         | 28/1868 [1:27:02<95:20:59, 186.55s/it]  2%|▏         | 29/1868 [1:30:09<95:18:02, 186.56s/it]{'loss': 1.6299, 'grad_norm': 0.0, 'learning_rate': 1.9710920770877946e-05, 'epoch': 0.01}
? Step 29  Epoch: 0.02  Loss: 1.5832
                                                         2%|▏         | 29/1868 [1:30:09<95:18:02, 186.56s/it]  2%|▏         | 30/1868 [1:33:15<95:11:12, 186.44s/it]{'loss': 1.5832, 'grad_norm': 0.0, 'learning_rate': 1.9700214132762314e-05, 'epoch': 0.02}
? Step 30  Epoch: 0.02  Loss: 2.5118
                                                         2%|▏         | 30/1868 [1:33:15<95:11:12, 186.44s/it]  2%|▏         | 31/1868 [1:36:21<95:09:58, 186.50s/it]{'loss': 2.5118, 'grad_norm': 0.0, 'learning_rate': 1.9689507494646683e-05, 'epoch': 0.02}
? Step 31  Epoch: 0.02  Loss: 1.3131
                                                         2%|▏         | 31/1868 [1:36:21<95:09:58, 186.50s/it]  2%|▏         | 32/1868 [1:39:28<95:07:24, 186.52s/it]{'loss': 1.3131, 'grad_norm': 0.0, 'learning_rate': 1.967880085653105e-05, 'epoch': 0.02}
? Step 32  Epoch: 0.02  Loss: 1.8795
                                                         2%|▏         | 32/1868 [1:39:28<95:07:24, 186.52s/it]  2%|▏         | 33/1868 [1:42:35<95:08:26, 186.65s/it]{'loss': 1.8795, 'grad_norm': 0.0, 'learning_rate': 1.966809421841542e-05, 'epoch': 0.02}
? Step 33  Epoch: 0.02  Loss: 1.5211
                                                         2%|▏         | 33/1868 [1:42:35<95:08:26, 186.65s/it]  2%|▏         | 34/1868 [1:45:42<95:05:13, 186.65s/it]{'loss': 1.5211, 'grad_norm': 0.0, 'learning_rate': 1.9657387580299786e-05, 'epoch': 0.02}
? Step 34  Epoch: 0.02  Loss: 1.9435
                                                         2%|▏         | 34/1868 [1:45:42<95:05:13, 186.65s/it]  2%|▏         | 35/1868 [1:48:49<95:05:12, 186.75s/it]{'loss': 1.9435, 'grad_norm': 0.0, 'learning_rate': 1.9646680942184154e-05, 'epoch': 0.02}
? Step 35  Epoch: 0.02  Loss: 1.4664
                                                         2%|▏         | 35/1868 [1:48:49<95:05:12, 186.75s/it]  2%|▏         | 36/1868 [1:51:55<95:03:02, 186.78s/it]{'loss': 1.4664, 'grad_norm': 0.0, 'learning_rate': 1.9635974304068523e-05, 'epoch': 0.02}
? Step 36  Epoch: 0.02  Loss: 1.4449
                                                         2%|▏         | 36/1868 [1:51:55<95:03:02, 186.78s/it]  2%|▏         | 37/1868 [1:55:02<95:00:18, 186.79s/it]{'loss': 1.4449, 'grad_norm': 0.0, 'learning_rate': 1.962526766595289e-05, 'epoch': 0.02}
? Step 37  Epoch: 0.02  Loss: 1.6231
                                                         2%|▏         | 37/1868 [1:55:02<95:00:18, 186.79s/it]  2%|▏         | 38/1868 [1:58:09<94:56:00, 186.75s/it]{'loss': 1.6231, 'grad_norm': 0.0, 'learning_rate': 1.961456102783726e-05, 'epoch': 0.02}
? Step 38  Epoch: 0.02  Loss: 1.8839
                                                         2%|▏         | 38/1868 [1:58:09<94:56:00, 186.75s/it]  2%|▏         | 39/1868 [2:01:15<94:51:20, 186.70s/it]{'loss': 1.8839, 'grad_norm': 0.0, 'learning_rate': 1.960385438972163e-05, 'epoch': 0.02}
? Step 39  Epoch: 0.02  Loss: 1.6247
                                                         2%|▏         | 39/1868 [2:01:15<94:51:20, 186.70s/it]  2%|▏         | 40/1868 [2:04:22<94:48:49, 186.72s/it]{'loss': 1.6247, 'grad_norm': 0.0, 'learning_rate': 1.9593147751605998e-05, 'epoch': 0.02}
? Step 40  Epoch: 0.02  Loss: 1.5348
                                                         2%|▏         | 40/1868 [2:04:22<94:48:49, 186.72s/it]  2%|▏         | 41/1868 [2:07:29<94:46:02, 186.73s/it]{'loss': 1.5348, 'grad_norm': 0.0, 'learning_rate': 1.9582441113490366e-05, 'epoch': 0.02}
? Step 41  Epoch: 0.02  Loss: 1.5886
                                                         2%|▏         | 41/1868 [2:07:29<94:46:02, 186.73s/it]  2%|▏         | 42/1868 [2:10:36<94:44:20, 186.78s/it]{'loss': 1.5886, 'grad_norm': 0.0, 'learning_rate': 1.957173447537473e-05, 'epoch': 0.02}
? Step 42  Epoch: 0.02  Loss: 1.4693
                                                         2%|▏         | 42/1868 [2:10:36<94:44:20, 186.78s/it]  2%|▏         | 43/1868 [2:13:43<94:42:38, 186.83s/it]{'loss': 1.4693, 'grad_norm': 0.0, 'learning_rate': 1.95610278372591e-05, 'epoch': 0.02}
? Step 43  Epoch: 0.02  Loss: 1.4118
                                                         2%|▏         | 43/1868 [2:13:43<94:42:38, 186.83s/it]  2%|▏         | 44/1868 [2:16:50<94:40:35, 186.86s/it]{'loss': 1.4118, 'grad_norm': 0.0, 'learning_rate': 1.9550321199143472e-05, 'epoch': 0.02}
? Step 44  Epoch: 0.02  Loss: 1.5425
                                                         2%|▏         | 44/1868 [2:16:50<94:40:35, 186.86s/it]  2%|▏         | 45/1868 [2:19:56<94:33:55, 186.74s/it]{'loss': 1.5425, 'grad_norm': 0.0, 'learning_rate': 1.953961456102784e-05, 'epoch': 0.02}
? Step 45  Epoch: 0.02  Loss: 1.7314
                                                         2%|▏         | 45/1868 [2:19:56<94:33:55, 186.74s/it]  2%|▏         | 46/1868 [2:23:03<94:29:51, 186.71s/it]{'loss': 1.7314, 'grad_norm': 0.0, 'learning_rate': 1.952890792291221e-05, 'epoch': 0.02}
? Step 46  Epoch: 0.02  Loss: 1.5379
                                                         2%|▏         | 46/1868 [2:23:03<94:29:51, 186.71s/it]  3%|▎         | 47/1868 [2:26:10<94:27:53, 186.75s/it]{'loss': 1.5379, 'grad_norm': 0.0, 'learning_rate': 1.9518201284796575e-05, 'epoch': 0.02}
? Step 47  Epoch: 0.03  Loss: 1.5782
                                                         3%|▎         | 47/1868 [2:26:10<94:27:53, 186.75s/it]  3%|▎         | 48/1868 [2:29:16<94:22:20, 186.67s/it]{'loss': 1.5782, 'grad_norm': 0.0, 'learning_rate': 1.9507494646680944e-05, 'epoch': 0.03}
? Step 48  Epoch: 0.03  Loss: 1.4503
                                                         3%|▎         | 48/1868 [2:29:16<94:22:20, 186.67s/it]  3%|▎         | 49/1868 [2:32:23<94:17:07, 186.60s/it]{'loss': 1.4503, 'grad_norm': 0.0, 'learning_rate': 1.9496788008565312e-05, 'epoch': 0.03}
? Step 49  Epoch: 0.03  Loss: 1.4845
                                                         3%|▎         | 49/1868 [2:32:23<94:17:07, 186.60s/it]  3%|▎         | 50/1868 [2:35:30<94:17:07, 186.70s/it]{'loss': 1.4845, 'grad_norm': 0.0, 'learning_rate': 1.948608137044968e-05, 'epoch': 0.03}
? Step 50  Epoch: 0.03  Loss: 1.4529
                                                         3%|▎         | 50/1868 [2:35:30<94:17:07, 186.70s/it]  3%|▎         | 51/1868 [2:38:36<94:13:47, 186.70s/it]{'loss': 1.4529, 'grad_norm': 0.0, 'learning_rate': 1.947537473233405e-05, 'epoch': 0.03}
? Step 51  Epoch: 0.03  Loss: 1.5740
                                                         3%|▎         | 51/1868 [2:38:36<94:13:47, 186.70s/it]  3%|▎         | 52/1868 [2:41:43<94:10:14, 186.68s/it]{'loss': 1.574, 'grad_norm': 0.0, 'learning_rate': 1.9464668094218418e-05, 'epoch': 0.03}
? Step 52  Epoch: 0.03  Loss: 1.4097
                                                         3%|▎         | 52/1868 [2:41:43<94:10:14, 186.68s/it]  3%|▎         | 53/1868 [2:44:50<94:07:01, 186.68s/it]{'loss': 1.4097, 'grad_norm': 0.0, 'learning_rate': 1.9453961456102787e-05, 'epoch': 0.03}
? Step 53  Epoch: 0.03  Loss: 1.4748
                                                         3%|▎         | 53/1868 [2:44:50<94:07:01, 186.68s/it]  3%|▎         | 54/1868 [2:47:56<94:04:04, 186.68s/it]{'loss': 1.4748, 'grad_norm': 0.0, 'learning_rate': 1.9443254817987152e-05, 'epoch': 0.03}
? Step 54  Epoch: 0.03  Loss: 1.6856
                                                         3%|▎         | 54/1868 [2:47:56<94:04:04, 186.68s/it]  3%|▎         | 55/1868 [2:51:03<94:01:26, 186.70s/it]{'loss': 1.6856, 'grad_norm': 0.0, 'learning_rate': 1.943254817987152e-05, 'epoch': 0.03}
? Step 55  Epoch: 0.03  Loss: 1.5378
                                                         3%|▎         | 55/1868 [2:51:03<94:01:26, 186.70s/it]  3%|▎         | 56/1868 [2:54:10<93:58:12, 186.70s/it]{'loss': 1.5378, 'grad_norm': 0.0, 'learning_rate': 1.942184154175589e-05, 'epoch': 0.03}
? Step 56  Epoch: 0.03  Loss: 1.6205
                                                         3%|▎         | 56/1868 [2:54:10<93:58:12, 186.70s/it]  3%|▎         | 57/1868 [2:57:16<93:54:59, 186.69s/it]{'loss': 1.6205, 'grad_norm': 0.0, 'learning_rate': 1.9411134903640258e-05, 'epoch': 0.03}
? Step 57  Epoch: 0.03  Loss: 1.4556
                                                         3%|▎         | 57/1868 [2:57:16<93:54:59, 186.69s/it]  3%|▎         | 58/1868 [3:00:23<93:50:25, 186.64s/it]{'loss': 1.4556, 'grad_norm': 0.0, 'learning_rate': 1.9400428265524627e-05, 'epoch': 0.03}
? Step 58  Epoch: 0.03  Loss: 1.8696
                                                         3%|▎         | 58/1868 [3:00:23<93:50:25, 186.64s/it]  3%|▎         | 59/1868 [3:03:29<93:46:50, 186.63s/it]{'loss': 1.8696, 'grad_norm': 0.0, 'learning_rate': 1.9389721627408996e-05, 'epoch': 0.03}
? Step 59  Epoch: 0.03  Loss: 1.3533
                                                         3%|▎         | 59/1868 [3:03:29<93:46:50, 186.63s/it]  3%|▎         | 60/1868 [3:06:36<93:45:15, 186.68s/it]{'loss': 1.3533, 'grad_norm': 0.0, 'learning_rate': 1.9379014989293364e-05, 'epoch': 0.03}
? Step 60  Epoch: 0.03  Loss: 1.4627
                                                         3%|▎         | 60/1868 [3:06:36<93:45:15, 186.68s/it]  3%|▎         | 61/1868 [3:09:43<93:40:37, 186.63s/it]{'loss': 1.4627, 'grad_norm': 0.0, 'learning_rate': 1.9368308351177733e-05, 'epoch': 0.03}
? Step 61  Epoch: 0.03  Loss: 1.3942
                                                         3%|▎         | 61/1868 [3:09:43<93:40:37, 186.63s/it]  3%|▎         | 62/1868 [3:12:50<93:39:07, 186.68s/it]{'loss': 1.3942, 'grad_norm': 0.0, 'learning_rate': 1.9357601713062098e-05, 'epoch': 0.03}
? Step 62  Epoch: 0.03  Loss: 1.4912
                                                         3%|▎         | 62/1868 [3:12:50<93:39:07, 186.68s/it]  3%|▎         | 63/1868 [3:15:56<93:34:23, 186.63s/it]{'loss': 1.4912, 'grad_norm': 0.0, 'learning_rate': 1.9346895074946467e-05, 'epoch': 0.03}
? Step 63  Epoch: 0.03  Loss: 1.4372
                                                         3%|▎         | 63/1868 [3:15:56<93:34:23, 186.63s/it]  3%|▎         | 64/1868 [3:19:02<93:28:43, 186.54s/it]{'loss': 1.4372, 'grad_norm': 0.0, 'learning_rate': 1.9336188436830836e-05, 'epoch': 0.03}
? Step 64  Epoch: 0.03  Loss: 2.0181
                                                         3%|▎         | 64/1868 [3:19:02<93:28:43, 186.54s/it]  3%|▎         | 65/1868 [3:22:09<93:26:53, 186.59s/it]{'loss': 2.0181, 'grad_norm': 0.0, 'learning_rate': 1.9325481798715204e-05, 'epoch': 0.03}
? Step 65  Epoch: 0.03  Loss: 1.5480
                                                         3%|▎         | 65/1868 [3:22:09<93:26:53, 186.59s/it]  4%|▎         | 66/1868 [3:25:16<93:23:45, 186.58s/it]{'loss': 1.548, 'grad_norm': 0.0, 'learning_rate': 1.9314775160599573e-05, 'epoch': 0.03}
? Step 66  Epoch: 0.04  Loss: 1.8410
                                                         4%|▎         | 66/1868 [3:25:16<93:23:45, 186.58s/it]  4%|▎         | 67/1868 [3:28:22<93:21:02, 186.60s/it]{'loss': 1.841, 'grad_norm': 0.0, 'learning_rate': 1.930406852248394e-05, 'epoch': 0.04}
? Step 67  Epoch: 0.04  Loss: 1.5558
                                                         4%|▎         | 67/1868 [3:28:22<93:21:02, 186.60s/it]  4%|▎         | 68/1868 [3:31:29<93:17:14, 186.57s/it]{'loss': 1.5558, 'grad_norm': 0.0, 'learning_rate': 1.929336188436831e-05, 'epoch': 0.04}
? Step 68  Epoch: 0.04  Loss: 1.5069
                                                         4%|▎         | 68/1868 [3:31:29<93:17:14, 186.57s/it]  4%|▎         | 69/1868 [3:34:36<93:17:11, 186.68s/it]{'loss': 1.5069, 'grad_norm': 0.0, 'learning_rate': 1.928265524625268e-05, 'epoch': 0.04}
? Step 69  Epoch: 0.04  Loss: 1.5094
                                                         4%|▎         | 69/1868 [3:34:36<93:17:11, 186.68s/it]  4%|▎         | 70/1868 [3:37:42<93:13:20, 186.65s/it]{'loss': 1.5094, 'grad_norm': 0.0, 'learning_rate': 1.9271948608137044e-05, 'epoch': 0.04}
? Step 70  Epoch: 0.04  Loss: 1.9591
                                                         4%|▎         | 70/1868 [3:37:42<93:13:20, 186.65s/it]  4%|▍         | 71/1868 [3:40:49<93:10:31, 186.66s/it]{'loss': 1.9591, 'grad_norm': 0.0, 'learning_rate': 1.9261241970021416e-05, 'epoch': 0.04}
? Step 71  Epoch: 0.04  Loss: 1.5507
                                                         4%|▍         | 71/1868 [3:40:49<93:10:31, 186.66s/it]  4%|▍         | 72/1868 [3:43:56<93:07:25, 186.66s/it]{'loss': 1.5507, 'grad_norm': 0.0, 'learning_rate': 1.9250535331905785e-05, 'epoch': 0.04}
? Step 72  Epoch: 0.04  Loss: 1.6392
                                                         4%|▍         | 72/1868 [3:43:56<93:07:25, 186.66s/it]  4%|▍         | 73/1868 [3:47:03<93:06:06, 186.72s/it]{'loss': 1.6392, 'grad_norm': 0.0, 'learning_rate': 1.9239828693790154e-05, 'epoch': 0.04}
? Step 73  Epoch: 0.04  Loss: 1.5803
                                                         4%|▍         | 73/1868 [3:47:03<93:06:06, 186.72s/it]  4%|▍         | 74/1868 [3:50:09<93:01:08, 186.66s/it]{'loss': 1.5803, 'grad_norm': 0.0, 'learning_rate': 1.9229122055674522e-05, 'epoch': 0.04}
? Step 74  Epoch: 0.04  Loss: 1.8303
                                                         4%|▍         | 74/1868 [3:50:09<93:01:08, 186.66s/it]  4%|▍         | 75/1868 [3:53:16<92:56:38, 186.61s/it]{'loss': 1.8303, 'grad_norm': 0.0, 'learning_rate': 1.9218415417558888e-05, 'epoch': 0.04}
? Step 75  Epoch: 0.04  Loss: 1.7447
                                                         4%|▍         | 75/1868 [3:53:16<92:56:38, 186.61s/it]  4%|▍         | 76/1868 [3:56:22<92:55:24, 186.68s/it]{'loss': 1.7447, 'grad_norm': 0.0, 'learning_rate': 1.9207708779443256e-05, 'epoch': 0.04}
? Step 76  Epoch: 0.04  Loss: 1.4138
                                                         4%|▍         | 76/1868 [3:56:22<92:55:24, 186.68s/it]  4%|▍         | 77/1868 [3:59:29<92:52:09, 186.67s/it]{'loss': 1.4138, 'grad_norm': 0.0, 'learning_rate': 1.9197002141327625e-05, 'epoch': 0.04}
? Step 77  Epoch: 0.04  Loss: 1.4472
                                                         4%|▍         | 77/1868 [3:59:29<92:52:09, 186.67s/it]  4%|▍         | 78/1868 [4:02:36<92:50:06, 186.71s/it]{'loss': 1.4472, 'grad_norm': 0.0, 'learning_rate': 1.9186295503211994e-05, 'epoch': 0.04}
? Step 78  Epoch: 0.04  Loss: 1.7130
                                                         4%|▍         | 78/1868 [4:02:36<92:50:06, 186.71s/it]  4%|▍         | 79/1868 [4:05:42<92:46:04, 186.68s/it]{'loss': 1.713, 'grad_norm': 0.0, 'learning_rate': 1.9175588865096362e-05, 'epoch': 0.04}
? Step 79  Epoch: 0.04  Loss: 1.9824
                                                         4%|▍         | 79/1868 [4:05:42<92:46:04, 186.68s/it]  4%|▍         | 80/1868 [4:08:49<92:44:12, 186.72s/it]{'loss': 1.9824, 'grad_norm': 0.0, 'learning_rate': 1.916488222698073e-05, 'epoch': 0.04}
? Step 80  Epoch: 0.04  Loss: 1.4192
                                                         4%|▍         | 80/1868 [4:08:49<92:44:12, 186.72s/it]  4%|▍         | 81/1868 [4:11:56<92:44:40, 186.84s/it]{'loss': 1.4192, 'grad_norm': 0.0, 'learning_rate': 1.91541755888651e-05, 'epoch': 0.04}
? Step 81  Epoch: 0.04  Loss: 1.5134
                                                         4%|▍         | 81/1868 [4:11:56<92:44:40, 186.84s/it]  4%|▍         | 82/1868 [4:15:03<92:42:21, 186.87s/it]{'loss': 1.5134, 'grad_norm': 0.0, 'learning_rate': 1.9143468950749465e-05, 'epoch': 0.04}
? Step 82  Epoch: 0.04  Loss: 1.4574
                                                         4%|▍         | 82/1868 [4:15:03<92:42:21, 186.87s/it]  4%|▍         | 83/1868 [4:18:10<92:36:56, 186.79s/it]{'loss': 1.4574, 'grad_norm': 0.0, 'learning_rate': 1.9132762312633834e-05, 'epoch': 0.04}
? Step 83  Epoch: 0.04  Loss: 1.4824
                                                         4%|▍         | 83/1868 [4:18:10<92:36:56, 186.79s/it]  4%|▍         | 84/1868 [4:21:17<92:34:00, 186.79s/it]{'loss': 1.4824, 'grad_norm': 0.0, 'learning_rate': 1.9122055674518202e-05, 'epoch': 0.04}
? Step 84  Epoch: 0.04  Loss: 1.4062
                                                         4%|▍         | 84/1868 [4:21:17<92:34:00, 186.79s/it]  5%|▍         | 85/1868 [4:24:23<92:26:19, 186.64s/it]{'loss': 1.4062, 'grad_norm': 0.0, 'learning_rate': 1.911134903640257e-05, 'epoch': 0.04}
? Step 85  Epoch: 0.05  Loss: 1.8261
                                                         5%|▍         | 85/1868 [4:24:23<92:26:19, 186.64s/it]  5%|▍         | 86/1868 [4:27:30<92:22:21, 186.61s/it]{'loss': 1.8261, 'grad_norm': 0.0, 'learning_rate': 1.910064239828694e-05, 'epoch': 0.05}
? Step 86  Epoch: 0.05  Loss: 1.5364
                                                         5%|▍         | 86/1868 [4:27:30<92:22:21, 186.61s/it]  5%|▍         | 87/1868 [4:30:36<92:18:24, 186.58s/it]{'loss': 1.5364, 'grad_norm': 0.0, 'learning_rate': 1.9089935760171308e-05, 'epoch': 0.05}
? Step 87  Epoch: 0.05  Loss: 1.7854
                                                         5%|▍         | 87/1868 [4:30:36<92:18:24, 186.58s/it]  5%|▍         | 88/1868 [4:33:42<92:12:56, 186.50s/it]{'loss': 1.7854, 'grad_norm': 0.0, 'learning_rate': 1.9079229122055677e-05, 'epoch': 0.05}
? Step 88  Epoch: 0.05  Loss: 1.8661
                                                         5%|▍         | 88/1868 [4:33:42<92:12:56, 186.50s/it]  5%|▍         | 89/1868 [4:36:49<92:09:20, 186.49s/it]{'loss': 1.8661, 'grad_norm': 0.0, 'learning_rate': 1.9068522483940046e-05, 'epoch': 0.05}
? Step 89  Epoch: 0.05  Loss: 1.8321
                                                         5%|▍         | 89/1868 [4:36:49<92:09:20, 186.49s/it]  5%|▍         | 90/1868 [4:39:56<92:08:09, 186.55s/it]{'loss': 1.8321, 'grad_norm': 0.0, 'learning_rate': 1.905781584582441e-05, 'epoch': 0.05}
? Step 90  Epoch: 0.05  Loss: 1.4096
                                                         5%|▍         | 90/1868 [4:39:56<92:08:09, 186.55s/it]  5%|▍         | 91/1868 [4:43:02<92:04:13, 186.52s/it]{'loss': 1.4096, 'grad_norm': 0.0, 'learning_rate': 1.904710920770878e-05, 'epoch': 0.05}
? Step 91  Epoch: 0.05  Loss: 1.6048
                                                         5%|▍         | 91/1868 [4:43:02<92:04:13, 186.52s/it]  5%|▍         | 92/1868 [4:46:09<92:01:03, 186.52s/it]{'loss': 1.6048, 'grad_norm': 0.0, 'learning_rate': 1.9036402569593148e-05, 'epoch': 0.05}
? Step 92  Epoch: 0.05  Loss: 1.7153
                                                         5%|▍         | 92/1868 [4:46:09<92:01:03, 186.52s/it]  5%|▍         | 93/1868 [4:49:15<91:57:02, 186.49s/it]{'loss': 1.7153, 'grad_norm': 0.0, 'learning_rate': 1.9025695931477517e-05, 'epoch': 0.05}
? Step 93  Epoch: 0.05  Loss: 1.6852
                                                         5%|▍         | 93/1868 [4:49:15<91:57:02, 186.49s/it]  5%|▌         | 94/1868 [4:52:21<91:54:01, 186.49s/it]{'loss': 1.6852, 'grad_norm': 0.0, 'learning_rate': 1.9014989293361886e-05, 'epoch': 0.05}
? Step 94  Epoch: 0.05  Loss: 1.4767
                                                         5%|▌         | 94/1868 [4:52:21<91:54:01, 186.49s/it]  5%|▌         | 95/1868 [4:55:28<91:49:20, 186.44s/it]{'loss': 1.4767, 'grad_norm': 0.0, 'learning_rate': 1.9004282655246254e-05, 'epoch': 0.05}
? Step 95  Epoch: 0.05  Loss: 1.6450
                                                         5%|▌         | 95/1868 [4:55:28<91:49:20, 186.44s/it]  5%|▌         | 96/1868 [4:58:35<91:50:04, 186.57s/it]{'loss': 1.645, 'grad_norm': 0.0, 'learning_rate': 1.8993576017130623e-05, 'epoch': 0.05}
? Step 96  Epoch: 0.05  Loss: 1.7718
                                                         5%|▌         | 96/1868 [4:58:35<91:50:04, 186.57s/it]  5%|▌         | 97/1868 [5:01:41<91:49:02, 186.64s/it]{'loss': 1.7718, 'grad_norm': 0.0, 'learning_rate': 1.8982869379014988e-05, 'epoch': 0.05}
? Step 97  Epoch: 0.05  Loss: 1.4263
                                                         5%|▌         | 97/1868 [5:01:41<91:49:02, 186.64s/it]  5%|▌         | 98/1868 [5:04:48<91:43:36, 186.56s/it]{'loss': 1.4263, 'grad_norm': 0.0, 'learning_rate': 1.8972162740899357e-05, 'epoch': 0.05}
? Step 98  Epoch: 0.05  Loss: 1.5589
                                                         5%|▌         | 98/1868 [5:04:48<91:43:36, 186.56s/it]  5%|▌         | 99/1868 [5:07:54<91:40:30, 186.56s/it]{'loss': 1.5589, 'grad_norm': 0.0, 'learning_rate': 1.896145610278373e-05, 'epoch': 0.05}
? Step 99  Epoch: 0.05  Loss: 1.5204
                                                         5%|▌         | 99/1868 [5:07:54<91:40:30, 186.56s/it]  5%|▌         | 100/1868 [5:11:01<91:36:42, 186.54s/it]{'loss': 1.5204, 'grad_norm': 0.0, 'learning_rate': 1.8950749464668098e-05, 'epoch': 0.05}
? Step 100  Epoch: 0.05  Loss: 1.5505
                                                          5%|▌         | 100/1868 [5:11:01<91:36:42, 186.54s/it]/home/dev25-01/mistral-env/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
  5%|▌         | 101/1868 [5:14:08<91:37:16, 186.66s/it]{'loss': 1.5505, 'grad_norm': 0.0, 'learning_rate': 1.8940042826552466e-05, 'epoch': 0.05}
? Step 101  Epoch: 0.05  Loss: 1.6524
                                                          5%|▌         | 101/1868 [5:14:08<91:37:16, 186.66s/it]  5%|▌         | 102/1868 [5:17:15<91:34:59, 186.69s/it]{'loss': 1.6524, 'grad_norm': 0.0, 'learning_rate': 1.892933618843683e-05, 'epoch': 0.05}
? Step 102  Epoch: 0.05  Loss: 1.5252
                                                          5%|▌         | 102/1868 [5:17:15<91:34:59, 186.69s/it]  6%|▌         | 103/1868 [5:20:21<91:32:41, 186.72s/it]{'loss': 1.5252, 'grad_norm': 0.0, 'learning_rate': 1.89186295503212e-05, 'epoch': 0.05}
? Step 103  Epoch: 0.06  Loss: 1.6086
                                                          6%|▌         | 103/1868 [5:20:21<91:32:41, 186.72s/it]  6%|▌         | 104/1868 [5:23:28<91:27:00, 186.63s/it]{'loss': 1.6086, 'grad_norm': 0.0, 'learning_rate': 1.890792291220557e-05, 'epoch': 0.06}
? Step 104  Epoch: 0.06  Loss: 1.6290
                                                          6%|▌         | 104/1868 [5:23:28<91:27:00, 186.63s/it]  6%|▌         | 105/1868 [5:26:34<91:22:03, 186.57s/it]{'loss': 1.629, 'grad_norm': 0.0, 'learning_rate': 1.8897216274089938e-05, 'epoch': 0.06}
? Step 105  Epoch: 0.06  Loss: 1.9452
                                                          6%|▌         | 105/1868 [5:26:34<91:22:03, 186.57s/it]  6%|▌         | 106/1868 [5:29:41<91:20:00, 186.61s/it]{'loss': 1.9452, 'grad_norm': 0.0, 'learning_rate': 1.8886509635974306e-05, 'epoch': 0.06}
? Step 106  Epoch: 0.06  Loss: 1.4602
                                                          6%|▌         | 106/1868 [5:29:41<91:20:00, 186.61s/it]  6%|▌         | 107/1868 [5:32:47<91:16:15, 186.58s/it]{'loss': 1.4602, 'grad_norm': 0.0, 'learning_rate': 1.8875802997858675e-05, 'epoch': 0.06}
? Step 107  Epoch: 0.06  Loss: 1.5507
                                                          6%|▌         | 107/1868 [5:32:47<91:16:15, 186.58s/it]  6%|▌         | 108/1868 [5:35:54<91:14:34, 186.63s/it]{'loss': 1.5507, 'grad_norm': 0.0, 'learning_rate': 1.8865096359743044e-05, 'epoch': 0.06}
? Step 108  Epoch: 0.06  Loss: 1.5703
                                                          6%|▌         | 108/1868 [5:35:54<91:14:34, 186.63s/it]  6%|▌         | 109/1868 [5:39:01<91:12:50, 186.68s/it]{'loss': 1.5703, 'grad_norm': 0.0, 'learning_rate': 1.8854389721627412e-05, 'epoch': 0.06}
? Step 109  Epoch: 0.06  Loss: 1.5880
                                                          6%|▌         | 109/1868 [5:39:01<91:12:50, 186.68s/it]  6%|▌         | 110/1868 [5:42:08<91:11:33, 186.74s/it]{'loss': 1.588, 'grad_norm': 0.0, 'learning_rate': 1.8843683083511778e-05, 'epoch': 0.06}
? Step 110  Epoch: 0.06  Loss: 1.4668
                                                          6%|▌         | 110/1868 [5:42:08<91:11:33, 186.74s/it]  6%|▌         | 111/1868 [5:45:15<91:08:08, 186.73s/it]{'loss': 1.4668, 'grad_norm': 0.0, 'learning_rate': 1.8832976445396146e-05, 'epoch': 0.06}
? Step 111  Epoch: 0.06  Loss: 1.6350
                                                          6%|▌         | 111/1868 [5:45:15<91:08:08, 186.73s/it]  6%|▌         | 112/1868 [5:48:21<91:05:57, 186.76s/it]{'loss': 1.635, 'grad_norm': 0.0, 'learning_rate': 1.8822269807280515e-05, 'epoch': 0.06}
? Step 112  Epoch: 0.06  Loss: 1.5208
                                                          6%|▌         | 112/1868 [5:48:21<91:05:57, 186.76s/it]  6%|▌         | 113/1868 [5:51:28<91:04:03, 186.81s/it]{'loss': 1.5208, 'grad_norm': 0.0, 'learning_rate': 1.8811563169164884e-05, 'epoch': 0.06}
? Step 113  Epoch: 0.06  Loss: 1.6780
                                                          6%|▌         | 113/1868 [5:51:28<91:04:03, 186.81s/it]  6%|▌         | 114/1868 [5:54:35<91:01:24, 186.82s/it]{'loss': 1.678, 'grad_norm': 0.0, 'learning_rate': 1.8800856531049252e-05, 'epoch': 0.06}
? Step 114  Epoch: 0.06  Loss: 1.4388
                                                          6%|▌         | 114/1868 [5:54:35<91:01:24, 186.82s/it]  6%|▌         | 115/1868 [5:57:42<90:56:46, 186.77s/it]{'loss': 1.4388, 'grad_norm': 0.0, 'learning_rate': 1.879014989293362e-05, 'epoch': 0.06}
? Step 115  Epoch: 0.06  Loss: 1.5295
                                                          6%|▌         | 115/1868 [5:57:42<90:56:46, 186.77s/it]  6%|▌         | 116/1868 [6:00:49<90:53:28, 186.76s/it]{'loss': 1.5295, 'grad_norm': 0.0, 'learning_rate': 1.877944325481799e-05, 'epoch': 0.06}
? Step 116  Epoch: 0.06  Loss: 1.4786
                                                          6%|▌         | 116/1868 [6:00:49<90:53:28, 186.76s/it]  6%|▋         | 117/1868 [6:03:55<90:47:02, 186.65s/it]{'loss': 1.4786, 'grad_norm': 0.0, 'learning_rate': 1.8768736616702358e-05, 'epoch': 0.06}
? Step 117  Epoch: 0.06  Loss: 1.7381
                                                          6%|▋         | 117/1868 [6:03:55<90:47:02, 186.65s/it]  6%|▋         | 118/1868 [6:07:02<90:43:57, 186.65s/it]{'loss': 1.7381, 'grad_norm': 0.0, 'learning_rate': 1.8758029978586724e-05, 'epoch': 0.06}
? Step 118  Epoch: 0.06  Loss: 1.5178
                                                          6%|▋         | 118/1868 [6:07:02<90:43:57, 186.65s/it]  6%|▋         | 119/1868 [6:10:08<90:38:33, 186.57s/it]{'loss': 1.5178, 'grad_norm': 0.0, 'learning_rate': 1.8747323340471092e-05, 'epoch': 0.06}
? Step 119  Epoch: 0.06  Loss: 2.0646
                                                          6%|▋         | 119/1868 [6:10:08<90:38:33, 186.57s/it]  6%|▋         | 120/1868 [6:13:15<90:37:38, 186.65s/it]{'loss': 2.0646, 'grad_norm': 0.0, 'learning_rate': 1.873661670235546e-05, 'epoch': 0.06}
? Step 120  Epoch: 0.06  Loss: 1.4792
                                                          6%|▋         | 120/1868 [6:13:15<90:37:38, 186.65s/it]  6%|▋         | 121/1868 [6:16:21<90:34:00, 186.63s/it]{'loss': 1.4792, 'grad_norm': 0.0, 'learning_rate': 1.872591006423983e-05, 'epoch': 0.06}
? Step 121  Epoch: 0.06  Loss: 1.5901
                                                          6%|▋         | 121/1868 [6:16:21<90:34:00, 186.63s/it]  7%|▋         | 122/1868 [6:19:28<90:31:30, 186.65s/it]{'loss': 1.5901, 'grad_norm': 0.0, 'learning_rate': 1.8715203426124198e-05, 'epoch': 0.06}
? Step 122  Epoch: 0.07  Loss: 1.7953
                                                          7%|▋         | 122/1868 [6:19:28<90:31:30, 186.65s/it]  7%|▋         | 123/1868 [6:22:35<90:29:08, 186.68s/it]{'loss': 1.7953, 'grad_norm': 0.0, 'learning_rate': 1.8704496788008567e-05, 'epoch': 0.07}
? Step 123  Epoch: 0.07  Loss: 1.6723
                                                          7%|▋         | 123/1868 [6:22:35<90:29:08, 186.68s/it]  7%|▋         | 124/1868 [6:25:41<90:24:10, 186.61s/it]{'loss': 1.6723, 'grad_norm': 0.0, 'learning_rate': 1.8693790149892936e-05, 'epoch': 0.07}
? Step 124  Epoch: 0.07  Loss: 1.6489
                                                          7%|▋         | 124/1868 [6:25:41<90:24:10, 186.61s/it]  7%|▋         | 125/1868 [6:28:48<90:20:22, 186.59s/it]{'loss': 1.6489, 'grad_norm': 0.0, 'learning_rate': 1.86830835117773e-05, 'epoch': 0.07}
? Step 125  Epoch: 0.07  Loss: 1.7243
                                                          7%|▋         | 125/1868 [6:28:48<90:20:22, 186.59s/it]  7%|▋         | 126/1868 [6:31:54<90:16:50, 186.57s/it]{'loss': 1.7243, 'grad_norm': 0.0, 'learning_rate': 1.8672376873661673e-05, 'epoch': 0.07}
? Step 126  Epoch: 0.07  Loss: 1.4683
                                                          7%|▋         | 126/1868 [6:31:54<90:16:50, 186.57s/it]  7%|▋         | 127/1868 [6:35:01<90:14:46, 186.61s/it]{'loss': 1.4683, 'grad_norm': 0.0, 'learning_rate': 1.866167023554604e-05, 'epoch': 0.07}
? Step 127  Epoch: 0.07  Loss: 1.4586
                                                          7%|▋         | 127/1868 [6:35:01<90:14:46, 186.61s/it]  7%|▋         | 128/1868 [6:38:08<90:12:38, 186.64s/it]{'loss': 1.4586, 'grad_norm': 0.0, 'learning_rate': 1.865096359743041e-05, 'epoch': 0.07}
? Step 128  Epoch: 0.07  Loss: 1.5185
                                                          7%|▋         | 128/1868 [6:38:08<90:12:38, 186.64s/it]  7%|▋         | 129/1868 [6:41:14<90:07:05, 186.56s/it]{'loss': 1.5185, 'grad_norm': 0.0, 'learning_rate': 1.864025695931478e-05, 'epoch': 0.07}
? Step 129  Epoch: 0.07  Loss: 1.8419
                                                          7%|▋         | 129/1868 [6:41:14<90:07:05, 186.56s/it]  7%|▋         | 130/1868 [6:44:21<90:02:49, 186.52s/it]{'loss': 1.8419, 'grad_norm': 0.0, 'learning_rate': 1.8629550321199144e-05, 'epoch': 0.07}
? Step 130  Epoch: 0.07  Loss: 1.5832
                                                          7%|▋         | 130/1868 [6:44:21<90:02:49, 186.52s/it]  7%|▋         | 131/1868 [6:47:27<89:59:19, 186.51s/it]{'loss': 1.5832, 'grad_norm': 0.0, 'learning_rate': 1.8618843683083513e-05, 'epoch': 0.07}
? Step 131  Epoch: 0.07  Loss: 1.7501
                                                          7%|▋         | 131/1868 [6:47:27<89:59:19, 186.51s/it]  7%|▋         | 132/1868 [6:50:34<89:56:26, 186.51s/it]{'loss': 1.7501, 'grad_norm': 0.0, 'learning_rate': 1.860813704496788e-05, 'epoch': 0.07}
? Step 132  Epoch: 0.07  Loss: 1.4240
                                                          7%|▋         | 132/1868 [6:50:34<89:56:26, 186.51s/it]  7%|▋         | 133/1868 [6:53:40<89:50:28, 186.41s/it]{'loss': 1.424, 'grad_norm': 0.0, 'learning_rate': 1.859743040685225e-05, 'epoch': 0.07}
? Step 133  Epoch: 0.07  Loss: 1.5724
                                                          7%|▋         | 133/1868 [6:53:40<89:50:28, 186.41s/it]  7%|▋         | 134/1868 [6:56:46<89:48:47, 186.46s/it]{'loss': 1.5724, 'grad_norm': 0.0, 'learning_rate': 1.858672376873662e-05, 'epoch': 0.07}
? Step 134  Epoch: 0.07  Loss: 1.6076
                                                          7%|▋         | 134/1868 [6:56:46<89:48:47, 186.46s/it]  7%|▋         | 135/1868 [6:59:53<89:47:24, 186.52s/it]{'loss': 1.6076, 'grad_norm': 0.0, 'learning_rate': 1.8576017130620988e-05, 'epoch': 0.07}
? Step 135  Epoch: 0.07  Loss: 1.3298
                                                          7%|▋         | 135/1868 [6:59:53<89:47:24, 186.52s/it]  7%|▋         | 136/1868 [7:02:59<89:43:12, 186.49s/it]{'loss': 1.3298, 'grad_norm': 0.0, 'learning_rate': 1.8565310492505356e-05, 'epoch': 0.07}
? Step 136  Epoch: 0.07  Loss: 1.7244
                                                          7%|▋         | 136/1868 [7:02:59<89:43:12, 186.49s/it]  7%|▋         | 137/1868 [7:06:06<89:41:13, 186.52s/it]{'loss': 1.7244, 'grad_norm': 0.0, 'learning_rate': 1.8554603854389725e-05, 'epoch': 0.07}
? Step 137  Epoch: 0.07  Loss: 1.4609
                                                          7%|▋         | 137/1868 [7:06:06<89:41:13, 186.52s/it]  7%|▋         | 138/1868 [7:09:13<89:39:24, 186.57s/it]{'loss': 1.4609, 'grad_norm': 0.0, 'learning_rate': 1.854389721627409e-05, 'epoch': 0.07}
? Step 138  Epoch: 0.07  Loss: 1.5577
                                                          7%|▋         | 138/1868 [7:09:13<89:39:24, 186.57s/it]  7%|▋         | 139/1868 [7:12:19<89:37:48, 186.62s/it]{'loss': 1.5577, 'grad_norm': 0.0, 'learning_rate': 1.853319057815846e-05, 'epoch': 0.07}
? Step 139  Epoch: 0.07  Loss: 1.4937
                                                          7%|▋         | 139/1868 [7:12:19<89:37:48, 186.62s/it]  7%|▋         | 140/1868 [7:15:26<89:31:36, 186.51s/it]{'loss': 1.4937, 'grad_norm': 0.0, 'learning_rate': 1.8522483940042828e-05, 'epoch': 0.07}
? Step 140  Epoch: 0.07  Loss: 2.1691
                                                          7%|▋         | 140/1868 [7:15:26<89:31:36, 186.51s/it]  8%|▊         | 141/1868 [7:18:33<89:31:07, 186.61s/it]{'loss': 2.1691, 'grad_norm': 0.0, 'learning_rate': 1.8511777301927196e-05, 'epoch': 0.07}
? Step 141  Epoch: 0.08  Loss: 1.6261
                                                          8%|▊         | 141/1868 [7:18:33<89:31:07, 186.61s/it]  8%|▊         | 142/1868 [7:21:39<89:29:19, 186.65s/it]{'loss': 1.6261, 'grad_norm': 0.0, 'learning_rate': 1.8501070663811565e-05, 'epoch': 0.08}
? Step 142  Epoch: 0.08  Loss: 1.7687
                                                          8%|▊         | 142/1868 [7:21:39<89:29:19, 186.65s/it]  8%|▊         | 143/1868 [7:24:46<89:22:57, 186.54s/it]{'loss': 1.7687, 'grad_norm': 0.0, 'learning_rate': 1.8490364025695934e-05, 'epoch': 0.08}
? Step 143  Epoch: 0.08  Loss: 1.4492
                                                          8%|▊         | 143/1868 [7:24:46<89:22:57, 186.54s/it]  8%|▊         | 144/1868 [7:27:52<89:18:40, 186.50s/it]{'loss': 1.4492, 'grad_norm': 0.0, 'learning_rate': 1.8479657387580302e-05, 'epoch': 0.08}
? Step 144  Epoch: 0.08  Loss: 1.5651
                                                          8%|▊         | 144/1868 [7:27:52<89:18:40, 186.50s/it]  8%|▊         | 145/1868 [7:30:59<89:18:49, 186.61s/it]{'loss': 1.5651, 'grad_norm': 0.0, 'learning_rate': 1.8468950749464668e-05, 'epoch': 0.08}
? Step 145  Epoch: 0.08  Loss: 1.4875
                                                          8%|▊         | 145/1868 [7:30:59<89:18:49, 186.61s/it]  8%|▊         | 146/1868 [7:34:05<89:14:08, 186.56s/it]{'loss': 1.4875, 'grad_norm': 0.0, 'learning_rate': 1.8458244111349036e-05, 'epoch': 0.08}
? Step 146  Epoch: 0.08  Loss: 1.7592
                                                          8%|▊         | 146/1868 [7:34:05<89:14:08, 186.56s/it]  8%|▊         | 147/1868 [7:37:12<89:12:04, 186.59s/it]{'loss': 1.7592, 'grad_norm': 0.0, 'learning_rate': 1.8447537473233405e-05, 'epoch': 0.08}
? Step 147  Epoch: 0.08  Loss: 1.4586
                                                          8%|▊         | 147/1868 [7:37:12<89:12:04, 186.59s/it]  8%|▊         | 148/1868 [7:40:18<89:07:45, 186.55s/it]{'loss': 1.4586, 'grad_norm': 0.0, 'learning_rate': 1.8436830835117774e-05, 'epoch': 0.08}
? Step 148  Epoch: 0.08  Loss: 1.7043
                                                          8%|▊         | 148/1868 [7:40:18<89:07:45, 186.55s/it]  8%|▊         | 149/1868 [7:43:25<89:06:35, 186.62s/it]{'loss': 1.7043, 'grad_norm': 0.0, 'learning_rate': 1.8426124197002142e-05, 'epoch': 0.08}
? Step 149  Epoch: 0.08  Loss: 1.4618
                                                          8%|▊         | 149/1868 [7:43:25<89:06:35, 186.62s/it]  8%|▊         | 150/1868 [7:46:32<89:01:54, 186.56s/it]{'loss': 1.4618, 'grad_norm': 0.0, 'learning_rate': 1.841541755888651e-05, 'epoch': 0.08}
? Step 150  Epoch: 0.08  Loss: 1.6210
                                                          8%|▊         | 150/1868 [7:46:32<89:01:54, 186.56s/it]  8%|▊         | 151/1868 [7:49:39<89:02:24, 186.69s/it]{'loss': 1.621, 'grad_norm': 0.0, 'learning_rate': 1.840471092077088e-05, 'epoch': 0.08}
? Step 151  Epoch: 0.08  Loss: 1.4782
                                                          8%|▊         | 151/1868 [7:49:39<89:02:24, 186.69s/it]  8%|▊         | 152/1868 [7:52:45<89:00:45, 186.74s/it]{'loss': 1.4782, 'grad_norm': 0.0, 'learning_rate': 1.8394004282655248e-05, 'epoch': 0.08}
? Step 152  Epoch: 0.08  Loss: 1.7506
                                                          8%|▊         | 152/1868 [7:52:45<89:00:45, 186.74s/it]  8%|▊         | 153/1868 [7:55:52<88:55:29, 186.66s/it]{'loss': 1.7506, 'grad_norm': 0.0, 'learning_rate': 1.8383297644539614e-05, 'epoch': 0.08}
? Step 153  Epoch: 0.08  Loss: 1.6162
                                                          8%|▊         | 153/1868 [7:55:52<88:55:29, 186.66s/it]  8%|▊         | 154/1868 [7:58:59<88:51:30, 186.63s/it]{'loss': 1.6162, 'grad_norm': 0.0, 'learning_rate': 1.8372591006423986e-05, 'epoch': 0.08}
? Step 154  Epoch: 0.08  Loss: 1.5326
                                                          8%|▊         | 154/1868 [7:58:59<88:51:30, 186.63s/it]  8%|▊         | 155/1868 [8:02:05<88:50:10, 186.70s/it]{'loss': 1.5326, 'grad_norm': 0.0, 'learning_rate': 1.8361884368308354e-05, 'epoch': 0.08}
? Step 155  Epoch: 0.08  Loss: 1.4316
                                                          8%|▊         | 155/1868 [8:02:05<88:50:10, 186.70s/it]  8%|▊         | 156/1868 [8:05:12<88:45:23, 186.64s/it]{'loss': 1.4316, 'grad_norm': 0.0, 'learning_rate': 1.8351177730192723e-05, 'epoch': 0.08}
? Step 156  Epoch: 0.08  Loss: 1.3787
                                                          8%|▊         | 156/1868 [8:05:12<88:45:23, 186.64s/it]  8%|▊         | 157/1868 [8:08:18<88:41:52, 186.62s/it]{'loss': 1.3787, 'grad_norm': 0.0, 'learning_rate': 1.834047109207709e-05, 'epoch': 0.08}
? Step 157  Epoch: 0.08  Loss: 1.7515
                                                          8%|▊         | 157/1868 [8:08:18<88:41:52, 186.62s/it]  8%|▊         | 158/1868 [8:11:25<88:39:14, 186.64s/it]{'loss': 1.7515, 'grad_norm': 0.0, 'learning_rate': 1.8329764453961457e-05, 'epoch': 0.08}
? Step 158  Epoch: 0.08  Loss: 1.4528
                                                          8%|▊         | 158/1868 [8:11:25<88:39:14, 186.64s/it]  9%|▊         | 159/1868 [8:14:32<88:36:14, 186.64s/it]{'loss': 1.4528, 'grad_norm': 0.0, 'learning_rate': 1.8319057815845826e-05, 'epoch': 0.08}
? Step 159  Epoch: 0.09  Loss: 1.7530
                                                          9%|▊         | 159/1868 [8:14:32<88:36:14, 186.64s/it]  9%|▊         | 160/1868 [8:17:38<88:33:16, 186.65s/it]{'loss': 1.753, 'grad_norm': 0.0, 'learning_rate': 1.8308351177730194e-05, 'epoch': 0.09}
? Step 160  Epoch: 0.09  Loss: 1.4652
                                                          9%|▊         | 160/1868 [8:17:38<88:33:16, 186.65s/it]  9%|▊         | 161/1868 [8:20:45<88:30:43, 186.67s/it]{'loss': 1.4652, 'grad_norm': 0.0, 'learning_rate': 1.8297644539614563e-05, 'epoch': 0.09}
? Step 161  Epoch: 0.09  Loss: 1.3640
                                                          9%|▊         | 161/1868 [8:20:45<88:30:43, 186.67s/it]  9%|▊         | 162/1868 [8:23:52<88:28:46, 186.71s/it]{'loss': 1.364, 'grad_norm': 0.0, 'learning_rate': 1.828693790149893e-05, 'epoch': 0.09}
? Step 162  Epoch: 0.09  Loss: 1.6536
                                                          9%|▊         | 162/1868 [8:23:52<88:28:46, 186.71s/it]  9%|▊         | 163/1868 [8:26:59<88:24:54, 186.68s/it]{'loss': 1.6536, 'grad_norm': 0.0, 'learning_rate': 1.82762312633833e-05, 'epoch': 0.09}
? Step 163  Epoch: 0.09  Loss: 1.6465
                                                          9%|▊         | 163/1868 [8:26:59<88:24:54, 186.68s/it]  9%|▉         | 164/1868 [8:30:06<88:23:50, 186.76s/it]{'loss': 1.6465, 'grad_norm': 0.0, 'learning_rate': 1.826552462526767e-05, 'epoch': 0.09}
? Step 164  Epoch: 0.09  Loss: 1.7695
                                                          9%|▉         | 164/1868 [8:30:06<88:23:50, 186.76s/it]  9%|▉         | 165/1868 [8:33:12<88:18:56, 186.69s/it]{'loss': 1.7695, 'grad_norm': 0.0, 'learning_rate': 1.8254817987152038e-05, 'epoch': 0.09}
? Step 165  Epoch: 0.09  Loss: 1.5366
                                                          9%|▉         | 165/1868 [8:33:12<88:18:56, 186.69s/it]  9%|▉         | 166/1868 [8:36:18<88:13:18, 186.60s/it]{'loss': 1.5366, 'grad_norm': 0.0, 'learning_rate': 1.8244111349036403e-05, 'epoch': 0.09}
? Step 166  Epoch: 0.09  Loss: 1.3066
                                                          9%|▉         | 166/1868 [8:36:18<88:13:18, 186.60s/it]  9%|▉         | 167/1868 [8:39:25<88:06:40, 186.48s/it]{'loss': 1.3066, 'grad_norm': 0.0, 'learning_rate': 1.823340471092077e-05, 'epoch': 0.09}
? Step 167  Epoch: 0.09  Loss: 1.5323
                                                          9%|▉         | 167/1868 [8:39:25<88:06:40, 186.48s/it]  9%|▉         | 168/1868 [8:42:31<88:02:53, 186.45s/it]{'loss': 1.5323, 'grad_norm': 0.0, 'learning_rate': 1.822269807280514e-05, 'epoch': 0.09}
? Step 168  Epoch: 0.09  Loss: 1.6706
                                                          9%|▉         | 168/1868 [8:42:31<88:02:53, 186.45s/it]  9%|▉         | 169/1868 [8:45:38<88:02:51, 186.56s/it]{'loss': 1.6706, 'grad_norm': 0.0, 'learning_rate': 1.821199143468951e-05, 'epoch': 0.09}
? Step 169  Epoch: 0.09  Loss: 1.3640
                                                          9%|▉         | 169/1868 [8:45:38<88:02:51, 186.56s/it]  9%|▉         | 170/1868 [8:48:45<88:01:26, 186.62s/it]{'loss': 1.364, 'grad_norm': 0.0, 'learning_rate': 1.8201284796573878e-05, 'epoch': 0.09}
? Step 170  Epoch: 0.09  Loss: 1.5616
                                                          9%|▉         | 170/1868 [8:48:45<88:01:26, 186.62s/it]  9%|▉         | 171/1868 [8:51:51<87:58:42, 186.64s/it]{'loss': 1.5616, 'grad_norm': 0.0, 'learning_rate': 1.8190578158458246e-05, 'epoch': 0.09}
? Step 171  Epoch: 0.09  Loss: 1.7082
                                                          9%|▉         | 171/1868 [8:51:51<87:58:42, 186.64s/it]  9%|▉         | 172/1868 [8:54:58<87:56:21, 186.66s/it]{'loss': 1.7082, 'grad_norm': 0.0, 'learning_rate': 1.8179871520342615e-05, 'epoch': 0.09}
? Step 172  Epoch: 0.09  Loss: 1.6768
                                                          9%|▉         | 172/1868 [8:54:58<87:56:21, 186.66s/it]  9%|▉         | 173/1868 [8:58:05<87:54:40, 186.71s/it]{'loss': 1.6768, 'grad_norm': 0.0, 'learning_rate': 1.816916488222698e-05, 'epoch': 0.09}
? Step 173  Epoch: 0.09  Loss: 1.4271
                                                          9%|▉         | 173/1868 [8:58:05<87:54:40, 186.71s/it]  9%|▉         | 174/1868 [9:01:12<87:52:48, 186.76s/it]{'loss': 1.4271, 'grad_norm': 0.0, 'learning_rate': 1.815845824411135e-05, 'epoch': 0.09}
? Step 174  Epoch: 0.09  Loss: 1.4515
                                                          9%|▉         | 174/1868 [9:01:12<87:52:48, 186.76s/it]  9%|▉         | 175/1868 [9:04:18<87:48:27, 186.71s/it]{'loss': 1.4515, 'grad_norm': 0.0, 'learning_rate': 1.8147751605995718e-05, 'epoch': 0.09}
? Step 175  Epoch: 0.09  Loss: 1.5154
                                                          9%|▉         | 175/1868 [9:04:18<87:48:27, 186.71s/it]  9%|▉         | 176/1868 [9:07:25<87:44:20, 186.68s/it]{'loss': 1.5154, 'grad_norm': 0.0, 'learning_rate': 1.8137044967880086e-05, 'epoch': 0.09}
? Step 176  Epoch: 0.09  Loss: 1.4488
                                                          9%|▉         | 176/1868 [9:07:25<87:44:20, 186.68s/it]  9%|▉         | 177/1868 [9:10:32<87:42:18, 186.72s/it]{'loss': 1.4488, 'grad_norm': 0.0, 'learning_rate': 1.8126338329764455e-05, 'epoch': 0.09}
? Step 177  Epoch: 0.09  Loss: 1.5860
                                                          9%|▉         | 177/1868 [9:10:32<87:42:18, 186.72s/it] 10%|▉         | 178/1868 [9:13:38<87:39:28, 186.73s/it]{'loss': 1.586, 'grad_norm': 0.0, 'learning_rate': 1.8115631691648824e-05, 'epoch': 0.09}
? Step 178  Epoch: 0.10  Loss: 1.3638
                                                         10%|▉         | 178/1868 [9:13:38<87:39:28, 186.73s/it] 10%|▉         | 179/1868 [9:16:45<87:36:38, 186.74s/it]{'loss': 1.3638, 'grad_norm': 0.0, 'learning_rate': 1.8104925053533192e-05, 'epoch': 0.1}
? Step 179  Epoch: 0.10  Loss: 1.5177
                                                         10%|▉         | 179/1868 [9:16:45<87:36:38, 186.74s/it] 10%|▉         | 180/1868 [9:19:52<87:31:55, 186.68s/it]{'loss': 1.5177, 'grad_norm': 0.0, 'learning_rate': 1.809421841541756e-05, 'epoch': 0.1}
? Step 180  Epoch: 0.10  Loss: 1.7601
                                                         10%|▉         | 180/1868 [9:19:52<87:31:55, 186.68s/it] 10%|▉         | 181/1868 [9:22:58<87:28:21, 186.66s/it]{'loss': 1.7601, 'grad_norm': 0.0, 'learning_rate': 1.808351177730193e-05, 'epoch': 0.1}
? Step 181  Epoch: 0.10  Loss: 1.2627
                                                         10%|▉         | 181/1868 [9:22:58<87:28:21, 186.66s/it] 10%|▉         | 182/1868 [9:26:05<87:26:25, 186.71s/it]{'loss': 1.2627, 'grad_norm': 0.0, 'learning_rate': 1.8072805139186298e-05, 'epoch': 0.1}
? Step 182  Epoch: 0.10  Loss: 1.6633
                                                         10%|▉         | 182/1868 [9:26:05<87:26:25, 186.71s/it] 10%|▉         | 183/1868 [9:29:12<87:23:52, 186.73s/it]{'loss': 1.6633, 'grad_norm': 0.0, 'learning_rate': 1.8062098501070667e-05, 'epoch': 0.1}
? Step 183  Epoch: 0.10  Loss: 1.7217
                                                         10%|▉         | 183/1868 [9:29:12<87:23:52, 186.73s/it] 10%|▉         | 184/1868 [9:32:19<87:22:40, 186.79s/it]{'loss': 1.7217, 'grad_norm': 0.0, 'learning_rate': 1.8051391862955036e-05, 'epoch': 0.1}
? Step 184  Epoch: 0.10  Loss: 1.6408
                                                         10%|▉         | 184/1868 [9:32:19<87:22:40, 186.79s/it] 10%|▉         | 185/1868 [9:35:26<87:18:33, 186.76s/it]{'loss': 1.6408, 'grad_norm': 0.0, 'learning_rate': 1.8040685224839404e-05, 'epoch': 0.1}
? Step 185  Epoch: 0.10  Loss: 1.7098
                                                         10%|▉         | 185/1868 [9:35:26<87:18:33, 186.76s/it] 10%|▉         | 186/1868 [9:38:33<87:17:19, 186.82s/it]{'loss': 1.7098, 'grad_norm': 0.0, 'learning_rate': 1.802997858672377e-05, 'epoch': 0.1}
? Step 186  Epoch: 0.10  Loss: 1.8170
                                                         10%|▉         | 186/1868 [9:38:33<87:17:19, 186.82s/it] 10%|█         | 187/1868 [9:41:40<87:18:32, 186.98s/it]{'loss': 1.817, 'grad_norm': 0.0, 'learning_rate': 1.8019271948608138e-05, 'epoch': 0.1}
? Step 187  Epoch: 0.10  Loss: 1.4078
                                                         10%|█         | 187/1868 [9:41:40<87:18:32, 186.98s/it] 10%|█         | 188/1868 [9:44:47<87:13:55, 186.93s/it]{'loss': 1.4078, 'grad_norm': 0.0, 'learning_rate': 1.8008565310492507e-05, 'epoch': 0.1}
? Step 188  Epoch: 0.10  Loss: 1.6496
                                                         10%|█         | 188/1868 [9:44:47<87:13:55, 186.93s/it] 10%|█         | 189/1868 [9:47:53<87:08:57, 186.86s/it]{'loss': 1.6496, 'grad_norm': 0.0, 'learning_rate': 1.7997858672376876e-05, 'epoch': 0.1}
? Step 189  Epoch: 0.10  Loss: 1.9178
                                                         10%|█         | 189/1868 [9:47:53<87:08:57, 186.86s/it] 10%|█         | 190/1868 [9:51:00<87:05:19, 186.84s/it]{'loss': 1.9178, 'grad_norm': 0.0, 'learning_rate': 1.7987152034261244e-05, 'epoch': 0.1}
? Step 190  Epoch: 0.10  Loss: 1.7432
                                                         10%|█         | 190/1868 [9:51:00<87:05:19, 186.84s/it] 10%|█         | 191/1868 [9:54:07<87:00:31, 186.78s/it]{'loss': 1.7432, 'grad_norm': 0.0, 'learning_rate': 1.7976445396145613e-05, 'epoch': 0.1}
? Step 191  Epoch: 0.10  Loss: 1.7704
                                                         10%|█         | 191/1868 [9:54:07<87:00:31, 186.78s/it] 10%|█         | 192/1868 [9:57:13<86:54:30, 186.68s/it]{'loss': 1.7704, 'grad_norm': 0.0, 'learning_rate': 1.796573875802998e-05, 'epoch': 0.1}
? Step 192  Epoch: 0.10  Loss: 1.9851
                                                         10%|█         | 192/1868 [9:57:13<86:54:30, 186.68s/it] 10%|█         | 193/1868 [10:00:20<86:52:59, 186.73s/it]{'loss': 1.9851, 'grad_norm': 0.0, 'learning_rate': 1.7955032119914347e-05, 'epoch': 0.1}
? Step 193  Epoch: 0.10  Loss: 1.6439
                                                          10%|█         | 193/1868 [10:00:20<86:52:59, 186.73s/it] 10%|█         | 194/1868 [10:03:27<86:48:55, 186.70s/it]{'loss': 1.6439, 'grad_norm': 0.0, 'learning_rate': 1.7944325481798716e-05, 'epoch': 0.1}
? Step 194  Epoch: 0.10  Loss: 1.5743
                                                          10%|█         | 194/1868 [10:03:27<86:48:55, 186.70s/it] 10%|█         | 195/1868 [10:06:33<86:45:28, 186.69s/it]{'loss': 1.5743, 'grad_norm': 0.0, 'learning_rate': 1.7933618843683084e-05, 'epoch': 0.1}
? Step 195  Epoch: 0.10  Loss: 1.5115
                                                          10%|█         | 195/1868 [10:06:33<86:45:28, 186.69s/it] 10%|█         | 196/1868 [10:09:41<86:46:45, 186.85s/it]{'loss': 1.5115, 'grad_norm': 0.0, 'learning_rate': 1.7922912205567453e-05, 'epoch': 0.1}
? Step 196  Epoch: 0.10  Loss: 1.7026
                                                          10%|█         | 196/1868 [10:09:41<86:46:45, 186.85s/it] 11%|█         | 197/1868 [10:12:48<86:43:54, 186.85s/it]{'loss': 1.7026, 'grad_norm': 0.0, 'learning_rate': 1.791220556745182e-05, 'epoch': 0.1}
? Step 197  Epoch: 0.11  Loss: 1.5340
                                                          11%|█         | 197/1868 [10:12:48<86:43:54, 186.85s/it] 11%|█         | 198/1868 [10:15:54<86:41:20, 186.87s/it]{'loss': 1.534, 'grad_norm': 0.0, 'learning_rate': 1.790149892933619e-05, 'epoch': 0.11}
? Step 198  Epoch: 0.11  Loss: 1.3990
                                                          11%|█         | 198/1868 [10:15:54<86:41:20, 186.87s/it] 11%|█         | 199/1868 [10:19:02<86:39:37, 186.92s/it]{'loss': 1.399, 'grad_norm': 0.0, 'learning_rate': 1.789079229122056e-05, 'epoch': 0.11}
? Step 199  Epoch: 0.11  Loss: 1.5679
                                                          11%|█         | 199/1868 [10:19:02<86:39:37, 186.92s/it] 11%|█         | 200/1868 [10:22:08<86:36:50, 186.94s/it]{'loss': 1.5679, 'grad_norm': 0.0, 'learning_rate': 1.7880085653104928e-05, 'epoch': 0.11}
? Step 200  Epoch: 0.11  Loss: 1.7500
                                                          11%|█         | 200/1868 [10:22:08<86:36:50, 186.94s/it]/home/dev25-01/mistral-env/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 11%|█         | 201/1868 [10:25:16<86:36:27, 187.03s/it]{'loss': 1.75, 'grad_norm': 0.0, 'learning_rate': 1.7869379014989293e-05, 'epoch': 0.11}
? Step 201  Epoch: 0.11  Loss: 1.5753
                                                          11%|█         | 201/1868 [10:25:16<86:36:27, 187.03s/it] 11%|█         | 202/1868 [10:28:22<86:30:42, 186.94s/it]{'loss': 1.5753, 'grad_norm': 0.0, 'learning_rate': 1.785867237687366e-05, 'epoch': 0.11}
? Step 202  Epoch: 0.11  Loss: 1.5138
                                                          11%|█         | 202/1868 [10:28:22<86:30:42, 186.94s/it] 11%|█         | 203/1868 [10:31:29<86:28:15, 186.96s/it]{'loss': 1.5138, 'grad_norm': 0.0, 'learning_rate': 1.784796573875803e-05, 'epoch': 0.11}
? Step 203  Epoch: 0.11  Loss: 1.4937
                                                          11%|█         | 203/1868 [10:31:29<86:28:15, 186.96s/it] 11%|█         | 204/1868 [10:34:36<86:23:19, 186.90s/it]{'loss': 1.4937, 'grad_norm': 0.0, 'learning_rate': 1.78372591006424e-05, 'epoch': 0.11}
? Step 204  Epoch: 0.11  Loss: 1.8170
                                                          11%|█         | 204/1868 [10:34:36<86:23:19, 186.90s/it] 11%|█         | 205/1868 [10:37:43<86:20:41, 186.92s/it]{'loss': 1.817, 'grad_norm': 0.0, 'learning_rate': 1.7826552462526768e-05, 'epoch': 0.11}
? Step 205  Epoch: 0.11  Loss: 1.3989
                                                          11%|█         | 205/1868 [10:37:43<86:20:41, 186.92s/it] 11%|█         | 206/1868 [10:40:50<86:16:39, 186.88s/it]{'loss': 1.3989, 'grad_norm': 0.0, 'learning_rate': 1.7815845824411136e-05, 'epoch': 0.11}
? Step 206  Epoch: 0.11  Loss: 1.9481
                                                          11%|█         | 206/1868 [10:40:50<86:16:39, 186.88s/it] 11%|█         | 207/1868 [10:43:57<86:13:54, 186.90s/it]{'loss': 1.9481, 'grad_norm': 0.0, 'learning_rate': 1.7805139186295505e-05, 'epoch': 0.11}
? Step 207  Epoch: 0.11  Loss: 1.4127
                                                          11%|█         | 207/1868 [10:43:57<86:13:54, 186.90s/it] 11%|█         | 208/1868 [10:47:04<86:09:55, 186.87s/it]{'loss': 1.4127, 'grad_norm': 0.0, 'learning_rate': 1.7794432548179874e-05, 'epoch': 0.11}
? Step 208  Epoch: 0.11  Loss: 1.6739
                                                          11%|█         | 208/1868 [10:47:04<86:09:55, 186.87s/it] 11%|█         | 209/1868 [10:50:11<86:08:21, 186.92s/it]{'loss': 1.6739, 'grad_norm': 0.0, 'learning_rate': 1.7783725910064242e-05, 'epoch': 0.11}
? Step 209  Epoch: 0.11  Loss: 1.6394
                                                          11%|█         | 209/1868 [10:50:11<86:08:21, 186.92s/it] 11%|█         | 210/1868 [10:53:18<86:05:32, 186.93s/it]{'loss': 1.6394, 'grad_norm': 0.0, 'learning_rate': 1.777301927194861e-05, 'epoch': 0.11}
? Step 210  Epoch: 0.11  Loss: 1.4755
                                                          11%|█         | 210/1868 [10:53:18<86:05:32, 186.93s/it] 11%|█▏        | 211/1868 [10:56:25<86:02:23, 186.93s/it]{'loss': 1.4755, 'grad_norm': 0.0, 'learning_rate': 1.776231263383298e-05, 'epoch': 0.11}
? Step 211  Epoch: 0.11  Loss: 1.7553
                                                          11%|█▏        | 211/1868 [10:56:25<86:02:23, 186.93s/it] 11%|█▏        | 212/1868 [10:59:31<85:58:02, 186.89s/it]{'loss': 1.7553, 'grad_norm': 0.0, 'learning_rate': 1.7751605995717348e-05, 'epoch': 0.11}
? Step 212  Epoch: 0.11  Loss: 1.4428
                                                          11%|█▏        | 212/1868 [10:59:31<85:58:02, 186.89s/it] 11%|█▏        | 213/1868 [11:02:39<85:58:03, 187.00s/it]{'loss': 1.4428, 'grad_norm': 0.0, 'learning_rate': 1.7740899357601717e-05, 'epoch': 0.11}
? Step 213  Epoch: 0.11  Loss: 1.3561
                                                          11%|█▏        | 213/1868 [11:02:39<85:58:03, 187.00s/it] 11%|█▏        | 214/1868 [11:05:46<85:57:08, 187.08s/it]{'loss': 1.3561, 'grad_norm': 0.0, 'learning_rate': 1.7730192719486082e-05, 'epoch': 0.11}
? Step 214  Epoch: 0.11  Loss: 1.6654
                                                          11%|█▏        | 214/1868 [11:05:46<85:57:08, 187.08s/it] 12%|█▏        | 215/1868 [11:08:53<85:52:04, 187.01s/it]{'loss': 1.6654, 'grad_norm': 0.0, 'learning_rate': 1.771948608137045e-05, 'epoch': 0.11}
? Step 215  Epoch: 0.12  Loss: 1.4674
                                                          12%|█▏        | 215/1868 [11:08:53<85:52:04, 187.01s/it] 12%|█▏        | 216/1868 [11:12:00<85:53:46, 187.18s/it]{'loss': 1.4674, 'grad_norm': 0.0, 'learning_rate': 1.770877944325482e-05, 'epoch': 0.12}
? Step 216  Epoch: 0.12  Loss: 1.3942
                                                          12%|█▏        | 216/1868 [11:12:00<85:53:46, 187.18s/it] 12%|█▏        | 217/1868 [11:15:07<85:48:44, 187.11s/it]{'loss': 1.3942, 'grad_norm': 0.0, 'learning_rate': 1.7698072805139188e-05, 'epoch': 0.12}
? Step 217  Epoch: 0.12  Loss: 1.4743
                                                          12%|█▏        | 217/1868 [11:15:07<85:48:44, 187.11s/it] 12%|█▏        | 218/1868 [11:18:14<85:45:15, 187.10s/it]{'loss': 1.4743, 'grad_norm': 0.0, 'learning_rate': 1.7687366167023557e-05, 'epoch': 0.12}
? Step 218  Epoch: 0.12  Loss: 1.6683
                                                          12%|█▏        | 218/1868 [11:18:14<85:45:15, 187.10s/it] 12%|█▏        | 219/1868 [11:21:21<85:41:12, 187.07s/it]{'loss': 1.6683, 'grad_norm': 0.0, 'learning_rate': 1.7676659528907926e-05, 'epoch': 0.12}
? Step 219  Epoch: 0.12  Loss: 1.5061
                                                          12%|█▏        | 219/1868 [11:21:21<85:41:12, 187.07s/it] 12%|█▏        | 220/1868 [11:24:29<85:40:56, 187.17s/it]{'loss': 1.5061, 'grad_norm': 0.0, 'learning_rate': 1.7665952890792294e-05, 'epoch': 0.12}
? Step 220  Epoch: 0.12  Loss: 1.3941
                                                          12%|█▏        | 220/1868 [11:24:29<85:40:56, 187.17s/it] 12%|█▏        | 221/1868 [11:27:36<85:39:23, 187.23s/it]{'loss': 1.3941, 'grad_norm': 0.0, 'learning_rate': 1.765524625267666e-05, 'epoch': 0.12}
? Step 221  Epoch: 0.12  Loss: 1.5458
                                                          12%|█▏        | 221/1868 [11:27:36<85:39:23, 187.23s/it] 12%|█▏        | 222/1868 [11:30:43<85:34:44, 187.17s/it]{'loss': 1.5458, 'grad_norm': 0.0, 'learning_rate': 1.7644539614561028e-05, 'epoch': 0.12}
? Step 222  Epoch: 0.12  Loss: 1.4750
                                                          12%|█▏        | 222/1868 [11:30:43<85:34:44, 187.17s/it] 12%|█▏        | 223/1868 [11:33:50<85:30:42, 187.14s/it]{'loss': 1.475, 'grad_norm': 0.0, 'learning_rate': 1.7633832976445397e-05, 'epoch': 0.12}
? Step 223  Epoch: 0.12  Loss: 1.6466
                                                          12%|█▏        | 223/1868 [11:33:50<85:30:42, 187.14s/it] 12%|█▏        | 224/1868 [11:36:57<85:26:33, 187.10s/it]{'loss': 1.6466, 'grad_norm': 0.0, 'learning_rate': 1.7623126338329766e-05, 'epoch': 0.12}
? Step 224  Epoch: 0.12  Loss: 1.7878
                                                          12%|█▏        | 224/1868 [11:36:57<85:26:33, 187.10s/it] 12%|█▏        | 225/1868 [11:40:04<85:23:21, 187.10s/it]{'loss': 1.7878, 'grad_norm': 0.0, 'learning_rate': 1.7612419700214134e-05, 'epoch': 0.12}
? Step 225  Epoch: 0.12  Loss: 1.4703
                                                          12%|█▏        | 225/1868 [11:40:04<85:23:21, 187.10s/it] 12%|█▏        | 226/1868 [11:43:11<85:18:48, 187.05s/it]{'loss': 1.4703, 'grad_norm': 0.0, 'learning_rate': 1.7601713062098503e-05, 'epoch': 0.12}
? Step 226  Epoch: 0.12  Loss: 1.5942
                                                          12%|█▏        | 226/1868 [11:43:11<85:18:48, 187.05s/it] 12%|█▏        | 227/1868 [11:46:18<85:15:57, 187.06s/it]{'loss': 1.5942, 'grad_norm': 0.0, 'learning_rate': 1.759100642398287e-05, 'epoch': 0.12}
? Step 227  Epoch: 0.12  Loss: 1.5650
                                                          12%|█▏        | 227/1868 [11:46:18<85:15:57, 187.06s/it] 12%|█▏        | 228/1868 [11:49:25<85:12:52, 187.06s/it]{'loss': 1.565, 'grad_norm': 0.0, 'learning_rate': 1.758029978586724e-05, 'epoch': 0.12}
? Step 228  Epoch: 0.12  Loss: 1.7435
                                                          12%|█▏        | 228/1868 [11:49:25<85:12:52, 187.06s/it] 12%|█▏        | 229/1868 [11:52:32<85:08:19, 187.00s/it]{'loss': 1.7435, 'grad_norm': 0.0, 'learning_rate': 1.7569593147751605e-05, 'epoch': 0.12}
? Step 229  Epoch: 0.12  Loss: 1.3542
                                                          12%|█▏        | 229/1868 [11:52:32<85:08:19, 187.00s/it] 12%|█▏        | 230/1868 [11:55:39<85:05:57, 187.03s/it]{'loss': 1.3542, 'grad_norm': 0.0, 'learning_rate': 1.7558886509635974e-05, 'epoch': 0.12}
? Step 230  Epoch: 0.12  Loss: 1.5419
                                                          12%|█▏        | 230/1868 [11:55:39<85:05:57, 187.03s/it] 12%|█▏        | 231/1868 [11:58:47<85:03:50, 187.07s/it]{'loss': 1.5419, 'grad_norm': 0.0, 'learning_rate': 1.7548179871520343e-05, 'epoch': 0.12}
? Step 231  Epoch: 0.12  Loss: 1.4995
                                                          12%|█▏        | 231/1868 [11:58:47<85:03:50, 187.07s/it] 12%|█▏        | 232/1868 [12:01:54<85:03:42, 187.18s/it]{'loss': 1.4995, 'grad_norm': 0.0, 'learning_rate': 1.753747323340471e-05, 'epoch': 0.12}
? Step 232  Epoch: 0.12  Loss: 1.5453
                                                          12%|█▏        | 232/1868 [12:01:54<85:03:42, 187.18s/it] 12%|█▏        | 233/1868 [12:05:01<85:02:21, 187.24s/it]{'loss': 1.5453, 'grad_norm': 0.0, 'learning_rate': 1.752676659528908e-05, 'epoch': 0.12}
? Step 233  Epoch: 0.12  Loss: 1.5428
                                                          12%|█▏        | 233/1868 [12:05:01<85:02:21, 187.24s/it] 13%|█▎        | 234/1868 [12:08:09<84:58:44, 187.22s/it]{'loss': 1.5428, 'grad_norm': 0.0, 'learning_rate': 1.751605995717345e-05, 'epoch': 0.12}
? Step 234  Epoch: 0.13  Loss: 1.5699
                                                          13%|█▎        | 234/1868 [12:08:09<84:58:44, 187.22s/it] 13%|█▎        | 235/1868 [12:11:16<84:57:02, 187.28s/it]{'loss': 1.5699, 'grad_norm': 0.0, 'learning_rate': 1.7505353319057818e-05, 'epoch': 0.13}
? Step 235  Epoch: 0.13  Loss: 1.4482
                                                          13%|█▎        | 235/1868 [12:11:16<84:57:02, 187.28s/it] 13%|█▎        | 236/1868 [12:14:23<84:54:36, 187.30s/it]{'loss': 1.4482, 'grad_norm': 0.0, 'learning_rate': 1.7494646680942186e-05, 'epoch': 0.13}
? Step 236  Epoch: 0.13  Loss: 1.4707
                                                          13%|█▎        | 236/1868 [12:14:23<84:54:36, 187.30s/it] 13%|█▎        | 237/1868 [12:17:30<84:49:15, 187.22s/it]{'loss': 1.4707, 'grad_norm': 0.0, 'learning_rate': 1.7483940042826555e-05, 'epoch': 0.13}
? Step 237  Epoch: 0.13  Loss: 1.5861
                                                          13%|█▎        | 237/1868 [12:17:30<84:49:15, 187.22s/it] 13%|█▎        | 238/1868 [12:20:38<84:46:34, 187.24s/it]{'loss': 1.5861, 'grad_norm': 0.0, 'learning_rate': 1.7473233404710924e-05, 'epoch': 0.13}
? Step 238  Epoch: 0.13  Loss: 1.5819
                                                          13%|█▎        | 238/1868 [12:20:38<84:46:34, 187.24s/it] 13%|█▎        | 239/1868 [12:23:45<84:43:17, 187.23s/it]{'loss': 1.5819, 'grad_norm': 0.0, 'learning_rate': 1.7462526766595292e-05, 'epoch': 0.13}
? Step 239  Epoch: 0.13  Loss: 1.4139
                                                          13%|█▎        | 239/1868 [12:23:45<84:43:17, 187.23s/it] 13%|█▎        | 240/1868 [12:26:52<84:38:20, 187.16s/it]{'loss': 1.4139, 'grad_norm': 0.0, 'learning_rate': 1.745182012847966e-05, 'epoch': 0.13}
? Step 240  Epoch: 0.13  Loss: 1.4152
                                                          13%|█▎        | 240/1868 [12:26:52<84:38:20, 187.16s/it] 13%|█▎        | 241/1868 [12:29:59<84:34:18, 187.13s/it]{'loss': 1.4152, 'grad_norm': 0.0, 'learning_rate': 1.7441113490364026e-05, 'epoch': 0.13}
? Step 241  Epoch: 0.13  Loss: 1.4947
                                                          13%|█▎        | 241/1868 [12:29:59<84:34:18, 187.13s/it] 13%|█▎        | 242/1868 [12:33:06<84:29:45, 187.08s/it]{'loss': 1.4947, 'grad_norm': 0.0, 'learning_rate': 1.7430406852248395e-05, 'epoch': 0.13}
? Step 242  Epoch: 0.13  Loss: 1.3944
                                                          13%|█▎        | 242/1868 [12:33:06<84:29:45, 187.08s/it] 13%|█▎        | 243/1868 [12:36:13<84:29:46, 187.19s/it]{'loss': 1.3944, 'grad_norm': 0.0, 'learning_rate': 1.7419700214132764e-05, 'epoch': 0.13}
? Step 243  Epoch: 0.13  Loss: 1.5315
                                                          13%|█▎        | 243/1868 [12:36:13<84:29:46, 187.19s/it] 13%|█▎        | 244/1868 [12:39:21<84:29:40, 187.30s/it]{'loss': 1.5315, 'grad_norm': 0.0, 'learning_rate': 1.7408993576017132e-05, 'epoch': 0.13}
? Step 244  Epoch: 0.13  Loss: 1.6775
                                                          13%|█▎        | 244/1868 [12:39:21<84:29:40, 187.30s/it] 13%|█▎        | 245/1868 [12:42:28<84:25:24, 187.26s/it]{'loss': 1.6775, 'grad_norm': 0.0, 'learning_rate': 1.73982869379015e-05, 'epoch': 0.13}
? Step 245  Epoch: 0.13  Loss: 1.7088
                                                          13%|█▎        | 245/1868 [12:42:28<84:25:24, 187.26s/it] 13%|█▎        | 246/1868 [12:45:35<84:23:24, 187.30s/it]{'loss': 1.7088, 'grad_norm': 0.0, 'learning_rate': 1.738758029978587e-05, 'epoch': 0.13}
? Step 246  Epoch: 0.13  Loss: 1.6824
                                                          13%|█▎        | 246/1868 [12:45:35<84:23:24, 187.30s/it] 13%|█▎        | 247/1868 [12:48:43<84:20:59, 187.33s/it]{'loss': 1.6824, 'grad_norm': 0.0, 'learning_rate': 1.7376873661670238e-05, 'epoch': 0.13}
? Step 247  Epoch: 0.13  Loss: 1.5205
                                                          13%|█▎        | 247/1868 [12:48:43<84:20:59, 187.33s/it] 13%|█▎        | 248/1868 [12:51:50<84:16:31, 187.28s/it]{'loss': 1.5205, 'grad_norm': 0.0, 'learning_rate': 1.7366167023554607e-05, 'epoch': 0.13}
? Step 248  Epoch: 0.13  Loss: 1.7406
                                                          13%|█▎        | 248/1868 [12:51:50<84:16:31, 187.28s/it] 13%|█▎        | 249/1868 [12:54:57<84:13:32, 187.28s/it]{'loss': 1.7406, 'grad_norm': 0.0, 'learning_rate': 1.7355460385438972e-05, 'epoch': 0.13}
? Step 249  Epoch: 0.13  Loss: 1.8253
                                                          13%|█▎        | 249/1868 [12:54:57<84:13:32, 187.28s/it] 13%|█▎        | 250/1868 [12:58:05<84:10:06, 187.27s/it]{'loss': 1.8253, 'grad_norm': 0.0, 'learning_rate': 1.734475374732334e-05, 'epoch': 0.13}
? Step 250  Epoch: 0.13  Loss: 1.5531
                                                          13%|█▎        | 250/1868 [12:58:05<84:10:06, 187.27s/it] 13%|█▎        | 251/1868 [13:01:12<84:08:29, 187.33s/it]{'loss': 1.5531, 'grad_norm': 0.0, 'learning_rate': 1.733404710920771e-05, 'epoch': 0.13}
? Step 251  Epoch: 0.13  Loss: 1.4859
                                                          13%|█▎        | 251/1868 [13:01:12<84:08:29, 187.33s/it] 13%|█▎        | 252/1868 [13:04:20<84:06:55, 187.39s/it]{'loss': 1.4859, 'grad_norm': 0.0, 'learning_rate': 1.7323340471092078e-05, 'epoch': 0.13}
? Step 252  Epoch: 0.13  Loss: 1.7594
                                                          13%|█▎        | 252/1868 [13:04:20<84:06:55, 187.39s/it] 14%|█▎        | 253/1868 [13:07:26<83:58:27, 187.19s/it]{'loss': 1.7594, 'grad_norm': 0.0, 'learning_rate': 1.7312633832976447e-05, 'epoch': 0.13}
? Step 253  Epoch: 0.14  Loss: 1.9124
                                                          14%|█▎        | 253/1868 [13:07:26<83:58:27, 187.19s/it] 14%|█▎        | 254/1868 [13:10:33<83:54:28, 187.16s/it]{'loss': 1.9124, 'grad_norm': 0.0, 'learning_rate': 1.7301927194860816e-05, 'epoch': 0.14}
? Step 254  Epoch: 0.14  Loss: 1.3815
                                                          14%|█▎        | 254/1868 [13:10:33<83:54:28, 187.16s/it] 14%|█▎        | 255/1868 [13:13:41<83:52:22, 187.19s/it]{'loss': 1.3815, 'grad_norm': 0.0, 'learning_rate': 1.7291220556745184e-05, 'epoch': 0.14}
? Step 255  Epoch: 0.14  Loss: 1.6484
                                                          14%|█▎        | 255/1868 [13:13:41<83:52:22, 187.19s/it] 14%|█▎        | 256/1868 [13:16:48<83:47:10, 187.12s/it]{'loss': 1.6484, 'grad_norm': 0.0, 'learning_rate': 1.7280513918629553e-05, 'epoch': 0.14}
? Step 256  Epoch: 0.14  Loss: 1.3344
                                                          14%|█▎        | 256/1868 [13:16:48<83:47:10, 187.12s/it] 14%|█▍        | 257/1868 [13:19:54<83:42:24, 187.05s/it]{'loss': 1.3344, 'grad_norm': 0.0, 'learning_rate': 1.7269807280513918e-05, 'epoch': 0.14}
? Step 257  Epoch: 0.14  Loss: 1.7331
                                                          14%|█▍        | 257/1868 [13:19:54<83:42:24, 187.05s/it] 14%|█▍        | 258/1868 [13:23:02<83:41:09, 187.12s/it]{'loss': 1.7331, 'grad_norm': 0.0, 'learning_rate': 1.7259100642398287e-05, 'epoch': 0.14}
? Step 258  Epoch: 0.14  Loss: 1.6189
                                                          14%|█▍        | 258/1868 [13:23:02<83:41:09, 187.12s/it] 14%|█▍        | 259/1868 [13:26:09<83:37:02, 187.09s/it]{'loss': 1.6189, 'grad_norm': 0.0, 'learning_rate': 1.7248394004282655e-05, 'epoch': 0.14}
? Step 259  Epoch: 0.14  Loss: 1.6782
                                                          14%|█▍        | 259/1868 [13:26:09<83:37:02, 187.09s/it] 14%|█▍        | 260/1868 [13:29:16<83:36:01, 187.16s/it]{'loss': 1.6782, 'grad_norm': 0.0, 'learning_rate': 1.7237687366167024e-05, 'epoch': 0.14}
? Step 260  Epoch: 0.14  Loss: 1.5142
                                                          14%|█▍        | 260/1868 [13:29:16<83:36:01, 187.16s/it] 14%|█▍        | 261/1868 [13:32:23<83:29:49, 187.05s/it]{'loss': 1.5142, 'grad_norm': 0.0, 'learning_rate': 1.7226980728051393e-05, 'epoch': 0.14}
? Step 261  Epoch: 0.14  Loss: 1.6884
                                                          14%|█▍        | 261/1868 [13:32:23<83:29:49, 187.05s/it] 14%|█▍        | 262/1868 [13:35:30<83:26:44, 187.05s/it]{'loss': 1.6884, 'grad_norm': 0.0, 'learning_rate': 1.721627408993576e-05, 'epoch': 0.14}
? Step 262  Epoch: 0.14  Loss: 1.4976
                                                          14%|█▍        | 262/1868 [13:35:30<83:26:44, 187.05s/it] 14%|█▍        | 263/1868 [13:38:37<83:24:38, 187.09s/it]{'loss': 1.4976, 'grad_norm': 0.0, 'learning_rate': 1.720556745182013e-05, 'epoch': 0.14}
? Step 263  Epoch: 0.14  Loss: 1.4508
                                                          14%|█▍        | 263/1868 [13:38:37<83:24:38, 187.09s/it] 14%|█▍        | 264/1868 [13:41:44<83:22:41, 187.13s/it]{'loss': 1.4508, 'grad_norm': 0.0, 'learning_rate': 1.71948608137045e-05, 'epoch': 0.14}
? Step 264  Epoch: 0.14  Loss: 1.4672
                                                          14%|█▍        | 264/1868 [13:41:44<83:22:41, 187.13s/it] 14%|█▍        | 265/1868 [13:44:52<83:20:18, 187.16s/it]{'loss': 1.4672, 'grad_norm': 0.0, 'learning_rate': 1.7184154175588868e-05, 'epoch': 0.14}
? Step 265  Epoch: 0.14  Loss: 1.5480
                                                          14%|█▍        | 265/1868 [13:44:52<83:20:18, 187.16s/it] 14%|█▍        | 266/1868 [13:47:59<83:17:10, 187.16s/it]{'loss': 1.548, 'grad_norm': 0.0, 'learning_rate': 1.7173447537473236e-05, 'epoch': 0.14}
? Step 266  Epoch: 0.14  Loss: 1.6091
                                                          14%|█▍        | 266/1868 [13:47:59<83:17:10, 187.16s/it] 14%|█▍        | 267/1868 [13:51:06<83:16:04, 187.24s/it]{'loss': 1.6091, 'grad_norm': 0.0, 'learning_rate': 1.7162740899357605e-05, 'epoch': 0.14}
? Step 267  Epoch: 0.14  Loss: 1.4661
                                                          14%|█▍        | 267/1868 [13:51:06<83:16:04, 187.24s/it] 14%|█▍        | 268/1868 [13:54:13<83:10:58, 187.16s/it]{'loss': 1.4661, 'grad_norm': 0.0, 'learning_rate': 1.7152034261241974e-05, 'epoch': 0.14}
? Step 268  Epoch: 0.14  Loss: 1.5160
                                                          14%|█▍        | 268/1868 [13:54:13<83:10:58, 187.16s/it] 14%|█▍        | 269/1868 [13:57:20<83:04:08, 187.02s/it]{'loss': 1.516, 'grad_norm': 0.0, 'learning_rate': 1.714132762312634e-05, 'epoch': 0.14}
? Step 269  Epoch: 0.14  Loss: 1.7825
                                                          14%|█▍        | 269/1868 [13:57:20<83:04:08, 187.02s/it] 14%|█▍        | 270/1868 [14:00:27<83:01:44, 187.05s/it]{'loss': 1.7825, 'grad_norm': 0.0, 'learning_rate': 1.7130620985010707e-05, 'epoch': 0.14}
? Step 270  Epoch: 0.14  Loss: 1.6632
                                                          14%|█▍        | 270/1868 [14:00:27<83:01:44, 187.05s/it] 15%|█▍        | 271/1868 [14:03:34<82:56:58, 186.99s/it]{'loss': 1.6632, 'grad_norm': 0.0, 'learning_rate': 1.7119914346895076e-05, 'epoch': 0.14}
? Step 271  Epoch: 0.15  Loss: 1.8806
                                                          15%|█▍        | 271/1868 [14:03:34<82:56:58, 186.99s/it] 15%|█▍        | 272/1868 [14:06:41<82:54:42, 187.02s/it]{'loss': 1.8806, 'grad_norm': 0.0, 'learning_rate': 1.7109207708779445e-05, 'epoch': 0.15}
? Step 272  Epoch: 0.15  Loss: 1.5147
                                                          15%|█▍        | 272/1868 [14:06:41<82:54:42, 187.02s/it] 15%|█▍        | 273/1868 [14:09:48<82:54:24, 187.13s/it]{'loss': 1.5147, 'grad_norm': 0.0, 'learning_rate': 1.7098501070663814e-05, 'epoch': 0.15}
? Step 273  Epoch: 0.15  Loss: 1.5950
                                                          15%|█▍        | 273/1868 [14:09:48<82:54:24, 187.13s/it] 15%|█▍        | 274/1868 [14:12:55<82:48:15, 187.01s/it]{'loss': 1.595, 'grad_norm': 0.0, 'learning_rate': 1.7087794432548182e-05, 'epoch': 0.15}
? Step 274  Epoch: 0.15  Loss: 1.5631
                                                          15%|█▍        | 274/1868 [14:12:55<82:48:15, 187.01s/it] 15%|█▍        | 275/1868 [14:16:02<82:45:47, 187.04s/it]{'loss': 1.5631, 'grad_norm': 0.0, 'learning_rate': 1.707708779443255e-05, 'epoch': 0.15}
? Step 275  Epoch: 0.15  Loss: 1.4998
                                                          15%|█▍        | 275/1868 [14:16:02<82:45:47, 187.04s/it] 15%|█▍        | 276/1868 [14:19:09<82:41:39, 187.00s/it]{'loss': 1.4998, 'grad_norm': 0.0, 'learning_rate': 1.706638115631692e-05, 'epoch': 0.15}
? Step 276  Epoch: 0.15  Loss: 1.8800
                                                          15%|█▍        | 276/1868 [14:19:09<82:41:39, 187.00s/it] 15%|█▍        | 277/1868 [14:22:16<82:38:46, 187.01s/it]{'loss': 1.88, 'grad_norm': 0.0, 'learning_rate': 1.7055674518201285e-05, 'epoch': 0.15}
? Step 277  Epoch: 0.15  Loss: 1.6366
                                                          15%|█▍        | 277/1868 [14:22:16<82:38:46, 187.01s/it] 15%|█▍        | 278/1868 [14:25:23<82:35:08, 186.99s/it]{'loss': 1.6366, 'grad_norm': 0.0, 'learning_rate': 1.7044967880085653e-05, 'epoch': 0.15}
? Step 278  Epoch: 0.15  Loss: 1.6373
                                                          15%|█▍        | 278/1868 [14:25:23<82:35:08, 186.99s/it] 15%|█▍        | 279/1868 [14:28:30<82:31:40, 186.97s/it]{'loss': 1.6373, 'grad_norm': 0.0, 'learning_rate': 1.7034261241970022e-05, 'epoch': 0.15}
? Step 279  Epoch: 0.15  Loss: 1.6377
                                                          15%|█▍        | 279/1868 [14:28:30<82:31:40, 186.97s/it] 15%|█▍        | 280/1868 [14:31:37<82:28:05, 186.96s/it]{'loss': 1.6377, 'grad_norm': 0.0, 'learning_rate': 1.702355460385439e-05, 'epoch': 0.15}
? Step 280  Epoch: 0.15  Loss: 1.8916
                                                          15%|█▍        | 280/1868 [14:31:37<82:28:05, 186.96s/it] 15%|█▌        | 281/1868 [14:34:44<82:24:48, 186.95s/it]{'loss': 1.8916, 'grad_norm': 0.0, 'learning_rate': 1.701284796573876e-05, 'epoch': 0.15}
? Step 281  Epoch: 0.15  Loss: 1.4392
                                                          15%|█▌        | 281/1868 [14:34:44<82:24:48, 186.95s/it] 15%|█▌        | 282/1868 [14:37:51<82:23:03, 187.00s/it]{'loss': 1.4392, 'grad_norm': 0.0, 'learning_rate': 1.7002141327623128e-05, 'epoch': 0.15}
? Step 282  Epoch: 0.15  Loss: 1.7008
                                                          15%|█▌        | 282/1868 [14:37:51<82:23:03, 187.00s/it] 15%|█▌        | 283/1868 [14:40:58<82:19:45, 186.99s/it]{'loss': 1.7008, 'grad_norm': 0.0, 'learning_rate': 1.6991434689507497e-05, 'epoch': 0.15}
? Step 283  Epoch: 0.15  Loss: 1.4660
                                                          15%|█▌        | 283/1868 [14:40:58<82:19:45, 186.99s/it] 15%|█▌        | 284/1868 [14:44:05<82:17:04, 187.01s/it]{'loss': 1.466, 'grad_norm': 0.0, 'learning_rate': 1.6980728051391862e-05, 'epoch': 0.15}
? Step 284  Epoch: 0.15  Loss: 1.6264
                                                          15%|█▌        | 284/1868 [14:44:05<82:17:04, 187.01s/it] 15%|█▌        | 285/1868 [14:47:12<82:13:41, 187.00s/it]{'loss': 1.6264, 'grad_norm': 0.0, 'learning_rate': 1.697002141327623e-05, 'epoch': 0.15}
? Step 285  Epoch: 0.15  Loss: 1.3773
                                                          15%|█▌        | 285/1868 [14:47:12<82:13:41, 187.00s/it] 15%|█▌        | 286/1868 [14:50:19<82:10:17, 186.99s/it]{'loss': 1.3773, 'grad_norm': 0.0, 'learning_rate': 1.69593147751606e-05, 'epoch': 0.15}
? Step 286  Epoch: 0.15  Loss: 1.6396
                                                          15%|█▌        | 286/1868 [14:50:19<82:10:17, 186.99s/it] 15%|█▌        | 287/1868 [14:53:26<82:06:36, 186.97s/it]{'loss': 1.6396, 'grad_norm': 0.0, 'learning_rate': 1.6948608137044968e-05, 'epoch': 0.15}
? Step 287  Epoch: 0.15  Loss: 1.8771
                                                          15%|█▌        | 287/1868 [14:53:26<82:06:36, 186.97s/it] 15%|█▌        | 288/1868 [14:56:32<82:01:48, 186.90s/it]{'loss': 1.8771, 'grad_norm': 0.0, 'learning_rate': 1.6937901498929337e-05, 'epoch': 0.15}
? Step 288  Epoch: 0.15  Loss: 1.4236
                                                          15%|█▌        | 288/1868 [14:56:32<82:01:48, 186.90s/it] 15%|█▌        | 289/1868 [14:59:39<81:57:38, 186.86s/it]{'loss': 1.4236, 'grad_norm': 0.0, 'learning_rate': 1.6927194860813705e-05, 'epoch': 0.15}
? Step 289  Epoch: 0.15  Loss: 1.9030
                                                          15%|█▌        | 289/1868 [14:59:39<81:57:38, 186.86s/it] 16%|█▌        | 290/1868 [15:02:46<81:55:22, 186.90s/it]{'loss': 1.903, 'grad_norm': 0.0, 'learning_rate': 1.6916488222698074e-05, 'epoch': 0.15}
? Step 290  Epoch: 0.16  Loss: 1.6061
                                                          16%|█▌        | 290/1868 [15:02:46<81:55:22, 186.90s/it] 16%|█▌        | 291/1868 [15:05:53<81:51:21, 186.86s/it]{'loss': 1.6061, 'grad_norm': 0.0, 'learning_rate': 1.6905781584582443e-05, 'epoch': 0.16}
? Step 291  Epoch: 0.16  Loss: 1.6816
                                                          16%|█▌        | 291/1868 [15:05:53<81:51:21, 186.86s/it] 16%|█▌        | 292/1868 [15:09:00<81:49:29, 186.91s/it]{'loss': 1.6816, 'grad_norm': 0.0, 'learning_rate': 1.689507494646681e-05, 'epoch': 0.16}
? Step 292  Epoch: 0.16  Loss: 1.7413
                                                          16%|█▌        | 292/1868 [15:09:00<81:49:29, 186.91s/it] 16%|█▌        | 293/1868 [15:12:07<81:45:31, 186.88s/it]{'loss': 1.7413, 'grad_norm': 0.0, 'learning_rate': 1.688436830835118e-05, 'epoch': 0.16}
? Step 293  Epoch: 0.16  Loss: 1.6363
                                                          16%|█▌        | 293/1868 [15:12:07<81:45:31, 186.88s/it] 16%|█▌        | 294/1868 [15:15:14<81:41:35, 186.85s/it]{'loss': 1.6363, 'grad_norm': 0.0, 'learning_rate': 1.687366167023555e-05, 'epoch': 0.16}
? Step 294  Epoch: 0.16  Loss: 1.8938
                                                          16%|█▌        | 294/1868 [15:15:14<81:41:35, 186.85s/it] 16%|█▌        | 295/1868 [15:18:21<81:39:25, 186.88s/it]{'loss': 1.8938, 'grad_norm': 0.0, 'learning_rate': 1.6862955032119918e-05, 'epoch': 0.16}
? Step 295  Epoch: 0.16  Loss: 1.4195
                                                          16%|█▌        | 295/1868 [15:18:21<81:39:25, 186.88s/it] 16%|█▌        | 296/1868 [15:21:28<81:37:02, 186.91s/it]{'loss': 1.4195, 'grad_norm': 0.0, 'learning_rate': 1.6852248394004286e-05, 'epoch': 0.16}
? Step 296  Epoch: 0.16  Loss: 1.4866
                                                          16%|█▌        | 296/1868 [15:21:28<81:37:02, 186.91s/it] 16%|█▌        | 297/1868 [15:24:34<81:32:20, 186.85s/it]{'loss': 1.4866, 'grad_norm': 0.0, 'learning_rate': 1.684154175588865e-05, 'epoch': 0.16}
? Step 297  Epoch: 0.16  Loss: 2.0990
                                                          16%|█▌        | 297/1868 [15:24:34<81:32:20, 186.85s/it] 16%|█▌        | 298/1868 [15:27:41<81:29:29, 186.86s/it]{'loss': 2.099, 'grad_norm': 0.0, 'learning_rate': 1.683083511777302e-05, 'epoch': 0.16}
? Step 298  Epoch: 0.16  Loss: 1.5276
                                                          16%|█▌        | 298/1868 [15:27:41<81:29:29, 186.86s/it] 16%|█▌        | 299/1868 [15:30:48<81:27:42, 186.91s/it]{'loss': 1.5276, 'grad_norm': 0.0, 'learning_rate': 1.682012847965739e-05, 'epoch': 0.16}
? Step 299  Epoch: 0.16  Loss: 1.7516
                                                          16%|█▌        | 299/1868 [15:30:48<81:27:42, 186.91s/it] 16%|█▌        | 300/1868 [15:33:55<81:24:46, 186.92s/it]{'loss': 1.7516, 'grad_norm': 0.0, 'learning_rate': 1.6809421841541757e-05, 'epoch': 0.16}
? Step 300  Epoch: 0.16  Loss: 1.4088
                                                          16%|█▌        | 300/1868 [15:33:55<81:24:46, 186.92s/it]/home/dev25-01/mistral-env/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 16%|█▌        | 301/1868 [15:37:02<81:25:07, 187.05s/it]{'loss': 1.4088, 'grad_norm': 0.0, 'learning_rate': 1.6798715203426126e-05, 'epoch': 0.16}
? Step 301  Epoch: 0.16  Loss: 1.7525
                                                          16%|█▌        | 301/1868 [15:37:02<81:25:07, 187.05s/it] 16%|█▌        | 302/1868 [15:40:10<81:22:45, 187.08s/it]{'loss': 1.7525, 'grad_norm': 0.0, 'learning_rate': 1.6788008565310495e-05, 'epoch': 0.16}
? Step 302  Epoch: 0.16  Loss: 1.6854
                                                          16%|█▌        | 302/1868 [15:40:10<81:22:45, 187.08s/it] 16%|█▌        | 303/1868 [15:43:17<81:20:15, 187.10s/it]{'loss': 1.6854, 'grad_norm': 0.0, 'learning_rate': 1.6777301927194864e-05, 'epoch': 0.16}
? Step 303  Epoch: 0.16  Loss: 1.4649
                                                          16%|█▌        | 303/1868 [15:43:17<81:20:15, 187.10s/it] 16%|█▋        | 304/1868 [15:46:24<81:16:44, 187.09s/it]{'loss': 1.4649, 'grad_norm': 0.0, 'learning_rate': 1.6766595289079232e-05, 'epoch': 0.16}
? Step 304  Epoch: 0.16  Loss: 1.8290
                                                          16%|█▋        | 304/1868 [15:46:24<81:16:44, 187.09s/it] 16%|█▋        | 305/1868 [15:49:31<81:14:50, 187.13s/it]{'loss': 1.829, 'grad_norm': 0.0, 'learning_rate': 1.6755888650963597e-05, 'epoch': 0.16}
? Step 305  Epoch: 0.16  Loss: 1.7196
                                                          16%|█▋        | 305/1868 [15:49:31<81:14:50, 187.13s/it] 16%|█▋        | 306/1868 [15:52:38<81:10:30, 187.09s/it]{'loss': 1.7196, 'grad_norm': 0.0, 'learning_rate': 1.6745182012847966e-05, 'epoch': 0.16}
? Step 306  Epoch: 0.16  Loss: 1.6046
                                                          16%|█▋        | 306/1868 [15:52:38<81:10:30, 187.09s/it] 16%|█▋        | 307/1868 [15:55:45<81:07:24, 187.09s/it]{'loss': 1.6046, 'grad_norm': 0.0, 'learning_rate': 1.6734475374732335e-05, 'epoch': 0.16}
? Step 307  Epoch: 0.16  Loss: 1.4820
                                                          16%|█▋        | 307/1868 [15:55:45<81:07:24, 187.09s/it] 16%|█▋        | 308/1868 [15:58:52<81:02:24, 187.02s/it]{'loss': 1.482, 'grad_norm': 0.0, 'learning_rate': 1.6723768736616703e-05, 'epoch': 0.16}
? Step 308  Epoch: 0.16  Loss: 1.8289
                                                          16%|█▋        | 308/1868 [15:58:52<81:02:24, 187.02s/it] 17%|█▋        | 309/1868 [16:01:59<80:58:46, 187.00s/it]{'loss': 1.8289, 'grad_norm': 0.0, 'learning_rate': 1.6713062098501072e-05, 'epoch': 0.16}
? Step 309  Epoch: 0.17  Loss: 1.6102
                                                          17%|█▋        | 309/1868 [16:01:59<80:58:46, 187.00s/it] 17%|█▋        | 310/1868 [16:05:05<80:52:04, 186.86s/it]{'loss': 1.6102, 'grad_norm': 0.0, 'learning_rate': 1.670235546038544e-05, 'epoch': 0.17}
? Step 310  Epoch: 0.17  Loss: 1.5344
                                                          17%|█▋        | 310/1868 [16:05:05<80:52:04, 186.86s/it] 17%|█▋        | 311/1868 [16:08:13<80:52:57, 187.01s/it]{'loss': 1.5344, 'grad_norm': 0.0, 'learning_rate': 1.669164882226981e-05, 'epoch': 0.17}
? Step 311  Epoch: 0.17  Loss: 1.5399
                                                          17%|█▋        | 311/1868 [16:08:13<80:52:57, 187.01s/it] 17%|█▋        | 312/1868 [16:11:20<80:48:37, 186.97s/it]{'loss': 1.5399, 'grad_norm': 0.0, 'learning_rate': 1.6680942184154175e-05, 'epoch': 0.17}
? Step 312  Epoch: 0.17  Loss: 1.6436
                                                          17%|█▋        | 312/1868 [16:11:20<80:48:37, 186.97s/it] 17%|█▋        | 313/1868 [16:14:27<80:45:09, 186.95s/it]{'loss': 1.6436, 'grad_norm': 0.0, 'learning_rate': 1.6670235546038543e-05, 'epoch': 0.17}
? Step 313  Epoch: 0.17  Loss: 1.6258
                                                          17%|█▋        | 313/1868 [16:14:27<80:45:09, 186.95s/it] 17%|█▋        | 314/1868 [16:17:33<80:40:52, 186.91s/it]{'loss': 1.6258, 'grad_norm': 0.0, 'learning_rate': 1.6659528907922912e-05, 'epoch': 0.17}
? Step 314  Epoch: 0.17  Loss: 1.5275
                                                          17%|█▋        | 314/1868 [16:17:33<80:40:52, 186.91s/it] 17%|█▋        | 315/1868 [16:20:40<80:37:21, 186.89s/it]{'loss': 1.5275, 'grad_norm': 0.0, 'learning_rate': 1.664882226980728e-05, 'epoch': 0.17}
? Step 315  Epoch: 0.17  Loss: 1.5145
                                                          17%|█▋        | 315/1868 [16:20:40<80:37:21, 186.89s/it] 17%|█▋        | 316/1868 [16:23:47<80:34:15, 186.89s/it]{'loss': 1.5145, 'grad_norm': 0.0, 'learning_rate': 1.663811563169165e-05, 'epoch': 0.17}
? Step 316  Epoch: 0.17  Loss: 1.5156
                                                          17%|█▋        | 316/1868 [16:23:47<80:34:15, 186.89s/it] 17%|█▋        | 317/1868 [16:26:54<80:33:17, 186.97s/it]{'loss': 1.5156, 'grad_norm': 0.0, 'learning_rate': 1.6627408993576018e-05, 'epoch': 0.17}
? Step 317  Epoch: 0.17  Loss: 1.6611
                                                          17%|█▋        | 317/1868 [16:26:54<80:33:17, 186.97s/it] 17%|█▋        | 318/1868 [16:30:01<80:29:22, 186.94s/it]{'loss': 1.6611, 'grad_norm': 0.0, 'learning_rate': 1.6616702355460387e-05, 'epoch': 0.17}
? Step 318  Epoch: 0.17  Loss: 1.4857
                                                          17%|█▋        | 318/1868 [16:30:01<80:29:22, 186.94s/it] 17%|█▋        | 319/1868 [16:33:08<80:26:16, 186.94s/it]{'loss': 1.4857, 'grad_norm': 0.0, 'learning_rate': 1.6605995717344755e-05, 'epoch': 0.17}
? Step 319  Epoch: 0.17  Loss: 1.7085
                                                          17%|█▋        | 319/1868 [16:33:08<80:26:16, 186.94s/it] 17%|█▋        | 320/1868 [16:36:15<80:21:41, 186.89s/it]{'loss': 1.7085, 'grad_norm': 0.0, 'learning_rate': 1.6595289079229124e-05, 'epoch': 0.17}
? Step 320  Epoch: 0.17  Loss: 1.7817
                                                          17%|█▋        | 320/1868 [16:36:15<80:21:41, 186.89s/it] 17%|█▋        | 321/1868 [16:39:21<80:16:13, 186.80s/it]{'loss': 1.7817, 'grad_norm': 0.0, 'learning_rate': 1.6584582441113493e-05, 'epoch': 0.17}
? Step 321  Epoch: 0.17  Loss: 1.6279
                                                          17%|█▋        | 321/1868 [16:39:21<80:16:13, 186.80s/it] 17%|█▋        | 322/1868 [16:42:28<80:14:27, 186.85s/it]{'loss': 1.6279, 'grad_norm': 0.0, 'learning_rate': 1.657387580299786e-05, 'epoch': 0.17}
? Step 322  Epoch: 0.17  Loss: 1.4848
                                                          17%|█▋        | 322/1868 [16:42:28<80:14:27, 186.85s/it] 17%|█▋        | 323/1868 [16:45:35<80:12:18, 186.89s/it]{'loss': 1.4848, 'grad_norm': 0.0, 'learning_rate': 1.656316916488223e-05, 'epoch': 0.17}
? Step 323  Epoch: 0.17  Loss: 1.4685
                                                          17%|█▋        | 323/1868 [16:45:35<80:12:18, 186.89s/it] 17%|█▋        | 324/1868 [16:48:42<80:08:10, 186.85s/it]{'loss': 1.4685, 'grad_norm': 0.0, 'learning_rate': 1.65524625267666e-05, 'epoch': 0.17}
? Step 324  Epoch: 0.17  Loss: 1.7269
                                                          17%|█▋        | 324/1868 [16:48:42<80:08:10, 186.85s/it] 17%|█▋        | 325/1868 [16:51:49<80:04:40, 186.83s/it]{'loss': 1.7269, 'grad_norm': 0.0, 'learning_rate': 1.6541755888650964e-05, 'epoch': 0.17}
? Step 325  Epoch: 0.17  Loss: 1.5956
                                                          17%|█▋        | 325/1868 [16:51:49<80:04:40, 186.83s/it] 17%|█▋        | 326/1868 [16:54:56<80:05:10, 186.97s/it]{'loss': 1.5956, 'grad_norm': 0.0, 'learning_rate': 1.6531049250535333e-05, 'epoch': 0.17}
? Step 326  Epoch: 0.17  Loss: 1.6978
                                                          17%|█▋        | 326/1868 [16:54:56<80:05:10, 186.97s/it] 18%|█▊        | 327/1868 [16:58:03<80:01:51, 186.96s/it]{'loss': 1.6978, 'grad_norm': 0.0, 'learning_rate': 1.65203426124197e-05, 'epoch': 0.17}
? Step 327  Epoch: 0.18  Loss: 1.7593
                                                          18%|█▊        | 327/1868 [16:58:03<80:01:51, 186.96s/it] 18%|█▊        | 328/1868 [17:01:10<79:58:58, 186.97s/it]{'loss': 1.7593, 'grad_norm': 0.0, 'learning_rate': 1.650963597430407e-05, 'epoch': 0.18}
? Step 328  Epoch: 0.18  Loss: 1.4007
                                                          18%|█▊        | 328/1868 [17:01:10<79:58:58, 186.97s/it] 18%|█▊        | 329/1868 [17:04:17<79:55:39, 186.97s/it]{'loss': 1.4007, 'grad_norm': 0.0, 'learning_rate': 1.649892933618844e-05, 'epoch': 0.18}
? Step 329  Epoch: 0.18  Loss: 1.7340
                                                          18%|█▊        | 329/1868 [17:04:17<79:55:39, 186.97s/it] 18%|█▊        | 330/1868 [17:07:24<79:51:56, 186.94s/it]{'loss': 1.734, 'grad_norm': 0.0, 'learning_rate': 1.6488222698072807e-05, 'epoch': 0.18}
? Step 330  Epoch: 0.18  Loss: 1.5193
                                                          18%|█▊        | 330/1868 [17:07:24<79:51:56, 186.94s/it] 18%|█▊        | 331/1868 [17:10:31<79:47:36, 186.89s/it]{'loss': 1.5193, 'grad_norm': 0.0, 'learning_rate': 1.6477516059957176e-05, 'epoch': 0.18}
? Step 331  Epoch: 0.18  Loss: 1.6191
                                                          18%|█▊        | 331/1868 [17:10:31<79:47:36, 186.89s/it] 18%|█▊        | 332/1868 [17:13:38<79:46:14, 186.96s/it]{'loss': 1.6191, 'grad_norm': 0.0, 'learning_rate': 1.646680942184154e-05, 'epoch': 0.18}
? Step 332  Epoch: 0.18  Loss: 1.4719
                                                          18%|█▊        | 332/1868 [17:13:38<79:46:14, 186.96s/it] 18%|█▊        | 333/1868 [17:16:45<79:43:27, 186.98s/it]{'loss': 1.4719, 'grad_norm': 0.0, 'learning_rate': 1.645610278372591e-05, 'epoch': 0.18}
? Step 333  Epoch: 0.18  Loss: 1.4347
                                                          18%|█▊        | 333/1868 [17:16:45<79:43:27, 186.98s/it] 18%|█▊        | 334/1868 [17:19:52<79:40:44, 186.99s/it]{'loss': 1.4347, 'grad_norm': 0.0, 'learning_rate': 1.644539614561028e-05, 'epoch': 0.18}
? Step 334  Epoch: 0.18  Loss: 1.5593
                                                          18%|█▊        | 334/1868 [17:19:52<79:40:44, 186.99s/it] 18%|█▊        | 335/1868 [17:22:59<79:34:22, 186.86s/it]{'loss': 1.5593, 'grad_norm': 0.0, 'learning_rate': 1.6434689507494647e-05, 'epoch': 0.18}
? Step 335  Epoch: 0.18  Loss: 1.8666
                                                          18%|█▊        | 335/1868 [17:22:59<79:34:22, 186.86s/it] 18%|█▊        | 336/1868 [17:26:05<79:30:17, 186.83s/it]{'loss': 1.8666, 'grad_norm': 0.0, 'learning_rate': 1.6423982869379016e-05, 'epoch': 0.18}
? Step 336  Epoch: 0.18  Loss: 1.4644
                                                          18%|█▊        | 336/1868 [17:26:05<79:30:17, 186.83s/it] 18%|█▊        | 337/1868 [17:29:12<79:26:44, 186.81s/it]{'loss': 1.4644, 'grad_norm': 0.0, 'learning_rate': 1.6413276231263385e-05, 'epoch': 0.18}
? Step 337  Epoch: 0.18  Loss: 1.6870
                                                          18%|█▊        | 337/1868 [17:29:12<79:26:44, 186.81s/it] 18%|█▊        | 338/1868 [17:32:19<79:23:35, 186.81s/it]{'loss': 1.687, 'grad_norm': 0.0, 'learning_rate': 1.6402569593147753e-05, 'epoch': 0.18}
? Step 338  Epoch: 0.18  Loss: 1.7750
                                                          18%|█▊        | 338/1868 [17:32:19<79:23:35, 186.81s/it] 18%|█▊        | 339/1868 [17:35:26<79:19:27, 186.77s/it]{'loss': 1.775, 'grad_norm': 0.0, 'learning_rate': 1.6391862955032122e-05, 'epoch': 0.18}
? Step 339  Epoch: 0.18  Loss: 1.8361
                                                          18%|█▊        | 339/1868 [17:35:26<79:19:27, 186.77s/it] 18%|█▊        | 340/1868 [17:38:33<79:18:45, 186.86s/it]{'loss': 1.8361, 'grad_norm': 0.0, 'learning_rate': 1.6381156316916487e-05, 'epoch': 0.18}
? Step 340  Epoch: 0.18  Loss: 1.5627
                                                          18%|█▊        | 340/1868 [17:38:33<79:18:45, 186.86s/it] 18%|█▊        | 341/1868 [17:41:39<79:15:23, 186.85s/it]{'loss': 1.5627, 'grad_norm': 0.0, 'learning_rate': 1.6370449678800856e-05, 'epoch': 0.18}
? Step 341  Epoch: 0.18  Loss: 1.6799
                                                          18%|█▊        | 341/1868 [17:41:39<79:15:23, 186.85s/it] 18%|█▊        | 342/1868 [17:44:46<79:12:04, 186.84s/it]{'loss': 1.6799, 'grad_norm': 0.0, 'learning_rate': 1.6359743040685225e-05, 'epoch': 0.18}
? Step 342  Epoch: 0.18  Loss: 1.5080
                                                          18%|█▊        | 342/1868 [17:44:46<79:12:04, 186.84s/it] 18%|█▊        | 343/1868 [17:47:53<79:07:52, 186.80s/it]{'loss': 1.508, 'grad_norm': 0.0, 'learning_rate': 1.6349036402569593e-05, 'epoch': 0.18}
? Step 343  Epoch: 0.18  Loss: 1.4803
                                                          18%|█▊        | 343/1868 [17:47:53<79:07:52, 186.80s/it] 18%|█▊        | 344/1868 [17:51:00<79:05:11, 186.82s/it]{'loss': 1.4803, 'grad_norm': 0.0, 'learning_rate': 1.6338329764453962e-05, 'epoch': 0.18}
? Step 344  Epoch: 0.18  Loss: 1.5266
                                                          18%|█▊        | 344/1868 [17:51:00<79:05:11, 186.82s/it] 18%|█▊        | 345/1868 [17:54:07<79:02:00, 186.82s/it]{'loss': 1.5266, 'grad_norm': 0.0, 'learning_rate': 1.632762312633833e-05, 'epoch': 0.18}
? Step 345  Epoch: 0.18  Loss: 1.5260
                                                          18%|█▊        | 345/1868 [17:54:07<79:02:00, 186.82s/it] 19%|█▊        | 346/1868 [17:57:13<78:56:11, 186.71s/it]{'loss': 1.526, 'grad_norm': 0.0, 'learning_rate': 1.63169164882227e-05, 'epoch': 0.18}
? Step 346  Epoch: 0.19  Loss: 1.6689
                                                          19%|█▊        | 346/1868 [17:57:13<78:56:11, 186.71s/it] 19%|█▊        | 347/1868 [18:00:20<78:53:47, 186.74s/it]{'loss': 1.6689, 'grad_norm': 0.0, 'learning_rate': 1.6306209850107068e-05, 'epoch': 0.19}
? Step 347  Epoch: 0.19  Loss: 1.4089
                                                          19%|█▊        | 347/1868 [18:00:20<78:53:47, 186.74s/it] 19%|█▊        | 348/1868 [18:03:27<78:52:53, 186.82s/it]{'loss': 1.4089, 'grad_norm': 0.0, 'learning_rate': 1.6295503211991437e-05, 'epoch': 0.19}
? Step 348  Epoch: 0.19  Loss: 1.4778
                                                          19%|█▊        | 348/1868 [18:03:27<78:52:53, 186.82s/it] 19%|█▊        | 349/1868 [18:06:34<78:49:53, 186.83s/it]{'loss': 1.4778, 'grad_norm': 0.0, 'learning_rate': 1.6284796573875805e-05, 'epoch': 0.19}
? Step 349  Epoch: 0.19  Loss: 1.4858
                                                          19%|█▊        | 349/1868 [18:06:34<78:49:53, 186.83s/it] 19%|█▊        | 350/1868 [18:09:41<78:48:36, 186.90s/it]{'loss': 1.4858, 'grad_norm': 0.0, 'learning_rate': 1.6274089935760174e-05, 'epoch': 0.19}
? Step 350  Epoch: 0.19  Loss: 1.6624
                                                          19%|█▊        | 350/1868 [18:09:41<78:48:36, 186.90s/it] 19%|█▉        | 351/1868 [18:12:48<78:46:40, 186.95s/it]{'loss': 1.6624, 'grad_norm': 0.0, 'learning_rate': 1.6263383297644543e-05, 'epoch': 0.19}
? Step 351  Epoch: 0.19  Loss: 1.8012
                                                          19%|█▉        | 351/1868 [18:12:48<78:46:40, 186.95s/it] 19%|█▉        | 352/1868 [18:15:54<78:40:49, 186.84s/it]{'loss': 1.8012, 'grad_norm': 0.0, 'learning_rate': 1.625267665952891e-05, 'epoch': 0.19}
? Step 352  Epoch: 0.19  Loss: 1.5166
                                                          19%|█▉        | 352/1868 [18:15:54<78:40:49, 186.84s/it] 19%|█▉        | 353/1868 [18:19:01<78:37:52, 186.85s/it]{'loss': 1.5166, 'grad_norm': 0.0, 'learning_rate': 1.6241970021413277e-05, 'epoch': 0.19}
? Step 353  Epoch: 0.19  Loss: 1.2608
                                                          19%|█▉        | 353/1868 [18:19:01<78:37:52, 186.85s/it] 19%|█▉        | 354/1868 [18:22:09<78:37:55, 186.97s/it]{'loss': 1.2608, 'grad_norm': 0.0, 'learning_rate': 1.6231263383297645e-05, 'epoch': 0.19}
? Step 354  Epoch: 0.19  Loss: 1.5215
                                                          19%|█▉        | 354/1868 [18:22:09<78:37:55, 186.97s/it] 19%|█▉        | 355/1868 [18:25:16<78:34:36, 186.96s/it]{'loss': 1.5215, 'grad_norm': 0.0, 'learning_rate': 1.6220556745182014e-05, 'epoch': 0.19}
? Step 355  Epoch: 0.19  Loss: 1.6299
                                                          19%|█▉        | 355/1868 [18:25:16<78:34:36, 186.96s/it] 19%|█▉        | 356/1868 [18:28:22<78:29:21, 186.88s/it]{'loss': 1.6299, 'grad_norm': 0.0, 'learning_rate': 1.6209850107066383e-05, 'epoch': 0.19}
? Step 356  Epoch: 0.19  Loss: 1.4475
                                                          19%|█▉        | 356/1868 [18:28:22<78:29:21, 186.88s/it] 19%|█▉        | 357/1868 [18:31:29<78:26:23, 186.89s/it]{'loss': 1.4475, 'grad_norm': 0.0, 'learning_rate': 1.619914346895075e-05, 'epoch': 0.19}
? Step 357  Epoch: 0.19  Loss: 1.5485
                                                          19%|█▉        | 357/1868 [18:31:29<78:26:23, 186.89s/it] 19%|█▉        | 358/1868 [18:34:36<78:21:52, 186.83s/it]{'loss': 1.5485, 'grad_norm': 0.0, 'learning_rate': 1.618843683083512e-05, 'epoch': 0.19}
? Step 358  Epoch: 0.19  Loss: 1.4575
                                                          19%|█▉        | 358/1868 [18:34:36<78:21:52, 186.83s/it] 19%|█▉        | 359/1868 [18:37:43<78:20:57, 186.92s/it]{'loss': 1.4575, 'grad_norm': 0.0, 'learning_rate': 1.617773019271949e-05, 'epoch': 0.19}
? Step 359  Epoch: 0.19  Loss: 1.6399
                                                          19%|█▉        | 359/1868 [18:37:43<78:20:57, 186.92s/it] 19%|█▉        | 360/1868 [18:40:50<78:15:50, 186.84s/it]{'loss': 1.6399, 'grad_norm': 0.0, 'learning_rate': 1.6167023554603854e-05, 'epoch': 0.19}
? Step 360  Epoch: 0.19  Loss: 1.5605
                                                          19%|█▉        | 360/1868 [18:40:50<78:15:50, 186.84s/it] 19%|█▉        | 361/1868 [18:43:56<78:12:50, 186.84s/it]{'loss': 1.5605, 'grad_norm': 0.0, 'learning_rate': 1.6156316916488223e-05, 'epoch': 0.19}
? Step 361  Epoch: 0.19  Loss: 1.3810
                                                          19%|█▉        | 361/1868 [18:43:56<78:12:50, 186.84s/it] 19%|█▉        | 362/1868 [18:47:03<78:09:31, 186.83s/it]{'loss': 1.381, 'grad_norm': 0.0, 'learning_rate': 1.614561027837259e-05, 'epoch': 0.19}
? Step 362  Epoch: 0.19  Loss: 1.4831
                                                          19%|█▉        | 362/1868 [18:47:03<78:09:31, 186.83s/it] 19%|█▉        | 363/1868 [18:50:10<78:05:01, 186.78s/it]{'loss': 1.4831, 'grad_norm': 0.0, 'learning_rate': 1.613490364025696e-05, 'epoch': 0.19}
? Step 363  Epoch: 0.19  Loss: 1.7584
                                                          19%|█▉        | 363/1868 [18:50:10<78:05:01, 186.78s/it] 19%|█▉        | 364/1868 [18:53:17<78:00:46, 186.73s/it]{'loss': 1.7584, 'grad_norm': 0.0, 'learning_rate': 1.612419700214133e-05, 'epoch': 0.19}
? Step 364  Epoch: 0.19  Loss: 1.7558
                                                          19%|█▉        | 364/1868 [18:53:17<78:00:46, 186.73s/it] 20%|█▉        | 365/1868 [18:56:23<77:58:14, 186.76s/it]{'loss': 1.7558, 'grad_norm': 0.0, 'learning_rate': 1.6113490364025697e-05, 'epoch': 0.19}
? Step 365  Epoch: 0.20  Loss: 1.3996
                                                          20%|█▉        | 365/1868 [18:56:23<77:58:14, 186.76s/it] 20%|█▉        | 366/1868 [18:59:30<77:52:17, 186.64s/it]{'loss': 1.3996, 'grad_norm': 0.0, 'learning_rate': 1.6102783725910066e-05, 'epoch': 0.2}
? Step 366  Epoch: 0.20  Loss: 1.5660
                                                          20%|█▉        | 366/1868 [18:59:30<77:52:17, 186.64s/it] 20%|█▉        | 367/1868 [19:02:37<77:52:23, 186.77s/it]{'loss': 1.566, 'grad_norm': 0.0, 'learning_rate': 1.6092077087794435e-05, 'epoch': 0.2}
? Step 367  Epoch: 0.20  Loss: 1.5772
                                                          20%|█▉        | 367/1868 [19:02:37<77:52:23, 186.77s/it] 20%|█▉        | 368/1868 [19:05:44<77:50:13, 186.81s/it]{'loss': 1.5772, 'grad_norm': 0.0, 'learning_rate': 1.60813704496788e-05, 'epoch': 0.2}
? Step 368  Epoch: 0.20  Loss: 1.4356
                                                          20%|█▉        | 368/1868 [19:05:44<77:50:13, 186.81s/it] 20%|█▉        | 369/1868 [19:08:50<77:45:39, 186.75s/it]{'loss': 1.4356, 'grad_norm': 0.0, 'learning_rate': 1.607066381156317e-05, 'epoch': 0.2}
? Step 369  Epoch: 0.20  Loss: 1.5064
                                                          20%|█▉        | 369/1868 [19:08:50<77:45:39, 186.75s/it] 20%|█▉        | 370/1868 [19:11:57<77:43:30, 186.79s/it]{'loss': 1.5064, 'grad_norm': 0.0, 'learning_rate': 1.6059957173447537e-05, 'epoch': 0.2}
? Step 370  Epoch: 0.20  Loss: 1.6165
                                                          20%|█▉        | 370/1868 [19:11:57<77:43:30, 186.79s/it] 20%|█▉        | 371/1868 [19:15:04<77:40:05, 186.78s/it]{'loss': 1.6165, 'grad_norm': 0.0, 'learning_rate': 1.6049250535331906e-05, 'epoch': 0.2}
? Step 371  Epoch: 0.20  Loss: 1.6696
                                                          20%|█▉        | 371/1868 [19:15:04<77:40:05, 186.78s/it] 20%|█▉        | 372/1868 [19:18:11<77:37:54, 186.81s/it]{'loss': 1.6696, 'grad_norm': 0.0, 'learning_rate': 1.6038543897216275e-05, 'epoch': 0.2}
? Step 372  Epoch: 0.20  Loss: 1.3258
                                                          20%|█▉        | 372/1868 [19:18:11<77:37:54, 186.81s/it] 20%|█▉        | 373/1868 [19:21:18<77:35:25, 186.84s/it]{'loss': 1.3258, 'grad_norm': 0.0, 'learning_rate': 1.6027837259100643e-05, 'epoch': 0.2}
? Step 373  Epoch: 0.20  Loss: 1.4606
                                                          20%|█▉        | 373/1868 [19:21:18<77:35:25, 186.84s/it] 20%|██        | 374/1868 [19:24:25<77:34:02, 186.91s/it]{'loss': 1.4606, 'grad_norm': 0.0, 'learning_rate': 1.6017130620985012e-05, 'epoch': 0.2}
? Step 374  Epoch: 0.20  Loss: 1.3726
                                                          20%|██        | 374/1868 [19:24:25<77:34:02, 186.91s/it] 20%|██        | 375/1868 [19:27:32<77:30:15, 186.88s/it]{'loss': 1.3726, 'grad_norm': 0.0, 'learning_rate': 1.600642398286938e-05, 'epoch': 0.2}
? Step 375  Epoch: 0.20  Loss: 1.5618
                                                          20%|██        | 375/1868 [19:27:32<77:30:15, 186.88s/it] 20%|██        | 376/1868 [19:30:38<77:25:25, 186.81s/it]{'loss': 1.5618, 'grad_norm': 0.0, 'learning_rate': 1.599571734475375e-05, 'epoch': 0.2}
? Step 376  Epoch: 0.20  Loss: 1.5654
                                                          20%|██        | 376/1868 [19:30:38<77:25:25, 186.81s/it] 20%|██        | 377/1868 [19:33:45<77:20:23, 186.74s/it]{'loss': 1.5654, 'grad_norm': 0.0, 'learning_rate': 1.5985010706638118e-05, 'epoch': 0.2}
? Step 377  Epoch: 0.20  Loss: 2.1024
                                                          20%|██        | 377/1868 [19:33:45<77:20:23, 186.74s/it] 20%|██        | 378/1868 [19:36:52<77:19:48, 186.84s/it]{'loss': 2.1024, 'grad_norm': 0.0, 'learning_rate': 1.5974304068522487e-05, 'epoch': 0.2}
? Step 378  Epoch: 0.20  Loss: 1.5735
                                                          20%|██        | 378/1868 [19:36:52<77:19:48, 186.84s/it] 20%|██        | 379/1868 [19:39:59<77:17:27, 186.87s/it]{'loss': 1.5735, 'grad_norm': 0.0, 'learning_rate': 1.5963597430406855e-05, 'epoch': 0.2}
? Step 379  Epoch: 0.20  Loss: 1.5570
                                                          20%|██        | 379/1868 [19:39:59<77:17:27, 186.87s/it] 20%|██        | 380/1868 [19:43:06<77:13:10, 186.82s/it]{'loss': 1.557, 'grad_norm': 0.0, 'learning_rate': 1.595289079229122e-05, 'epoch': 0.2}
? Step 380  Epoch: 0.20  Loss: 1.3792
                                                          20%|██        | 380/1868 [19:43:06<77:13:10, 186.82s/it] 20%|██        | 381/1868 [19:46:13<77:12:11, 186.91s/it]{'loss': 1.3792, 'grad_norm': 0.0, 'learning_rate': 1.594218415417559e-05, 'epoch': 0.2}
? Step 381  Epoch: 0.20  Loss: 1.5578
                                                          20%|██        | 381/1868 [19:46:13<77:12:11, 186.91s/it] 20%|██        | 382/1868 [19:49:19<77:08:01, 186.86s/it]{'loss': 1.5578, 'grad_norm': 0.0, 'learning_rate': 1.5931477516059958e-05, 'epoch': 0.2}
? Step 382  Epoch: 0.20  Loss: 1.7536
                                                          20%|██        | 382/1868 [19:49:19<77:08:01, 186.86s/it] 21%|██        | 383/1868 [19:52:26<77:04:59, 186.87s/it]{'loss': 1.7536, 'grad_norm': 0.0, 'learning_rate': 1.5920770877944327e-05, 'epoch': 0.2}
? Step 383  Epoch: 0.21  Loss: 1.6186
                                                          21%|██        | 383/1868 [19:52:26<77:04:59, 186.87s/it] 21%|██        | 384/1868 [19:55:33<77:02:44, 186.90s/it]{'loss': 1.6186, 'grad_norm': 0.0, 'learning_rate': 1.5910064239828695e-05, 'epoch': 0.21}
? Step 384  Epoch: 0.21  Loss: 1.6597
                                                          21%|██        | 384/1868 [19:55:33<77:02:44, 186.90s/it] 21%|██        | 385/1868 [19:58:40<76:58:23, 186.85s/it]{'loss': 1.6597, 'grad_norm': 0.0, 'learning_rate': 1.5899357601713064e-05, 'epoch': 0.21}
? Step 385  Epoch: 0.21  Loss: 1.4421
                                                          21%|██        | 385/1868 [19:58:40<76:58:23, 186.85s/it] 21%|██        | 386/1868 [20:01:47<76:52:53, 186.76s/it]{'loss': 1.4421, 'grad_norm': 0.0, 'learning_rate': 1.5888650963597433e-05, 'epoch': 0.21}
? Step 386  Epoch: 0.21  Loss: 1.6974
                                                          21%|██        | 386/1868 [20:01:47<76:52:53, 186.76s/it] 21%|██        | 387/1868 [20:04:53<76:50:06, 186.77s/it]{'loss': 1.6974, 'grad_norm': 0.0, 'learning_rate': 1.58779443254818e-05, 'epoch': 0.21}
? Step 387  Epoch: 0.21  Loss: 1.6904
                                                          21%|██        | 387/1868 [20:04:53<76:50:06, 186.77s/it] 21%|██        | 388/1868 [20:08:00<76:46:23, 186.75s/it]{'loss': 1.6904, 'grad_norm': 0.0, 'learning_rate': 1.5867237687366167e-05, 'epoch': 0.21}
? Step 388  Epoch: 0.21  Loss: 1.7618
                                                          21%|██        | 388/1868 [20:08:00<76:46:23, 186.75s/it] 21%|██        | 389/1868 [20:11:07<76:42:51, 186.73s/it]{'loss': 1.7618, 'grad_norm': 0.0, 'learning_rate': 1.5856531049250535e-05, 'epoch': 0.21}
? Step 389  Epoch: 0.21  Loss: 1.7030
                                                          21%|██        | 389/1868 [20:11:07<76:42:51, 186.73s/it] 21%|██        | 390/1868 [20:14:14<76:40:06, 186.74s/it]{'loss': 1.703, 'grad_norm': 0.0, 'learning_rate': 1.5845824411134904e-05, 'epoch': 0.21}
? Step 390  Epoch: 0.21  Loss: 1.9268
                                                          21%|██        | 390/1868 [20:14:14<76:40:06, 186.74s/it] 21%|██        | 391/1868 [20:17:20<76:37:19, 186.76s/it]{'loss': 1.9268, 'grad_norm': 0.0, 'learning_rate': 1.5835117773019273e-05, 'epoch': 0.21}
? Step 391  Epoch: 0.21  Loss: 1.6374
                                                          21%|██        | 391/1868 [20:17:20<76:37:19, 186.76s/it] 21%|██        | 392/1868 [20:20:27<76:33:51, 186.74s/it]{'loss': 1.6374, 'grad_norm': 0.0, 'learning_rate': 1.582441113490364e-05, 'epoch': 0.21}
? Step 392  Epoch: 0.21  Loss: 1.6419
                                                          21%|██        | 392/1868 [20:20:27<76:33:51, 186.74s/it] 21%|██        | 393/1868 [20:23:34<76:29:09, 186.68s/it]{'loss': 1.6419, 'grad_norm': 0.0, 'learning_rate': 1.581370449678801e-05, 'epoch': 0.21}
? Step 393  Epoch: 0.21  Loss: 1.7966
                                                          21%|██        | 393/1868 [20:23:34<76:29:09, 186.68s/it] 21%|██        | 394/1868 [20:26:40<76:26:28, 186.70s/it]{'loss': 1.7966, 'grad_norm': 0.0, 'learning_rate': 1.580299785867238e-05, 'epoch': 0.21}
? Step 394  Epoch: 0.21  Loss: 1.6055
                                                          21%|██        | 394/1868 [20:26:40<76:26:28, 186.70s/it] 21%|██        | 395/1868 [20:29:47<76:22:45, 186.67s/it]{'loss': 1.6055, 'grad_norm': 0.0, 'learning_rate': 1.5792291220556747e-05, 'epoch': 0.21}
? Step 395  Epoch: 0.21  Loss: 1.5963
                                                          21%|██        | 395/1868 [20:29:47<76:22:45, 186.67s/it] 21%|██        | 396/1868 [20:32:54<76:19:44, 186.67s/it]{'loss': 1.5963, 'grad_norm': 0.0, 'learning_rate': 1.5781584582441113e-05, 'epoch': 0.21}
? Step 396  Epoch: 0.21  Loss: 1.7280
                                                          21%|██        | 396/1868 [20:32:54<76:19:44, 186.67s/it] 21%|██▏       | 397/1868 [20:36:00<76:17:15, 186.70s/it]{'loss': 1.728, 'grad_norm': 0.0, 'learning_rate': 1.577087794432548e-05, 'epoch': 0.21}
? Step 397  Epoch: 0.21  Loss: 1.5326
                                                          21%|██▏       | 397/1868 [20:36:00<76:17:15, 186.70s/it] 21%|██▏       | 398/1868 [20:39:07<76:13:38, 186.68s/it]{'loss': 1.5326, 'grad_norm': 0.0, 'learning_rate': 1.576017130620985e-05, 'epoch': 0.21}
? Step 398  Epoch: 0.21  Loss: 1.7133
                                                          21%|██▏       | 398/1868 [20:39:07<76:13:38, 186.68s/it] 21%|██▏       | 399/1868 [20:42:14<76:11:52, 186.73s/it]{'loss': 1.7133, 'grad_norm': 0.0, 'learning_rate': 1.574946466809422e-05, 'epoch': 0.21}
? Step 399  Epoch: 0.21  Loss: 1.5650
                                                          21%|██▏       | 399/1868 [20:42:14<76:11:52, 186.73s/it] 21%|██▏       | 400/1868 [20:45:21<76:08:09, 186.71s/it]{'loss': 1.565, 'grad_norm': 0.0, 'learning_rate': 1.5738758029978587e-05, 'epoch': 0.21}
? Step 400  Epoch: 0.21  Loss: 1.5632
                                                          21%|██▏       | 400/1868 [20:45:21<76:08:09, 186.71s/it]/home/dev25-01/mistral-env/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 21%|██▏       | 401/1868 [20:48:27<76:05:31, 186.73s/it]{'loss': 1.5632, 'grad_norm': 0.0, 'learning_rate': 1.5728051391862956e-05, 'epoch': 0.21}
? Step 401  Epoch: 0.21  Loss: 1.8976
                                                          21%|██▏       | 401/1868 [20:48:27<76:05:31, 186.73s/it] 22%|██▏       | 402/1868 [20:51:34<76:02:29, 186.73s/it]{'loss': 1.8976, 'grad_norm': 0.0, 'learning_rate': 1.5717344753747325e-05, 'epoch': 0.21}
? Step 402  Epoch: 0.22  Loss: 1.5386
                                                          22%|██▏       | 402/1868 [20:51:34<76:02:29, 186.73s/it] 22%|██▏       | 403/1868 [20:54:41<76:00:15, 186.77s/it]{'loss': 1.5386, 'grad_norm': 0.0, 'learning_rate': 1.5706638115631693e-05, 'epoch': 0.22}
? Step 403  Epoch: 0.22  Loss: 1.5382
                                                          22%|██▏       | 403/1868 [20:54:41<76:00:15, 186.77s/it] 22%|██▏       | 404/1868 [20:57:47<75:55:52, 186.72s/it]{'loss': 1.5382, 'grad_norm': 0.0, 'learning_rate': 1.5695931477516062e-05, 'epoch': 0.22}
? Step 404  Epoch: 0.22  Loss: 1.8347
                                                          22%|██▏       | 404/1868 [20:57:47<75:55:52, 186.72s/it] 22%|██▏       | 405/1868 [21:00:54<75:52:41, 186.71s/it]{'loss': 1.8347, 'grad_norm': 0.0, 'learning_rate': 1.568522483940043e-05, 'epoch': 0.22}
? Step 405  Epoch: 0.22  Loss: 1.6139
                                                          22%|██▏       | 405/1868 [21:00:54<75:52:41, 186.71s/it] 22%|██▏       | 406/1868 [21:04:01<75:46:50, 186.60s/it]{'loss': 1.6139, 'grad_norm': 0.0, 'learning_rate': 1.56745182012848e-05, 'epoch': 0.22}
? Step 406  Epoch: 0.22  Loss: 1.7970
                                                          22%|██▏       | 406/1868 [21:04:01<75:46:50, 186.60s/it] 22%|██▏       | 407/1868 [21:07:07<75:44:07, 186.62s/it]{'loss': 1.797, 'grad_norm': 0.0, 'learning_rate': 1.5663811563169168e-05, 'epoch': 0.22}
? Step 407  Epoch: 0.22  Loss: 1.6496
                                                          22%|██▏       | 407/1868 [21:07:07<75:44:07, 186.62s/it] 22%|██▏       | 408/1868 [21:10:14<75:40:06, 186.58s/it]{'loss': 1.6496, 'grad_norm': 0.0, 'learning_rate': 1.5653104925053533e-05, 'epoch': 0.22}
? Step 408  Epoch: 0.22  Loss: 1.6182
                                                          22%|██▏       | 408/1868 [21:10:14<75:40:06, 186.58s/it] 22%|██▏       | 409/1868 [21:13:20<75:36:44, 186.57s/it]{'loss': 1.6182, 'grad_norm': 0.0, 'learning_rate': 1.5642398286937902e-05, 'epoch': 0.22}
? Step 409  Epoch: 0.22  Loss: 1.7129
                                                          22%|██▏       | 409/1868 [21:13:20<75:36:44, 186.57s/it] 22%|██▏       | 410/1868 [21:16:27<75:33:03, 186.55s/it]{'loss': 1.7129, 'grad_norm': 0.0, 'learning_rate': 1.563169164882227e-05, 'epoch': 0.22}
? Step 410  Epoch: 0.22  Loss: 1.7275
                                                          22%|██▏       | 410/1868 [21:16:27<75:33:03, 186.55s/it] 22%|██▏       | 411/1868 [21:19:33<75:31:21, 186.60s/it]{'loss': 1.7275, 'grad_norm': 0.0, 'learning_rate': 1.562098501070664e-05, 'epoch': 0.22}
? Step 411  Epoch: 0.22  Loss: 1.5731
                                                          22%|██▏       | 411/1868 [21:19:33<75:31:21, 186.60s/it] 22%|██▏       | 412/1868 [21:22:40<75:29:11, 186.64s/it]{'loss': 1.5731, 'grad_norm': 0.0, 'learning_rate': 1.5610278372591008e-05, 'epoch': 0.22}
? Step 412  Epoch: 0.22  Loss: 1.5925
                                                          22%|██▏       | 412/1868 [21:22:40<75:29:11, 186.64s/it] 22%|██▏       | 413/1868 [21:25:47<75:27:55, 186.72s/it]{'loss': 1.5925, 'grad_norm': 0.0, 'learning_rate': 1.5599571734475377e-05, 'epoch': 0.22}
? Step 413  Epoch: 0.22  Loss: 1.5901
                                                          22%|██▏       | 413/1868 [21:25:47<75:27:55, 186.72s/it] 22%|██▏       | 414/1868 [21:28:54<75:23:56, 186.68s/it]{'loss': 1.5901, 'grad_norm': 0.0, 'learning_rate': 1.5588865096359745e-05, 'epoch': 0.22}
? Step 414  Epoch: 0.22  Loss: 1.5382
                                                          22%|██▏       | 414/1868 [21:28:54<75:23:56, 186.68s/it] 22%|██▏       | 415/1868 [21:32:00<75:19:29, 186.63s/it]{'loss': 1.5382, 'grad_norm': 0.0, 'learning_rate': 1.5578158458244114e-05, 'epoch': 0.22}
? Step 415  Epoch: 0.22  Loss: 1.5506
                                                          22%|██▏       | 415/1868 [21:32:00<75:19:29, 186.63s/it] 22%|██▏       | 416/1868 [21:35:07<75:17:00, 186.65s/it]{'loss': 1.5506, 'grad_norm': 0.0, 'learning_rate': 1.556745182012848e-05, 'epoch': 0.22}
? Step 416  Epoch: 0.22  Loss: 1.3878
                                                          22%|██▏       | 416/1868 [21:35:07<75:17:00, 186.65s/it] 22%|██▏       | 417/1868 [21:38:14<75:15:39, 186.73s/it]{'loss': 1.3878, 'grad_norm': 0.0, 'learning_rate': 1.5556745182012848e-05, 'epoch': 0.22}
? Step 417  Epoch: 0.22  Loss: 1.6352
                                                          22%|██▏       | 417/1868 [21:38:14<75:15:39, 186.73s/it] 22%|██▏       | 418/1868 [21:41:21<75:13:20, 186.76s/it]{'loss': 1.6352, 'grad_norm': 0.0, 'learning_rate': 1.5546038543897217e-05, 'epoch': 0.22}
? Step 418  Epoch: 0.22  Loss: 1.5891
                                                          22%|██▏       | 418/1868 [21:41:21<75:13:20, 186.76s/it] 22%|██▏       | 419/1868 [21:44:27<75:09:09, 186.71s/it]{'loss': 1.5891, 'grad_norm': 0.0, 'learning_rate': 1.5535331905781585e-05, 'epoch': 0.22}
? Step 419  Epoch: 0.22  Loss: 1.8758
                                                          22%|██▏       | 419/1868 [21:44:27<75:09:09, 186.71s/it] 22%|██▏       | 420/1868 [21:47:34<75:06:32, 186.73s/it]{'loss': 1.8758, 'grad_norm': 0.0, 'learning_rate': 1.5524625267665954e-05, 'epoch': 0.22}
? Step 420  Epoch: 0.22  Loss: 1.7728
                                                          22%|██▏       | 420/1868 [21:47:34<75:06:32, 186.73s/it] 23%|██▎       | 421/1868 [21:50:41<75:02:28, 186.70s/it]{'loss': 1.7728, 'grad_norm': 0.0, 'learning_rate': 1.5513918629550323e-05, 'epoch': 0.22}
? Step 421  Epoch: 0.23  Loss: 1.4761
                                                          23%|██▎       | 421/1868 [21:50:41<75:02:28, 186.70s/it] 23%|██▎       | 422/1868 [21:53:47<74:58:02, 186.64s/it]{'loss': 1.4761, 'grad_norm': 0.0, 'learning_rate': 1.550321199143469e-05, 'epoch': 0.23}
? Step 422  Epoch: 0.23  Loss: 1.6193
                                                          23%|██▎       | 422/1868 [21:53:47<74:58:02, 186.64s/it] 23%|██▎       | 423/1868 [21:56:54<74:53:26, 186.58s/it]{'loss': 1.6193, 'grad_norm': 0.0, 'learning_rate': 1.5492505353319057e-05, 'epoch': 0.23}
? Step 423  Epoch: 0.23  Loss: 1.4554
                                                          23%|██▎       | 423/1868 [21:56:54<74:53:26, 186.58s/it] 23%|██▎       | 424/1868 [22:00:00<74:50:16, 186.58s/it]{'loss': 1.4554, 'grad_norm': 0.0, 'learning_rate': 1.5481798715203425e-05, 'epoch': 0.23}
? Step 424  Epoch: 0.23  Loss: 1.6518
                                                          23%|██▎       | 424/1868 [22:00:00<74:50:16, 186.58s/it] 23%|██▎       | 425/1868 [22:03:06<74:44:49, 186.48s/it]{'loss': 1.6518, 'grad_norm': 0.0, 'learning_rate': 1.5471092077087794e-05, 'epoch': 0.23}
? Step 425  Epoch: 0.23  Loss: 1.8570
                                                          23%|██▎       | 425/1868 [22:03:06<74:44:49, 186.48s/it] 23%|██▎       | 426/1868 [22:06:13<74:43:00, 186.53s/it]{'loss': 1.857, 'grad_norm': 0.0, 'learning_rate': 1.5460385438972163e-05, 'epoch': 0.23}
? Step 426  Epoch: 0.23  Loss: 1.5657
                                                          23%|██▎       | 426/1868 [22:06:13<74:43:00, 186.53s/it] 23%|██▎       | 427/1868 [22:09:20<74:40:25, 186.56s/it]{'loss': 1.5657, 'grad_norm': 0.0, 'learning_rate': 1.544967880085653e-05, 'epoch': 0.23}
? Step 427  Epoch: 0.23  Loss: 1.4544
                                                          23%|██▎       | 427/1868 [22:09:20<74:40:25, 186.56s/it] 23%|██▎       | 428/1868 [22:12:26<74:37:24, 186.56s/it]{'loss': 1.4544, 'grad_norm': 0.0, 'learning_rate': 1.54389721627409e-05, 'epoch': 0.23}
? Step 428  Epoch: 0.23  Loss: 1.4851
                                                          23%|██▎       | 428/1868 [22:12:26<74:37:24, 186.56s/it] 23%|██▎       | 429/1868 [22:15:33<74:32:32, 186.49s/it]{'loss': 1.4851, 'grad_norm': 0.0, 'learning_rate': 1.542826552462527e-05, 'epoch': 0.23}
? Step 429  Epoch: 0.23  Loss: 1.9546
                                                          23%|██▎       | 429/1868 [22:15:33<74:32:32, 186.49s/it] 23%|██▎       | 430/1868 [22:18:39<74:30:57, 186.55s/it]{'loss': 1.9546, 'grad_norm': 0.0, 'learning_rate': 1.5417558886509637e-05, 'epoch': 0.23}
? Step 430  Epoch: 0.23  Loss: 1.6830
                                                          23%|██▎       | 430/1868 [22:18:39<74:30:57, 186.55s/it] 23%|██▎       | 431/1868 [22:21:46<74:28:30, 186.58s/it]{'loss': 1.683, 'grad_norm': 0.0, 'learning_rate': 1.5406852248394006e-05, 'epoch': 0.23}
? Step 431  Epoch: 0.23  Loss: 1.6028
                                                          23%|██▎       | 431/1868 [22:21:46<74:28:30, 186.58s/it] 23%|██▎       | 432/1868 [22:24:52<74:23:26, 186.49s/it]{'loss': 1.6028, 'grad_norm': 0.0, 'learning_rate': 1.5396145610278375e-05, 'epoch': 0.23}
? Step 432  Epoch: 0.23  Loss: 1.8822
                                                          23%|██▎       | 432/1868 [22:24:52<74:23:26, 186.49s/it] 23%|██▎       | 433/1868 [22:27:59<74:20:24, 186.50s/it]{'loss': 1.8822, 'grad_norm': 0.0, 'learning_rate': 1.5385438972162743e-05, 'epoch': 0.23}
? Step 433  Epoch: 0.23  Loss: 1.7780
                                                          23%|██▎       | 433/1868 [22:27:59<74:20:24, 186.50s/it] 23%|██▎       | 434/1868 [22:31:05<74:19:13, 186.58s/it]{'loss': 1.778, 'grad_norm': 0.0, 'learning_rate': 1.5374732334047112e-05, 'epoch': 0.23}
? Step 434  Epoch: 0.23  Loss: 1.6181
                                                          23%|██▎       | 434/1868 [22:31:05<74:19:13, 186.58s/it] 23%|██▎       | 435/1868 [22:34:12<74:19:00, 186.70s/it]{'loss': 1.6181, 'grad_norm': 0.0, 'learning_rate': 1.536402569593148e-05, 'epoch': 0.23}
? Step 435  Epoch: 0.23  Loss: 1.6118
                                                          23%|██▎       | 435/1868 [22:34:12<74:19:00, 186.70s/it] 23%|██▎       | 436/1868 [22:37:19<74:15:06, 186.67s/it]{'loss': 1.6118, 'grad_norm': 0.0, 'learning_rate': 1.5353319057815846e-05, 'epoch': 0.23}
? Step 436  Epoch: 0.23  Loss: 1.8217
                                                          23%|██▎       | 436/1868 [22:37:19<74:15:06, 186.67s/it] 23%|██▎       | 437/1868 [22:40:26<74:12:01, 186.67s/it]{'loss': 1.8217, 'grad_norm': 0.0, 'learning_rate': 1.5342612419700215e-05, 'epoch': 0.23}
? Step 437  Epoch: 0.23  Loss: 1.4545
                                                          23%|██▎       | 437/1868 [22:40:26<74:12:01, 186.67s/it] 23%|██▎       | 438/1868 [22:43:33<74:10:10, 186.72s/it]{'loss': 1.4545, 'grad_norm': 0.0, 'learning_rate': 1.5331905781584583e-05, 'epoch': 0.23}
? Step 438  Epoch: 0.23  Loss: 1.6519
                                                          23%|██▎       | 438/1868 [22:43:33<74:10:10, 186.72s/it] 24%|██▎       | 439/1868 [22:46:39<74:07:04, 186.72s/it]{'loss': 1.6519, 'grad_norm': 0.0, 'learning_rate': 1.5321199143468952e-05, 'epoch': 0.23}
? Step 439  Epoch: 0.24  Loss: 1.7370
                                                          24%|██▎       | 439/1868 [22:46:39<74:07:04, 186.72s/it] 24%|██▎       | 440/1868 [22:49:46<74:04:00, 186.72s/it]{'loss': 1.737, 'grad_norm': 0.0, 'learning_rate': 1.531049250535332e-05, 'epoch': 0.24}
? Step 440  Epoch: 0.24  Loss: 1.5572
                                                          24%|██▎       | 440/1868 [22:49:46<74:04:00, 186.72s/it] 24%|██▎       | 441/1868 [22:52:53<74:00:27, 186.70s/it]{'loss': 1.5572, 'grad_norm': 0.0, 'learning_rate': 1.529978586723769e-05, 'epoch': 0.24}
? Step 441  Epoch: 0.24  Loss: 1.4672
                                                          24%|██▎       | 441/1868 [22:52:53<74:00:27, 186.70s/it] 24%|██▎       | 442/1868 [22:55:59<73:56:55, 186.69s/it]{'loss': 1.4672, 'grad_norm': 0.0, 'learning_rate': 1.5289079229122058e-05, 'epoch': 0.24}
? Step 442  Epoch: 0.24  Loss: 1.6854
                                                          24%|██▎       | 442/1868 [22:55:59<73:56:55, 186.69s/it] 24%|██▎       | 443/1868 [22:59:06<73:53:18, 186.67s/it]{'loss': 1.6854, 'grad_norm': 0.0, 'learning_rate': 1.5278372591006427e-05, 'epoch': 0.24}
? Step 443  Epoch: 0.24  Loss: 1.3716
                                                          24%|██▎       | 443/1868 [22:59:06<73:53:18, 186.67s/it] 24%|██▍       | 444/1868 [23:02:13<73:51:32, 186.72s/it]{'loss': 1.3716, 'grad_norm': 0.0, 'learning_rate': 1.5267665952890792e-05, 'epoch': 0.24}
? Step 444  Epoch: 0.24  Loss: 1.5655
                                                          24%|██▍       | 444/1868 [23:02:13<73:51:32, 186.72s/it] 24%|██▍       | 445/1868 [23:05:19<73:46:36, 186.65s/it]{'loss': 1.5655, 'grad_norm': 0.0, 'learning_rate': 1.525695931477516e-05, 'epoch': 0.24}
? Step 445  Epoch: 0.24  Loss: 1.5560
                                                          24%|██▍       | 445/1868 [23:05:19<73:46:36, 186.65s/it] 24%|██▍       | 446/1868 [23:08:26<73:44:19, 186.68s/it]{'loss': 1.556, 'grad_norm': 0.0, 'learning_rate': 1.524625267665953e-05, 'epoch': 0.24}
? Step 446  Epoch: 0.24  Loss: 1.8337
                                                          24%|██▍       | 446/1868 [23:08:26<73:44:19, 186.68s/it] 24%|██▍       | 447/1868 [23:11:33<73:41:50, 186.71s/it]{'loss': 1.8337, 'grad_norm': 0.0, 'learning_rate': 1.5235546038543898e-05, 'epoch': 0.24}
? Step 447  Epoch: 0.24  Loss: 1.6804
                                                          24%|██▍       | 447/1868 [23:11:33<73:41:50, 186.71s/it] 24%|██▍       | 448/1868 [23:14:39<73:37:42, 186.66s/it]{'loss': 1.6804, 'grad_norm': 0.0, 'learning_rate': 1.5224839400428267e-05, 'epoch': 0.24}
? Step 448  Epoch: 0.24  Loss: 1.6860
                                                          24%|██▍       | 448/1868 [23:14:39<73:37:42, 186.66s/it] 24%|██▍       | 449/1868 [23:17:46<73:34:50, 186.67s/it]{'loss': 1.686, 'grad_norm': 0.0, 'learning_rate': 1.5214132762312634e-05, 'epoch': 0.24}
? Step 449  Epoch: 0.24  Loss: 1.8367
                                                          24%|██▍       | 449/1868 [23:17:46<73:34:50, 186.67s/it] 24%|██▍       | 450/1868 [23:20:53<73:32:12, 186.69s/it]{'loss': 1.8367, 'grad_norm': 0.0, 'learning_rate': 1.5203426124197002e-05, 'epoch': 0.24}
? Step 450  Epoch: 0.24  Loss: 1.6772
                                                          24%|██▍       | 450/1868 [23:20:53<73:32:12, 186.69s/it] 24%|██▍       | 451/1868 [23:23:59<73:29:20, 186.70s/it]{'loss': 1.6772, 'grad_norm': 0.0, 'learning_rate': 1.5192719486081371e-05, 'epoch': 0.24}
? Step 451  Epoch: 0.24  Loss: 1.4853
                                                          24%|██▍       | 451/1868 [23:23:59<73:29:20, 186.70s/it] 24%|██▍       | 452/1868 [23:27:06<73:26:17, 186.71s/it]{'loss': 1.4853, 'grad_norm': 0.0, 'learning_rate': 1.518201284796574e-05, 'epoch': 0.24}
? Step 452  Epoch: 0.24  Loss: 1.5171
                                                          24%|██▍       | 452/1868 [23:27:06<73:26:17, 186.71s/it] 24%|██▍       | 453/1868 [23:30:13<73:22:39, 186.69s/it]{'loss': 1.5171, 'grad_norm': 0.0, 'learning_rate': 1.5171306209850107e-05, 'epoch': 0.24}
? Step 453  Epoch: 0.24  Loss: 1.6232
                                                          24%|██▍       | 453/1868 [23:30:13<73:22:39, 186.69s/it] 24%|██▍       | 454/1868 [23:33:20<73:20:04, 186.71s/it]{'loss': 1.6232, 'grad_norm': 0.0, 'learning_rate': 1.5160599571734475e-05, 'epoch': 0.24}
? Step 454  Epoch: 0.24  Loss: 1.8811
                                                          24%|██▍       | 454/1868 [23:33:20<73:20:04, 186.71s/it] 24%|██▍       | 455/1868 [23:36:26<73:15:37, 186.65s/it]{'loss': 1.8811, 'grad_norm': 0.0, 'learning_rate': 1.5149892933618844e-05, 'epoch': 0.24}
? Step 455  Epoch: 0.24  Loss: 1.4666
                                                          24%|██▍       | 455/1868 [23:36:26<73:15:37, 186.65s/it] 24%|██▍       | 456/1868 [23:39:33<73:10:44, 186.58s/it]{'loss': 1.4666, 'grad_norm': 0.0, 'learning_rate': 1.5139186295503214e-05, 'epoch': 0.24}
? Step 456  Epoch: 0.24  Loss: 1.4356
                                                          24%|██▍       | 456/1868 [23:39:33<73:10:44, 186.58s/it] 24%|██▍       | 457/1868 [23:42:39<73:10:09, 186.68s/it]{'loss': 1.4356, 'grad_norm': 0.0, 'learning_rate': 1.5128479657387583e-05, 'epoch': 0.24}
? Step 457  Epoch: 0.24  Loss: 1.6265
                                                          24%|██▍       | 457/1868 [23:42:39<73:10:09, 186.68s/it] 25%|██▍       | 458/1868 [23:45:46<73:06:17, 186.65s/it]{'loss': 1.6265, 'grad_norm': 0.0, 'learning_rate': 1.511777301927195e-05, 'epoch': 0.24}
? Step 458  Epoch: 0.25  Loss: 1.4960
                                                          25%|██▍       | 458/1868 [23:45:46<73:06:17, 186.65s/it] 25%|██▍       | 459/1868 [23:48:53<73:02:55, 186.64s/it]{'loss': 1.496, 'grad_norm': 0.0, 'learning_rate': 1.5107066381156319e-05, 'epoch': 0.25}
? Step 459  Epoch: 0.25  Loss: 1.4705
                                                          25%|██▍       | 459/1868 [23:48:53<73:02:55, 186.64s/it] 25%|██▍       | 460/1868 [23:51:59<73:00:14, 186.66s/it]{'loss': 1.4705, 'grad_norm': 0.0, 'learning_rate': 1.5096359743040687e-05, 'epoch': 0.25}
? Step 460  Epoch: 0.25  Loss: 1.7471
                                                          25%|██▍       | 460/1868 [23:51:59<73:00:14, 186.66s/it] 25%|██▍       | 461/1868 [23:55:06<72:56:03, 186.61s/it]{'loss': 1.7471, 'grad_norm': 0.0, 'learning_rate': 1.5085653104925056e-05, 'epoch': 0.25}
? Step 461  Epoch: 0.25  Loss: 1.7883
                                                          25%|██▍       | 461/1868 [23:55:06<72:56:03, 186.61s/it] 25%|██▍       | 462/1868 [23:58:12<72:52:33, 186.60s/it]{'loss': 1.7883, 'grad_norm': 0.0, 'learning_rate': 1.5074946466809423e-05, 'epoch': 0.25}
? Step 462  Epoch: 0.25  Loss: 1.5252
                                                          25%|██▍       | 462/1868 [23:58:12<72:52:33, 186.60s/it] 25%|██▍       | 463/1868 [24:01:19<72:48:52, 186.57s/it]{'loss': 1.5252, 'grad_norm': 0.0, 'learning_rate': 1.5064239828693792e-05, 'epoch': 0.25}
? Step 463  Epoch: 0.25  Loss: 1.5631
                                                          25%|██▍       | 463/1868 [24:01:19<72:48:52, 186.57s/it] 25%|██▍       | 464/1868 [24:04:25<72:44:36, 186.52s/it]{'loss': 1.5631, 'grad_norm': 0.0, 'learning_rate': 1.505353319057816e-05, 'epoch': 0.25}
? Step 464  Epoch: 0.25  Loss: 1.5129
                                                          25%|██▍       | 464/1868 [24:04:25<72:44:36, 186.52s/it] 25%|██▍       | 465/1868 [24:07:32<72:41:02, 186.50s/it]{'loss': 1.5129, 'grad_norm': 0.0, 'learning_rate': 1.5042826552462529e-05, 'epoch': 0.25}
? Step 465  Epoch: 0.25  Loss: 1.4959
                                                          25%|██▍       | 465/1868 [24:07:32<72:41:02, 186.50s/it] 25%|██▍       | 466/1868 [24:10:39<72:39:49, 186.58s/it]{'loss': 1.4959, 'grad_norm': 0.0, 'learning_rate': 1.5032119914346896e-05, 'epoch': 0.25}
? Step 466  Epoch: 0.25  Loss: 1.6222
                                                          25%|██▍       | 466/1868 [24:10:39<72:39:49, 186.58s/it] 25%|██▌       | 467/1868 [24:13:45<72:36:44, 186.58s/it]{'loss': 1.6222, 'grad_norm': 0.0, 'learning_rate': 1.5021413276231265e-05, 'epoch': 0.25}
? Step 467  Epoch: 0.25  Loss: 1.6016
                                                          25%|██▌       | 467/1868 [24:13:45<72:36:44, 186.58s/it] 25%|██▌       | 468/1868 [24:16:52<72:34:25, 186.62s/it]{'loss': 1.6016, 'grad_norm': 0.0, 'learning_rate': 1.5010706638115633e-05, 'epoch': 0.25}
? Step 468  Epoch: 0.25  Loss: 1.5679
                                                          25%|██▌       | 468/1868 [24:16:52<72:34:25, 186.62s/it] 25%|██▌       | 469/1868 [24:19:58<72:30:39, 186.59s/it]{'loss': 1.5679, 'grad_norm': 0.0, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.25}
? Step 469  Epoch: 0.25  Loss: 1.7416
                                                          25%|██▌       | 469/1868 [24:19:58<72:30:39, 186.59s/it] 25%|██▌       | 470/1868 [24:23:05<72:28:03, 186.61s/it]{'loss': 1.7416, 'grad_norm': 0.0, 'learning_rate': 1.4989293361884369e-05, 'epoch': 0.25}
? Step 470  Epoch: 0.25  Loss: 1.5341
                                                          25%|██▌       | 470/1868 [24:23:05<72:28:03, 186.61s/it] 25%|██▌       | 471/1868 [24:26:12<72:24:57, 186.61s/it]{'loss': 1.5341, 'grad_norm': 0.0, 'learning_rate': 1.4978586723768738e-05, 'epoch': 0.25}
? Step 471  Epoch: 0.25  Loss: 1.4333
                                                          25%|██▌       | 471/1868 [24:26:12<72:24:57, 186.61s/it] 25%|██▌       | 472/1868 [24:29:18<72:20:25, 186.55s/it]{'loss': 1.4333, 'grad_norm': 0.0, 'learning_rate': 1.4967880085653106e-05, 'epoch': 0.25}
? Step 472  Epoch: 0.25  Loss: 1.6059
                                                          25%|██▌       | 472/1868 [24:29:18<72:20:25, 186.55s/it] 25%|██▌       | 473/1868 [24:32:25<72:17:27, 186.56s/it]{'loss': 1.6059, 'grad_norm': 0.0, 'learning_rate': 1.4957173447537473e-05, 'epoch': 0.25}
? Step 473  Epoch: 0.25  Loss: 1.5181
                                                          25%|██▌       | 473/1868 [24:32:25<72:17:27, 186.56s/it] 25%|██▌       | 474/1868 [24:35:31<72:14:39, 186.57s/it]{'loss': 1.5181, 'grad_norm': 0.0, 'learning_rate': 1.4946466809421842e-05, 'epoch': 0.25}
? Step 474  Epoch: 0.25  Loss: 1.8224
                                                          25%|██▌       | 474/1868 [24:35:31<72:14:39, 186.57s/it] 25%|██▌       | 475/1868 [24:38:38<72:09:37, 186.49s/it]{'loss': 1.8224, 'grad_norm': 0.0, 'learning_rate': 1.493576017130621e-05, 'epoch': 0.25}
? Step 475  Epoch: 0.25  Loss: 1.6916
                                                          25%|██▌       | 475/1868 [24:38:38<72:09:37, 186.49s/it] 25%|██▌       | 476/1868 [24:41:44<72:06:05, 186.47s/it]{'loss': 1.6916, 'grad_norm': 0.0, 'learning_rate': 1.492505353319058e-05, 'epoch': 0.25}
? Step 476  Epoch: 0.25  Loss: 1.6631
                                                          25%|██▌       | 476/1868 [24:41:44<72:06:05, 186.47s/it] 26%|██▌       | 477/1868 [24:44:50<72:02:42, 186.46s/it]{'loss': 1.6631, 'grad_norm': 0.0, 'learning_rate': 1.4914346895074946e-05, 'epoch': 0.25}
? Step 477  Epoch: 0.26  Loss: 1.5794
                                                          26%|██▌       | 477/1868 [24:44:50<72:02:42, 186.46s/it] 26%|██▌       | 478/1868 [24:47:57<72:02:03, 186.56s/it]{'loss': 1.5794, 'grad_norm': 0.0, 'learning_rate': 1.4903640256959315e-05, 'epoch': 0.26}
? Step 478  Epoch: 0.26  Loss: 1.6302
                                                          26%|██▌       | 478/1868 [24:47:57<72:02:03, 186.56s/it] 26%|██▌       | 479/1868 [24:51:04<71:57:57, 186.52s/it]{'loss': 1.6302, 'grad_norm': 0.0, 'learning_rate': 1.4892933618843684e-05, 'epoch': 0.26}
? Step 479  Epoch: 0.26  Loss: 1.4485
                                                          26%|██▌       | 479/1868 [24:51:04<71:57:57, 186.52s/it] 26%|██▌       | 480/1868 [24:54:10<71:55:41, 186.56s/it]{'loss': 1.4485, 'grad_norm': 0.0, 'learning_rate': 1.4882226980728052e-05, 'epoch': 0.26}
? Step 480  Epoch: 0.26  Loss: 1.5453
                                                          26%|██▌       | 480/1868 [24:54:10<71:55:41, 186.56s/it] 26%|██▌       | 481/1868 [24:57:17<71:53:07, 186.58s/it]{'loss': 1.5453, 'grad_norm': 0.0, 'learning_rate': 1.487152034261242e-05, 'epoch': 0.26}
? Step 481  Epoch: 0.26  Loss: 1.5093
                                                          26%|██▌       | 481/1868 [24:57:17<71:53:07, 186.58s/it] 26%|██▌       | 482/1868 [25:00:23<71:48:14, 186.50s/it]{'loss': 1.5093, 'grad_norm': 0.0, 'learning_rate': 1.4860813704496788e-05, 'epoch': 0.26}
? Step 482  Epoch: 0.26  Loss: 1.6572
                                                          26%|██▌       | 482/1868 [25:00:23<71:48:14, 186.50s/it] 26%|██▌       | 483/1868 [25:03:30<71:45:04, 186.50s/it]{'loss': 1.6572, 'grad_norm': 0.0, 'learning_rate': 1.4850107066381158e-05, 'epoch': 0.26}
? Step 483  Epoch: 0.26  Loss: 1.4688
                                                          26%|██▌       | 483/1868 [25:03:30<71:45:04, 186.50s/it] 26%|██▌       | 484/1868 [25:06:36<71:41:55, 186.50s/it]{'loss': 1.4688, 'grad_norm': 0.0, 'learning_rate': 1.4839400428265527e-05, 'epoch': 0.26}
? Step 484  Epoch: 0.26  Loss: 1.5744
                                                          26%|██▌       | 484/1868 [25:06:36<71:41:55, 186.50s/it] 26%|██▌       | 485/1868 [25:09:43<71:38:14, 186.47s/it]{'loss': 1.5744, 'grad_norm': 0.0, 'learning_rate': 1.4828693790149896e-05, 'epoch': 0.26}
? Step 485  Epoch: 0.26  Loss: 1.7582
                                                          26%|██▌       | 485/1868 [25:09:43<71:38:14, 186.47s/it] 26%|██▌       | 486/1868 [25:12:49<71:35:28, 186.49s/it]{'loss': 1.7582, 'grad_norm': 0.0, 'learning_rate': 1.4817987152034263e-05, 'epoch': 0.26}
? Step 486  Epoch: 0.26  Loss: 1.7436
                                                          26%|██▌       | 486/1868 [25:12:49<71:35:28, 186.49s/it] 26%|██▌       | 487/1868 [25:15:56<71:33:37, 186.54s/it]{'loss': 1.7436, 'grad_norm': 0.0, 'learning_rate': 1.4807280513918631e-05, 'epoch': 0.26}
? Step 487  Epoch: 0.26  Loss: 1.6583
                                                          26%|██▌       | 487/1868 [25:15:56<71:33:37, 186.54s/it] 26%|██▌       | 488/1868 [25:19:02<71:28:44, 186.47s/it]{'loss': 1.6583, 'grad_norm': 0.0, 'learning_rate': 1.4796573875803e-05, 'epoch': 0.26}
? Step 488  Epoch: 0.26  Loss: 1.7453
                                                          26%|██▌       | 488/1868 [25:19:02<71:28:44, 186.47s/it] 26%|██▌       | 489/1868 [25:22:09<71:26:54, 186.52s/it]{'loss': 1.7453, 'grad_norm': 0.0, 'learning_rate': 1.4785867237687369e-05, 'epoch': 0.26}
? Step 489  Epoch: 0.26  Loss: 1.7833
                                                          26%|██▌       | 489/1868 [25:22:09<71:26:54, 186.52s/it] 26%|██▌       | 490/1868 [25:25:15<71:23:54, 186.53s/it]{'loss': 1.7833, 'grad_norm': 0.0, 'learning_rate': 1.4775160599571736e-05, 'epoch': 0.26}
? Step 490  Epoch: 0.26  Loss: 1.4187
                                                          26%|██▌       | 490/1868 [25:25:15<71:23:54, 186.53s/it] 26%|██▋       | 491/1868 [25:28:22<71:22:19, 186.59s/it]{'loss': 1.4187, 'grad_norm': 0.0, 'learning_rate': 1.4764453961456104e-05, 'epoch': 0.26}
? Step 491  Epoch: 0.26  Loss: 1.3995
                                                          26%|██▋       | 491/1868 [25:28:22<71:22:19, 186.59s/it] 26%|██▋       | 492/1868 [25:31:29<71:19:50, 186.62s/it]{'loss': 1.3995, 'grad_norm': 0.0, 'learning_rate': 1.4753747323340473e-05, 'epoch': 0.26}
? Step 492  Epoch: 0.26  Loss: 1.6234
                                                          26%|██▋       | 492/1868 [25:31:29<71:19:50, 186.62s/it] 26%|██▋       | 493/1868 [25:34:35<71:15:46, 186.58s/it]{'loss': 1.6234, 'grad_norm': 0.0, 'learning_rate': 1.4743040685224842e-05, 'epoch': 0.26}
? Step 493  Epoch: 0.26  Loss: 1.7036
                                                          26%|██▋       | 493/1868 [25:34:35<71:15:46, 186.58s/it] 26%|██▋       | 494/1868 [25:37:42<71:12:50, 186.59s/it]{'loss': 1.7036, 'grad_norm': 0.0, 'learning_rate': 1.4732334047109209e-05, 'epoch': 0.26}
? Step 494  Epoch: 0.26  Loss: 1.4893
                                                          26%|██▋       | 494/1868 [25:37:42<71:12:50, 186.59s/it] 26%|██▋       | 495/1868 [25:40:48<71:08:18, 186.53s/it]{'loss': 1.4893, 'grad_norm': 0.0, 'learning_rate': 1.4721627408993577e-05, 'epoch': 0.26}
? Step 495  Epoch: 0.26  Loss: 1.5664
                                                          26%|██▋       | 495/1868 [25:40:48<71:08:18, 186.53s/it] 27%|██▋       | 496/1868 [25:43:55<71:06:38, 186.59s/it]{'loss': 1.5664, 'grad_norm': 0.0, 'learning_rate': 1.4710920770877946e-05, 'epoch': 0.26}
? Step 496  Epoch: 0.27  Loss: 1.6380
                                                          27%|██▋       | 496/1868 [25:43:55<71:06:38, 186.59s/it] 27%|██▋       | 497/1868 [25:47:01<71:01:57, 186.52s/it]{'loss': 1.638, 'grad_norm': 0.0, 'learning_rate': 1.4700214132762313e-05, 'epoch': 0.27}
? Step 497  Epoch: 0.27  Loss: 1.5496
                                                          27%|██▋       | 497/1868 [25:47:01<71:01:57, 186.52s/it] 27%|██▋       | 498/1868 [25:50:08<70:57:38, 186.47s/it]{'loss': 1.5496, 'grad_norm': 0.0, 'learning_rate': 1.4689507494646682e-05, 'epoch': 0.27}
? Step 498  Epoch: 0.27  Loss: 1.5405
                                                          27%|██▋       | 498/1868 [25:50:08<70:57:38, 186.47s/it] 27%|██▋       | 499/1868 [25:53:14<70:55:45, 186.52s/it]{'loss': 1.5405, 'grad_norm': 0.0, 'learning_rate': 1.467880085653105e-05, 'epoch': 0.27}
? Step 499  Epoch: 0.27  Loss: 1.5171
                                                          27%|██▋       | 499/1868 [25:53:14<70:55:45, 186.52s/it] 27%|██▋       | 500/1868 [25:56:21<70:53:27, 186.56s/it]{'loss': 1.5171, 'grad_norm': 0.0, 'learning_rate': 1.4668094218415419e-05, 'epoch': 0.27}
? Step 500  Epoch: 0.27  Loss: 1.4978
                                                          27%|██▋       | 500/1868 [25:56:21<70:53:27, 186.56s/it]/home/dev25-01/mistral-env/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 27%|██▋       | 501/1868 [25:59:28<70:50:47, 186.57s/it]{'loss': 1.4978, 'grad_norm': 0.0, 'learning_rate': 1.4657387580299786e-05, 'epoch': 0.27}
? Step 501  Epoch: 0.27  Loss: 1.8123
                                                          27%|██▋       | 501/1868 [25:59:28<70:50:47, 186.57s/it] 27%|██▋       | 502/1868 [26:02:34<70:47:30, 186.57s/it]{'loss': 1.8123, 'grad_norm': 0.0, 'learning_rate': 1.4646680942184155e-05, 'epoch': 0.27}
? Step 502  Epoch: 0.27  Loss: 1.5387
                                                          27%|██▋       | 502/1868 [26:02:34<70:47:30, 186.57s/it] 27%|██▋       | 503/1868 [26:05:41<70:44:52, 186.59s/it]{'loss': 1.5387, 'grad_norm': 0.0, 'learning_rate': 1.4635974304068523e-05, 'epoch': 0.27}
? Step 503  Epoch: 0.27  Loss: 1.4509
                                                          27%|██▋       | 503/1868 [26:05:41<70:44:52, 186.59s/it] 27%|██▋       | 504/1868 [26:08:47<70:39:50, 186.50s/it]{'loss': 1.4509, 'grad_norm': 0.0, 'learning_rate': 1.4625267665952892e-05, 'epoch': 0.27}
? Step 504  Epoch: 0.27  Loss: 1.5663
                                                          27%|██▋       | 504/1868 [26:08:47<70:39:50, 186.50s/it] 27%|██▋       | 505/1868 [26:11:54<70:37:15, 186.53s/it]{'loss': 1.5663, 'grad_norm': 0.0, 'learning_rate': 1.4614561027837259e-05, 'epoch': 0.27}
? Step 505  Epoch: 0.27  Loss: 1.5389
                                                          27%|██▋       | 505/1868 [26:11:54<70:37:15, 186.53s/it] 27%|██▋       | 506/1868 [26:15:00<70:33:32, 186.50s/it]{'loss': 1.5389, 'grad_norm': 0.0, 'learning_rate': 1.4603854389721628e-05, 'epoch': 0.27}
? Step 506  Epoch: 0.27  Loss: 1.5251
                                                          27%|██▋       | 506/1868 [26:15:00<70:33:32, 186.50s/it] 27%|██▋       | 507/1868 [26:18:07<70:31:51, 186.56s/it]{'loss': 1.5251, 'grad_norm': 0.0, 'learning_rate': 1.4593147751605996e-05, 'epoch': 0.27}
? Step 507  Epoch: 0.27  Loss: 1.7611
                                                          27%|██▋       | 507/1868 [26:18:07<70:31:51, 186.56s/it] 27%|██▋       | 508/1868 [26:21:13<70:29:07, 186.58s/it]{'loss': 1.7611, 'grad_norm': 0.0, 'learning_rate': 1.4582441113490365e-05, 'epoch': 0.27}
? Step 508  Epoch: 0.27  Loss: 1.6553
                                                          27%|██▋       | 508/1868 [26:21:13<70:29:07, 186.58s/it] 27%|██▋       | 509/1868 [26:24:20<70:25:33, 186.56s/it]{'loss': 1.6553, 'grad_norm': 0.0, 'learning_rate': 1.4571734475374732e-05, 'epoch': 0.27}
? Step 509  Epoch: 0.27  Loss: 1.4240
                                                          27%|██▋       | 509/1868 [26:24:20<70:25:33, 186.56s/it] 27%|██▋       | 510/1868 [26:27:27<70:23:46, 186.62s/it]{'loss': 1.424, 'grad_norm': 0.0, 'learning_rate': 1.45610278372591e-05, 'epoch': 0.27}
? Step 510  Epoch: 0.27  Loss: 1.5450
                                                          27%|██▋       | 510/1868 [26:27:27<70:23:46, 186.62s/it] 27%|██▋       | 511/1868 [26:30:33<70:20:40, 186.62s/it]{'loss': 1.545, 'grad_norm': 0.0, 'learning_rate': 1.4550321199143471e-05, 'epoch': 0.27}
? Step 511  Epoch: 0.27  Loss: 1.7176
                                                          27%|██▋       | 511/1868 [26:30:33<70:20:40, 186.62s/it] 27%|██▋       | 512/1868 [26:33:40<70:16:29, 186.57s/it]{'loss': 1.7176, 'grad_norm': 0.0, 'learning_rate': 1.453961456102784e-05, 'epoch': 0.27}
? Step 512  Epoch: 0.27  Loss: 1.9128
                                                          27%|██▋       | 512/1868 [26:33:40<70:16:29, 186.57s/it] 27%|██▋       | 513/1868 [26:36:46<70:13:16, 186.57s/it]{'loss': 1.9128, 'grad_norm': 0.0, 'learning_rate': 1.4528907922912208e-05, 'epoch': 0.27}
? Step 513  Epoch: 0.27  Loss: 1.4615
                                                          27%|██▋       | 513/1868 [26:36:46<70:13:16, 186.57s/it] 28%|██▊       | 514/1868 [26:39:53<70:11:57, 186.65s/it]{'loss': 1.4615, 'grad_norm': 0.0, 'learning_rate': 1.4518201284796575e-05, 'epoch': 0.27}
? Step 514  Epoch: 0.28  Loss: 1.4185
                                                          28%|██▊       | 514/1868 [26:39:53<70:11:57, 186.65s/it] 28%|██▊       | 515/1868 [26:43:00<70:09:05, 186.66s/it]{'loss': 1.4185, 'grad_norm': 0.0, 'learning_rate': 1.4507494646680944e-05, 'epoch': 0.28}
? Step 515  Epoch: 0.28  Loss: 1.6741
                                                          28%|██▊       | 515/1868 [26:43:00<70:09:05, 186.66s/it] 28%|██▊       | 516/1868 [26:46:06<70:04:04, 186.57s/it]{'loss': 1.6741, 'grad_norm': 0.0, 'learning_rate': 1.4496788008565313e-05, 'epoch': 0.28}
? Step 516  Epoch: 0.28  Loss: 1.4840
                                                          28%|██▊       | 516/1868 [26:46:06<70:04:04, 186.57s/it] 28%|██▊       | 517/1868 [26:49:13<70:01:51, 186.61s/it]{'loss': 1.484, 'grad_norm': 0.0, 'learning_rate': 1.4486081370449681e-05, 'epoch': 0.28}
? Step 517  Epoch: 0.28  Loss: 1.3236
                                                          28%|██▊       | 517/1868 [26:49:13<70:01:51, 186.61s/it] 28%|██▊       | 518/1868 [26:52:19<69:57:59, 186.58s/it]{'loss': 1.3236, 'grad_norm': 0.0, 'learning_rate': 1.4475374732334048e-05, 'epoch': 0.28}
? Step 518  Epoch: 0.28  Loss: 1.5380
                                                          28%|██▊       | 518/1868 [26:52:19<69:57:59, 186.58s/it] 28%|██▊       | 519/1868 [26:55:26<69:57:43, 186.70s/it]{'loss': 1.538, 'grad_norm': 0.0, 'learning_rate': 1.4464668094218417e-05, 'epoch': 0.28}
? Step 519  Epoch: 0.28  Loss: 1.4571
                                                          28%|██▊       | 519/1868 [26:55:26<69:57:43, 186.70s/it] 28%|██▊       | 520/1868 [26:58:33<69:53:02, 186.63s/it]{'loss': 1.4571, 'grad_norm': 0.0, 'learning_rate': 1.4453961456102786e-05, 'epoch': 0.28}
? Step 520  Epoch: 0.28  Loss: 1.5881
                                                          28%|██▊       | 520/1868 [26:58:33<69:53:02, 186.63s/it] 28%|██▊       | 521/1868 [27:01:39<69:47:40, 186.53s/it]{'loss': 1.5881, 'grad_norm': 0.0, 'learning_rate': 1.4443254817987153e-05, 'epoch': 0.28}
? Step 521  Epoch: 0.28  Loss: 1.7086
                                                          28%|██▊       | 521/1868 [27:01:39<69:47:40, 186.53s/it] 28%|██▊       | 522/1868 [27:04:46<69:43:47, 186.50s/it]{'loss': 1.7086, 'grad_norm': 0.0, 'learning_rate': 1.4432548179871521e-05, 'epoch': 0.28}
? Step 522  Epoch: 0.28  Loss: 1.9050
                                                          28%|██▊       | 522/1868 [27:04:46<69:43:47, 186.50s/it] 28%|██▊       | 523/1868 [27:07:52<69:42:29, 186.58s/it]{'loss': 1.905, 'grad_norm': 0.0, 'learning_rate': 1.442184154175589e-05, 'epoch': 0.28}
? Step 523  Epoch: 0.28  Loss: 1.3585
                                                          28%|██▊       | 523/1868 [27:07:52<69:42:29, 186.58s/it] 28%|██▊       | 524/1868 [27:10:59<69:39:13, 186.57s/it]{'loss': 1.3585, 'grad_norm': 0.0, 'learning_rate': 1.4411134903640259e-05, 'epoch': 0.28}
? Step 524  Epoch: 0.28  Loss: 1.5740
                                                          28%|██▊       | 524/1868 [27:10:59<69:39:13, 186.57s/it] 28%|██▊       | 525/1868 [27:14:06<69:37:18, 186.63s/it]{'loss': 1.574, 'grad_norm': 0.0, 'learning_rate': 1.4400428265524626e-05, 'epoch': 0.28}
? Step 525  Epoch: 0.28  Loss: 1.5906
                                                          28%|██▊       | 525/1868 [27:14:06<69:37:18, 186.63s/it] 28%|██▊       | 526/1868 [27:17:12<69:35:37, 186.69s/it]{'loss': 1.5906, 'grad_norm': 0.0, 'learning_rate': 1.4389721627408994e-05, 'epoch': 0.28}
? Step 526  Epoch: 0.28  Loss: 1.5151
                                                          28%|██▊       | 526/1868 [27:17:12<69:35:37, 186.69s/it] 28%|██▊       | 527/1868 [27:20:19<69:32:26, 186.69s/it]{'loss': 1.5151, 'grad_norm': 0.0, 'learning_rate': 1.4379014989293363e-05, 'epoch': 0.28}
? Step 527  Epoch: 0.28  Loss: 1.4657
                                                          28%|██▊       | 527/1868 [27:20:19<69:32:26, 186.69s/it] 28%|██▊       | 528/1868 [27:23:26<69:29:54, 186.71s/it]{'loss': 1.4657, 'grad_norm': 0.0, 'learning_rate': 1.4368308351177732e-05, 'epoch': 0.28}
? Step 528  Epoch: 0.28  Loss: 1.7918
                                                          28%|██▊       | 528/1868 [27:23:26<69:29:54, 186.71s/it] 28%|██▊       | 529/1868 [27:26:32<69:25:13, 186.64s/it]{'loss': 1.7918, 'grad_norm': 0.0, 'learning_rate': 1.4357601713062099e-05, 'epoch': 0.28}
? Step 529  Epoch: 0.28  Loss: 1.4425
                                                          28%|██▊       | 529/1868 [27:26:32<69:25:13, 186.64s/it] 28%|██▊       | 530/1868 [27:29:39<69:22:44, 186.67s/it]{'loss': 1.4425, 'grad_norm': 0.0, 'learning_rate': 1.4346895074946467e-05, 'epoch': 0.28}
? Step 530  Epoch: 0.28  Loss: 1.8762
                                                          28%|██▊       | 530/1868 [27:29:39<69:22:44, 186.67s/it] 28%|██▊       | 531/1868 [27:32:46<69:19:51, 186.68s/it]{'loss': 1.8762, 'grad_norm': 0.0, 'learning_rate': 1.4336188436830836e-05, 'epoch': 0.28}
? Step 531  Epoch: 0.28  Loss: 1.9354
                                                          28%|██▊       | 531/1868 [27:32:46<69:19:51, 186.68s/it] 28%|██▊       | 532/1868 [27:35:52<69:15:07, 186.61s/it]{'loss': 1.9354, 'grad_norm': 0.0, 'learning_rate': 1.4325481798715205e-05, 'epoch': 0.28}
? Step 532  Epoch: 0.28  Loss: 1.8520
                                                          28%|██▊       | 532/1868 [27:35:52<69:15:07, 186.61s/it] 29%|██▊       | 533/1868 [27:38:59<69:14:00, 186.70s/it]{'loss': 1.852, 'grad_norm': 0.0, 'learning_rate': 1.4314775160599572e-05, 'epoch': 0.28}
? Step 533  Epoch: 0.29  Loss: 1.5164
                                                          29%|██▊       | 533/1868 [27:38:59<69:14:00, 186.70s/it] 29%|██▊       | 534/1868 [27:42:06<69:09:44, 186.64s/it]{'loss': 1.5164, 'grad_norm': 0.0, 'learning_rate': 1.430406852248394e-05, 'epoch': 0.29}
? Step 534  Epoch: 0.29  Loss: 1.6276
                                                          29%|██▊       | 534/1868 [27:42:06<69:09:44, 186.64s/it] 29%|██▊       | 535/1868 [27:45:12<69:06:22, 186.63s/it]{'loss': 1.6276, 'grad_norm': 0.0, 'learning_rate': 1.4293361884368309e-05, 'epoch': 0.29}
? Step 535  Epoch: 0.29  Loss: 1.3970
                                                          29%|██▊       | 535/1868 [27:45:12<69:06:22, 186.63s/it] 29%|██▊       | 536/1868 [27:48:19<69:05:34, 186.74s/it]{'loss': 1.397, 'grad_norm': 0.0, 'learning_rate': 1.4282655246252678e-05, 'epoch': 0.29}
? Step 536  Epoch: 0.29  Loss: 1.4606
                                                          29%|██▊       | 536/1868 [27:48:19<69:05:34, 186.74s/it] 29%|██▊       | 537/1868 [27:51:26<69:01:44, 186.71s/it]{'loss': 1.4606, 'grad_norm': 0.0, 'learning_rate': 1.4271948608137045e-05, 'epoch': 0.29}
? Step 537  Epoch: 0.29  Loss: 1.5672
                                                          29%|██▊       | 537/1868 [27:51:26<69:01:44, 186.71s/it] 29%|██▉       | 538/1868 [27:54:32<68:56:52, 186.63s/it]{'loss': 1.5672, 'grad_norm': 0.0, 'learning_rate': 1.4261241970021415e-05, 'epoch': 0.29}
? Step 538  Epoch: 0.29  Loss: 1.3949
                                                          29%|██▉       | 538/1868 [27:54:32<68:56:52, 186.63s/it] 29%|██▉       | 539/1868 [27:57:39<68:53:37, 186.62s/it]{'loss': 1.3949, 'grad_norm': 0.0, 'learning_rate': 1.4250535331905784e-05, 'epoch': 0.29}
? Step 539  Epoch: 0.29  Loss: 1.6268
                                                          29%|██▉       | 539/1868 [27:57:39<68:53:37, 186.62s/it] 29%|██▉       | 540/1868 [28:00:46<68:50:47, 186.63s/it]{'loss': 1.6268, 'grad_norm': 0.0, 'learning_rate': 1.4239828693790152e-05, 'epoch': 0.29}
? Step 540  Epoch: 0.29  Loss: 1.5197
                                                          29%|██▉       | 540/1868 [28:00:46<68:50:47, 186.63s/it] 29%|██▉       | 541/1868 [28:03:52<68:47:50, 186.64s/it]{'loss': 1.5197, 'grad_norm': 0.0, 'learning_rate': 1.4229122055674521e-05, 'epoch': 0.29}
? Step 541  Epoch: 0.29  Loss: 1.5360
                                                          29%|██▉       | 541/1868 [28:03:52<68:47:50, 186.64s/it] 29%|██▉       | 542/1868 [28:06:59<68:44:58, 186.65s/it]{'loss': 1.536, 'grad_norm': 0.0, 'learning_rate': 1.4218415417558888e-05, 'epoch': 0.29}
? Step 542  Epoch: 0.29  Loss: 1.4124
                                                          29%|██▉       | 542/1868 [28:06:59<68:44:58, 186.65s/it] 29%|██▉       | 543/1868 [28:10:06<68:43:14, 186.71s/it]{'loss': 1.4124, 'grad_norm': 0.0, 'learning_rate': 1.4207708779443257e-05, 'epoch': 0.29}
? Step 543  Epoch: 0.29  Loss: 1.4986
                                                          29%|██▉       | 543/1868 [28:10:06<68:43:14, 186.71s/it] 29%|██▉       | 544/1868 [28:13:12<68:39:09, 186.67s/it]{'loss': 1.4986, 'grad_norm': 0.0, 'learning_rate': 1.4197002141327625e-05, 'epoch': 0.29}
? Step 544  Epoch: 0.29  Loss: 1.2798
                                                          29%|██▉       | 544/1868 [28:13:12<68:39:09, 186.67s/it] 29%|██▉       | 545/1868 [28:16:19<68:34:53, 186.62s/it]{'loss': 1.2798, 'grad_norm': 0.0, 'learning_rate': 1.4186295503211992e-05, 'epoch': 0.29}
? Step 545  Epoch: 0.29  Loss: 1.6619
                                                          29%|██▉       | 545/1868 [28:16:19<68:34:53, 186.62s/it] 29%|██▉       | 546/1868 [28:19:26<68:32:43, 186.66s/it]{'loss': 1.6619, 'grad_norm': 0.0, 'learning_rate': 1.4175588865096361e-05, 'epoch': 0.29}
? Step 546  Epoch: 0.29  Loss: 1.5824
                                                          29%|██▉       | 546/1868 [28:19:26<68:32:43, 186.66s/it] 29%|██▉       | 547/1868 [28:22:32<68:28:58, 186.63s/it]{'loss': 1.5824, 'grad_norm': 0.0, 'learning_rate': 1.416488222698073e-05, 'epoch': 0.29}
? Step 547  Epoch: 0.29  Loss: 1.5913
                                                          29%|██▉       | 547/1868 [28:22:32<68:28:58, 186.63s/it] 29%|██▉       | 548/1868 [28:25:39<68:25:36, 186.62s/it]{'loss': 1.5913, 'grad_norm': 0.0, 'learning_rate': 1.4154175588865098e-05, 'epoch': 0.29}
? Step 548  Epoch: 0.29  Loss: 1.5081
                                                          29%|██▉       | 548/1868 [28:25:39<68:25:36, 186.62s/it] 29%|██▉       | 549/1868 [28:28:45<68:21:38, 186.58s/it]{'loss': 1.5081, 'grad_norm': 0.0, 'learning_rate': 1.4143468950749465e-05, 'epoch': 0.29}
? Step 549  Epoch: 0.29  Loss: 1.6588
                                                          29%|██▉       | 549/1868 [28:28:45<68:21:38, 186.58s/it] 29%|██▉       | 550/1868 [28:31:52<68:18:08, 186.56s/it]{'loss': 1.6588, 'grad_norm': 0.0, 'learning_rate': 1.4132762312633834e-05, 'epoch': 0.29}
? Step 550  Epoch: 0.29  Loss: 1.5996
                                                          29%|██▉       | 550/1868 [28:31:52<68:18:08, 186.56s/it] 29%|██▉       | 551/1868 [28:34:58<68:15:41, 186.59s/it]{'loss': 1.5996, 'grad_norm': 0.0, 'learning_rate': 1.4122055674518203e-05, 'epoch': 0.29}
? Step 551  Epoch: 0.29  Loss: 1.6676
                                                          29%|██▉       | 551/1868 [28:34:58<68:15:41, 186.59s/it] 30%|██▉       | 552/1868 [28:38:05<68:13:49, 186.65s/it]{'loss': 1.6676, 'grad_norm': 0.0, 'learning_rate': 1.4111349036402571e-05, 'epoch': 0.29}
? Step 552  Epoch: 0.30  Loss: 1.5472
                                                          30%|██▉       | 552/1868 [28:38:05<68:13:49, 186.65s/it] 30%|██▉       | 553/1868 [28:41:12<68:09:39, 186.60s/it]{'loss': 1.5472, 'grad_norm': 0.0, 'learning_rate': 1.4100642398286938e-05, 'epoch': 0.3}
? Step 553  Epoch: 0.30  Loss: 1.6857
                                                          30%|██▉       | 553/1868 [28:41:12<68:09:39, 186.60s/it] 30%|██▉       | 554/1868 [28:44:19<68:08:11, 186.68s/it]{'loss': 1.6857, 'grad_norm': 0.0, 'learning_rate': 1.4089935760171307e-05, 'epoch': 0.3}
? Step 554  Epoch: 0.30  Loss: 1.6185
                                                          30%|██▉       | 554/1868 [28:44:19<68:08:11, 186.68s/it] 30%|██▉       | 555/1868 [28:47:25<68:04:12, 186.64s/it]{'loss': 1.6185, 'grad_norm': 0.0, 'learning_rate': 1.4079229122055676e-05, 'epoch': 0.3}
? Step 555  Epoch: 0.30  Loss: 1.4986
                                                          30%|██▉       | 555/1868 [28:47:25<68:04:12, 186.64s/it] 30%|██▉       | 556/1868 [28:50:32<68:02:33, 186.70s/it]{'loss': 1.4986, 'grad_norm': 0.0, 'learning_rate': 1.4068522483940044e-05, 'epoch': 0.3}
? Step 556  Epoch: 0.30  Loss: 1.5259
                                                          30%|██▉       | 556/1868 [28:50:32<68:02:33, 186.70s/it] 30%|██▉       | 557/1868 [28:53:39<67:59:22, 186.70s/it]{'loss': 1.5259, 'grad_norm': 0.0, 'learning_rate': 1.4057815845824411e-05, 'epoch': 0.3}
? Step 557  Epoch: 0.30  Loss: 1.7163
                                                          30%|██▉       | 557/1868 [28:53:39<67:59:22, 186.70s/it] 30%|██▉       | 558/1868 [28:56:45<67:55:58, 186.69s/it]{'loss': 1.7163, 'grad_norm': 0.0, 'learning_rate': 1.404710920770878e-05, 'epoch': 0.3}
? Step 558  Epoch: 0.30  Loss: 1.5799
                                                          30%|██▉       | 558/1868 [28:56:45<67:55:58, 186.69s/it] 30%|██▉       | 559/1868 [28:59:52<67:53:29, 186.71s/it]{'loss': 1.5799, 'grad_norm': 0.0, 'learning_rate': 1.4036402569593149e-05, 'epoch': 0.3}
? Step 559  Epoch: 0.30  Loss: 1.6528
                                                          30%|██▉       | 559/1868 [28:59:52<67:53:29, 186.71s/it] 30%|██▉       | 560/1868 [29:02:59<67:51:30, 186.77s/it]{'loss': 1.6528, 'grad_norm': 0.0, 'learning_rate': 1.4025695931477517e-05, 'epoch': 0.3}
? Step 560  Epoch: 0.30  Loss: 1.5588
                                                          30%|██▉       | 560/1868 [29:02:59<67:51:30, 186.77s/it] 30%|███       | 561/1868 [29:06:06<67:47:06, 186.71s/it]{'loss': 1.5588, 'grad_norm': 0.0, 'learning_rate': 1.4014989293361884e-05, 'epoch': 0.3}
? Step 561  Epoch: 0.30  Loss: 1.9074
                                                          30%|███       | 561/1868 [29:06:06<67:47:06, 186.71s/it] 30%|███       | 562/1868 [29:09:12<67:42:25, 186.64s/it]{'loss': 1.9074, 'grad_norm': 0.0, 'learning_rate': 1.4004282655246253e-05, 'epoch': 0.3}
? Step 562  Epoch: 0.30  Loss: 1.9300
                                                          30%|███       | 562/1868 [29:09:12<67:42:25, 186.64s/it] 30%|███       | 563/1868 [29:12:19<67:39:16, 186.63s/it]{'loss': 1.93, 'grad_norm': 0.0, 'learning_rate': 1.3993576017130622e-05, 'epoch': 0.3}
? Step 563  Epoch: 0.30  Loss: 1.5387
                                                          30%|███       | 563/1868 [29:12:19<67:39:16, 186.63s/it] 30%|███       | 564/1868 [29:15:25<67:36:12, 186.64s/it]{'loss': 1.5387, 'grad_norm': 0.0, 'learning_rate': 1.3982869379014989e-05, 'epoch': 0.3}
? Step 564  Epoch: 0.30  Loss: 1.7452
                                                          30%|███       | 564/1868 [29:15:25<67:36:12, 186.64s/it] 30%|███       | 565/1868 [29:18:32<67:32:50, 186.62s/it]{'loss': 1.7452, 'grad_norm': 0.0, 'learning_rate': 1.3972162740899357e-05, 'epoch': 0.3}
? Step 565  Epoch: 0.30  Loss: 1.6295
                                                          30%|███       | 565/1868 [29:18:32<67:32:50, 186.62s/it] 30%|███       | 566/1868 [29:21:39<67:30:29, 186.66s/it]{'loss': 1.6295, 'grad_norm': 0.0, 'learning_rate': 1.3961456102783728e-05, 'epoch': 0.3}
? Step 566  Epoch: 0.30  Loss: 1.5645
                                                          30%|███       | 566/1868 [29:21:39<67:30:29, 186.66s/it] 30%|███       | 567/1868 [29:24:46<67:29:13, 186.74s/it]{'loss': 1.5645, 'grad_norm': 0.0, 'learning_rate': 1.3950749464668096e-05, 'epoch': 0.3}
? Step 567  Epoch: 0.30  Loss: 1.7093
                                                          30%|███       | 567/1868 [29:24:46<67:29:13, 186.74s/it] 30%|███       | 568/1868 [29:27:52<67:25:52, 186.73s/it]{'loss': 1.7093, 'grad_norm': 0.0, 'learning_rate': 1.3940042826552465e-05, 'epoch': 0.3}
? Step 568  Epoch: 0.30  Loss: 1.6070
                                                          30%|███       | 568/1868 [29:27:52<67:25:52, 186.73s/it] 30%|███       | 569/1868 [29:30:59<67:21:27, 186.67s/it]{'loss': 1.607, 'grad_norm': 0.0, 'learning_rate': 1.3929336188436832e-05, 'epoch': 0.3}
? Step 569  Epoch: 0.30  Loss: 1.8259
                                                          30%|███       | 569/1868 [29:30:59<67:21:27, 186.67s/it] 31%|███       | 570/1868 [29:34:06<67:19:05, 186.71s/it]{'loss': 1.8259, 'grad_norm': 0.0, 'learning_rate': 1.39186295503212e-05, 'epoch': 0.3}
? Step 570  Epoch: 0.31  Loss: 1.4352
                                                          31%|███       | 570/1868 [29:34:06<67:19:05, 186.71s/it] 31%|███       | 571/1868 [29:37:12<67:14:30, 186.64s/it]{'loss': 1.4352, 'grad_norm': 0.0, 'learning_rate': 1.390792291220557e-05, 'epoch': 0.31}
? Step 571  Epoch: 0.31  Loss: 1.6901
                                                          31%|███       | 571/1868 [29:37:12<67:14:30, 186.64s/it] 31%|███       | 572/1868 [29:40:19<67:11:32, 186.65s/it]{'loss': 1.6901, 'grad_norm': 0.0, 'learning_rate': 1.3897216274089938e-05, 'epoch': 0.31}
? Step 572  Epoch: 0.31  Loss: 1.6272
                                                          31%|███       | 572/1868 [29:40:19<67:11:32, 186.65s/it] 31%|███       | 573/1868 [29:43:25<67:08:29, 186.65s/it]{'loss': 1.6272, 'grad_norm': 0.0, 'learning_rate': 1.3886509635974305e-05, 'epoch': 0.31}
? Step 573  Epoch: 0.31  Loss: 1.5947
                                                          31%|███       | 573/1868 [29:43:25<67:08:29, 186.65s/it] 31%|███       | 574/1868 [29:46:32<67:06:02, 186.68s/it]{'loss': 1.5947, 'grad_norm': 0.0, 'learning_rate': 1.3875802997858674e-05, 'epoch': 0.31}
? Step 574  Epoch: 0.31  Loss: 1.6025
                                                          31%|███       | 574/1868 [29:46:32<67:06:02, 186.68s/it] 31%|███       | 575/1868 [29:49:39<67:05:55, 186.82s/it]{'loss': 1.6025, 'grad_norm': 0.0, 'learning_rate': 1.3865096359743042e-05, 'epoch': 0.31}
? Step 575  Epoch: 0.31  Loss: 1.6760
                                                          31%|███       | 575/1868 [29:49:39<67:05:55, 186.82s/it] 31%|███       | 576/1868 [29:52:46<67:03:38, 186.86s/it]{'loss': 1.676, 'grad_norm': 0.0, 'learning_rate': 1.3854389721627411e-05, 'epoch': 0.31}
? Step 576  Epoch: 0.31  Loss: 1.5127
                                                          31%|███       | 576/1868 [29:52:46<67:03:38, 186.86s/it] 31%|███       | 577/1868 [29:55:53<66:59:05, 186.79s/it]{'loss': 1.5127, 'grad_norm': 0.0, 'learning_rate': 1.3843683083511778e-05, 'epoch': 0.31}
? Step 577  Epoch: 0.31  Loss: 1.4282
                                                          31%|███       | 577/1868 [29:55:53<66:59:05, 186.79s/it] 31%|███       | 578/1868 [29:59:00<66:56:42, 186.82s/it]{'loss': 1.4282, 'grad_norm': 0.0, 'learning_rate': 1.3832976445396147e-05, 'epoch': 0.31}
? Step 578  Epoch: 0.31  Loss: 1.3319
                                                          31%|███       | 578/1868 [29:59:00<66:56:42, 186.82s/it] 31%|███       | 579/1868 [30:02:06<66:51:23, 186.72s/it]{'loss': 1.3319, 'grad_norm': 0.0, 'learning_rate': 1.3822269807280515e-05, 'epoch': 0.31}
? Step 579  Epoch: 0.31  Loss: 2.2043
                                                          31%|███       | 579/1868 [30:02:06<66:51:23, 186.72s/it] 31%|███       | 580/1868 [30:05:13<66:51:04, 186.85s/it]{'loss': 2.2043, 'grad_norm': 0.0, 'learning_rate': 1.3811563169164884e-05, 'epoch': 0.31}
? Step 580  Epoch: 0.31  Loss: 1.5829
                                                          31%|███       | 580/1868 [30:05:13<66:51:04, 186.85s/it] 31%|███       | 581/1868 [30:08:20<66:49:22, 186.92s/it]{'loss': 1.5829, 'grad_norm': 0.0, 'learning_rate': 1.3800856531049251e-05, 'epoch': 0.31}
? Step 581  Epoch: 0.31  Loss: 1.8554
                                                          31%|███       | 581/1868 [30:08:20<66:49:22, 186.92s/it] 31%|███       | 582/1868 [30:11:27<66:46:57, 186.95s/it]{'loss': 1.8554, 'grad_norm': 0.0, 'learning_rate': 1.379014989293362e-05, 'epoch': 0.31}
? Step 582  Epoch: 0.31  Loss: 1.3015
                                                          31%|███       | 582/1868 [30:11:27<66:46:57, 186.95s/it] 31%|███       | 583/1868 [30:14:35<66:46:08, 187.06s/it]{'loss': 1.3015, 'grad_norm': 0.0, 'learning_rate': 1.3779443254817988e-05, 'epoch': 0.31}
? Step 583  Epoch: 0.31  Loss: 1.6927
                                                          31%|███       | 583/1868 [30:14:35<66:46:08, 187.06s/it] 31%|███▏      | 584/1868 [30:17:42<66:41:12, 186.97s/it]{'loss': 1.6927, 'grad_norm': 0.0, 'learning_rate': 1.3768736616702357e-05, 'epoch': 0.31}
? Step 584  Epoch: 0.31  Loss: 1.5524
                                                          31%|███▏      | 584/1868 [30:17:42<66:41:12, 186.97s/it] 31%|███▏      | 585/1868 [30:20:48<66:37:08, 186.93s/it]{'loss': 1.5524, 'grad_norm': 0.0, 'learning_rate': 1.3758029978586724e-05, 'epoch': 0.31}
? Step 585  Epoch: 0.31  Loss: 1.5596
                                                          31%|███▏      | 585/1868 [30:20:48<66:37:08, 186.93s/it] 31%|███▏      | 586/1868 [30:23:56<66:35:30, 187.00s/it]{'loss': 1.5596, 'grad_norm': 0.0, 'learning_rate': 1.3747323340471093e-05, 'epoch': 0.31}
? Step 586  Epoch: 0.31  Loss: 1.5677
                                                          31%|███▏      | 586/1868 [30:23:56<66:35:30, 187.00s/it] 31%|███▏      | 587/1868 [30:27:03<66:32:08, 186.99s/it]{'loss': 1.5677, 'grad_norm': 0.0, 'learning_rate': 1.3736616702355461e-05, 'epoch': 0.31}
? Step 587  Epoch: 0.31  Loss: 1.3519
                                                          31%|███▏      | 587/1868 [30:27:03<66:32:08, 186.99s/it] 31%|███▏      | 588/1868 [30:30:10<66:29:43, 187.02s/it]{'loss': 1.3519, 'grad_norm': 0.0, 'learning_rate': 1.3725910064239828e-05, 'epoch': 0.31}
? Step 588  Epoch: 0.31  Loss: 1.4545
                                                          31%|███▏      | 588/1868 [30:30:10<66:29:43, 187.02s/it] 32%|███▏      | 589/1868 [30:33:16<66:25:46, 186.98s/it]{'loss': 1.4545, 'grad_norm': 0.0, 'learning_rate': 1.3715203426124197e-05, 'epoch': 0.31}
? Step 589  Epoch: 0.32  Loss: 1.5195
                                                          32%|███▏      | 589/1868 [30:33:16<66:25:46, 186.98s/it] 32%|███▏      | 590/1868 [30:36:24<66:23:34, 187.02s/it]{'loss': 1.5195, 'grad_norm': 0.0, 'learning_rate': 1.3704496788008566e-05, 'epoch': 0.32}
? Step 590  Epoch: 0.32  Loss: 1.5116
                                                          32%|███▏      | 590/1868 [30:36:24<66:23:34, 187.02s/it] 32%|███▏      | 591/1868 [30:39:31<66:21:38, 187.08s/it]{'loss': 1.5116, 'grad_norm': 0.0, 'learning_rate': 1.3693790149892934e-05, 'epoch': 0.32}
? Step 591  Epoch: 0.32  Loss: 1.4467
                                                          32%|███▏      | 591/1868 [30:39:31<66:21:38, 187.08s/it] 32%|███▏      | 592/1868 [30:42:38<66:16:26, 186.98s/it]{'loss': 1.4467, 'grad_norm': 0.0, 'learning_rate': 1.3683083511777301e-05, 'epoch': 0.32}
? Step 592  Epoch: 0.32  Loss: 2.0530
                                                          32%|███▏      | 592/1868 [30:42:38<66:16:26, 186.98s/it] 32%|███▏      | 593/1868 [30:45:45<66:14:02, 187.01s/it]{'loss': 2.053, 'grad_norm': 0.0, 'learning_rate': 1.3672376873661672e-05, 'epoch': 0.32}
? Step 593  Epoch: 0.32  Loss: 1.5156
                                                          32%|███▏      | 593/1868 [30:45:45<66:14:02, 187.01s/it] 32%|███▏      | 594/1868 [30:48:52<66:12:01, 187.07s/it]{'loss': 1.5156, 'grad_norm': 0.0, 'learning_rate': 1.366167023554604e-05, 'epoch': 0.32}
? Step 594  Epoch: 0.32  Loss: 1.3907
                                                          32%|███▏      | 594/1868 [30:48:52<66:12:01, 187.07s/it] 32%|███▏      | 595/1868 [30:51:59<66:08:02, 187.02s/it]{'loss': 1.3907, 'grad_norm': 0.0, 'learning_rate': 1.3650963597430409e-05, 'epoch': 0.32}
? Step 595  Epoch: 0.32  Loss: 1.6384
                                                          32%|███▏      | 595/1868 [30:51:59<66:08:02, 187.02s/it] 32%|███▏      | 596/1868 [30:55:06<66:04:55, 187.02s/it]{'loss': 1.6384, 'grad_norm': 0.0, 'learning_rate': 1.3640256959314778e-05, 'epoch': 0.32}
? Step 596  Epoch: 0.32  Loss: 1.6630
                                                          32%|███▏      | 596/1868 [30:55:06<66:04:55, 187.02s/it] 32%|███▏      | 597/1868 [30:58:13<66:01:44, 187.02s/it]{'loss': 1.663, 'grad_norm': 0.0, 'learning_rate': 1.3629550321199145e-05, 'epoch': 0.32}
? Step 597  Epoch: 0.32  Loss: 1.3544
                                                          32%|███▏      | 597/1868 [30:58:13<66:01:44, 187.02s/it] 32%|███▏      | 598/1868 [31:01:20<65:58:26, 187.01s/it]{'loss': 1.3544, 'grad_norm': 0.0, 'learning_rate': 1.3618843683083513e-05, 'epoch': 0.32}
? Step 598  Epoch: 0.32  Loss: 1.6744
                                                          32%|███▏      | 598/1868 [31:01:20<65:58:26, 187.01s/it] 32%|███▏      | 599/1868 [31:04:27<65:56:34, 187.07s/it]{'loss': 1.6744, 'grad_norm': 0.0, 'learning_rate': 1.3608137044967882e-05, 'epoch': 0.32}
? Step 599  Epoch: 0.32  Loss: 1.5567
                                                          32%|███▏      | 599/1868 [31:04:27<65:56:34, 187.07s/it] 32%|███▏      | 600/1868 [31:07:34<65:54:13, 187.11s/it]{'loss': 1.5567, 'grad_norm': 0.0, 'learning_rate': 1.359743040685225e-05, 'epoch': 0.32}
? Step 600  Epoch: 0.32  Loss: 1.4866
                                                          32%|███▏      | 600/1868 [31:07:34<65:54:13, 187.11s/it]/home/dev25-01/mistral-env/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 32%|███▏      | 601/1868 [31:10:42<65:52:39, 187.18s/it]{'loss': 1.4866, 'grad_norm': 0.0, 'learning_rate': 1.3586723768736618e-05, 'epoch': 0.32}
? Step 601  Epoch: 0.32  Loss: 1.6573
                                                          32%|███▏      | 601/1868 [31:10:42<65:52:39, 187.18s/it] 32%|███▏      | 602/1868 [31:13:49<65:48:51, 187.15s/it]{'loss': 1.6573, 'grad_norm': 0.0, 'learning_rate': 1.3576017130620986e-05, 'epoch': 0.32}
? Step 602  Epoch: 0.32  Loss: 1.2440
                                                          32%|███▏      | 602/1868 [31:13:49<65:48:51, 187.15s/it] 32%|███▏      | 603/1868 [31:16:56<65:46:52, 187.20s/it]{'loss': 1.244, 'grad_norm': 0.0, 'learning_rate': 1.3565310492505355e-05, 'epoch': 0.32}
? Step 603  Epoch: 0.32  Loss: 1.5138
                                                          32%|███▏      | 603/1868 [31:16:56<65:46:52, 187.20s/it] 32%|███▏      | 604/1868 [31:20:03<65:43:50, 187.21s/it]{'loss': 1.5138, 'grad_norm': 0.0, 'learning_rate': 1.3554603854389724e-05, 'epoch': 0.32}
? Step 604  Epoch: 0.32  Loss: 1.4330
                                                          32%|███▏      | 604/1868 [31:20:03<65:43:50, 187.21s/it] 32%|███▏      | 605/1868 [31:23:10<65:41:06, 187.23s/it]{'loss': 1.433, 'grad_norm': 0.0, 'learning_rate': 1.354389721627409e-05, 'epoch': 0.32}
? Step 605  Epoch: 0.32  Loss: 1.5969
                                                          32%|███▏      | 605/1868 [31:23:10<65:41:06, 187.23s/it] 32%|███▏      | 606/1868 [31:26:18<65:37:56, 187.22s/it]{'loss': 1.5969, 'grad_norm': 0.0, 'learning_rate': 1.353319057815846e-05, 'epoch': 0.32}
? Step 606  Epoch: 0.32  Loss: 1.8682
                                                          32%|███▏      | 606/1868 [31:26:18<65:37:56, 187.22s/it] 32%|███▏      | 607/1868 [31:29:25<65:34:35, 187.21s/it]{'loss': 1.8682, 'grad_norm': 0.0, 'learning_rate': 1.3522483940042828e-05, 'epoch': 0.32}
? Step 607  Epoch: 0.32  Loss: 1.8124
                                                          32%|███▏      | 607/1868 [31:29:25<65:34:35, 187.21s/it] 33%|███▎      | 608/1868 [31:32:32<65:30:13, 187.15s/it]{'loss': 1.8124, 'grad_norm': 0.0, 'learning_rate': 1.3511777301927197e-05, 'epoch': 0.32}
? Step 608  Epoch: 0.33  Loss: 2.0427
                                                          33%|███▎      | 608/1868 [31:32:32<65:30:13, 187.15s/it] 33%|███▎      | 609/1868 [31:35:39<65:28:53, 187.24s/it]{'loss': 2.0427, 'grad_norm': 0.0, 'learning_rate': 1.3501070663811564e-05, 'epoch': 0.33}
? Step 609  Epoch: 0.33  Loss: 1.4958
                                                          33%|███▎      | 609/1868 [31:35:39<65:28:53, 187.24s/it] 33%|███▎      | 610/1868 [31:38:46<65:24:47, 187.19s/it]{'loss': 1.4958, 'grad_norm': 0.0, 'learning_rate': 1.3490364025695932e-05, 'epoch': 0.33}
? Step 610  Epoch: 0.33  Loss: 1.5567
                                                          33%|███▎      | 610/1868 [31:38:46<65:24:47, 187.19s/it] 33%|███▎      | 611/1868 [31:41:54<65:22:38, 187.24s/it]{'loss': 1.5567, 'grad_norm': 0.0, 'learning_rate': 1.3479657387580301e-05, 'epoch': 0.33}
? Step 611  Epoch: 0.33  Loss: 1.4296
                                                          33%|███▎      | 611/1868 [31:41:54<65:22:38, 187.24s/it] 33%|███▎      | 612/1868 [31:45:01<65:18:48, 187.20s/it]{'loss': 1.4296, 'grad_norm': 0.0, 'learning_rate': 1.3468950749464668e-05, 'epoch': 0.33}
? Step 612  Epoch: 0.33  Loss: 1.6510
                                                          33%|███▎      | 612/1868 [31:45:01<65:18:48, 187.20s/it] 33%|███▎      | 613/1868 [31:48:08<65:16:13, 187.23s/it]{'loss': 1.651, 'grad_norm': 0.0, 'learning_rate': 1.3458244111349037e-05, 'epoch': 0.33}
? Step 613  Epoch: 0.33  Loss: 1.4084
                                                          33%|███▎      | 613/1868 [31:48:08<65:16:13, 187.23s/it] 33%|███▎      | 614/1868 [31:51:15<65:13:44, 187.26s/it]{'loss': 1.4084, 'grad_norm': 0.0, 'learning_rate': 1.3447537473233405e-05, 'epoch': 0.33}
? Step 614  Epoch: 0.33  Loss: 1.5477
                                                          33%|███▎      | 614/1868 [31:51:15<65:13:44, 187.26s/it] 33%|███▎      | 615/1868 [31:54:23<65:11:16, 187.29s/it]{'loss': 1.5477, 'grad_norm': 0.0, 'learning_rate': 1.3436830835117774e-05, 'epoch': 0.33}
? Step 615  Epoch: 0.33  Loss: 1.5447
                                                          33%|███▎      | 615/1868 [31:54:23<65:11:16, 187.29s/it] 33%|███▎      | 616/1868 [31:57:30<65:08:11, 187.29s/it]{'loss': 1.5447, 'grad_norm': 0.0, 'learning_rate': 1.3426124197002141e-05, 'epoch': 0.33}
? Step 616  Epoch: 0.33  Loss: 1.6327
                                                          33%|███▎      | 616/1868 [31:57:30<65:08:11, 187.29s/it] 33%|███▎      | 617/1868 [32:00:37<65:03:53, 187.24s/it]{'loss': 1.6327, 'grad_norm': 0.0, 'learning_rate': 1.341541755888651e-05, 'epoch': 0.33}
? Step 617  Epoch: 0.33  Loss: 2.0164
                                                          33%|███▎      | 617/1868 [32:00:37<65:03:53, 187.24s/it] 33%|███▎      | 618/1868 [32:03:45<65:01:11, 187.26s/it]{'loss': 2.0164, 'grad_norm': 0.0, 'learning_rate': 1.3404710920770878e-05, 'epoch': 0.33}
? Step 618  Epoch: 0.33  Loss: 1.3701
                                                          33%|███▎      | 618/1868 [32:03:45<65:01:11, 187.26s/it] 33%|███▎      | 619/1868 [32:06:52<64:57:29, 187.23s/it]{'loss': 1.3701, 'grad_norm': 0.0, 'learning_rate': 1.3394004282655247e-05, 'epoch': 0.33}
? Step 619  Epoch: 0.33  Loss: 1.7301
                                                          33%|███▎      | 619/1868 [32:06:52<64:57:29, 187.23s/it] 33%|███▎      | 620/1868 [32:09:59<64:56:00, 187.31s/it]{'loss': 1.7301, 'grad_norm': 0.0, 'learning_rate': 1.3383297644539614e-05, 'epoch': 0.33}
? Step 620  Epoch: 0.33  Loss: 1.6572
                                                          33%|███▎      | 620/1868 [32:09:59<64:56:00, 187.31s/it] 33%|███▎      | 621/1868 [32:13:06<64:52:04, 187.27s/it]{'loss': 1.6572, 'grad_norm': 0.0, 'learning_rate': 1.3372591006423984e-05, 'epoch': 0.33}
? Step 621  Epoch: 0.33  Loss: 1.4831
                                                          33%|███▎      | 621/1868 [32:13:06<64:52:04, 187.27s/it] 33%|███▎      | 622/1868 [32:16:14<64:49:39, 187.30s/it]{'loss': 1.4831, 'grad_norm': 0.0, 'learning_rate': 1.3361884368308353e-05, 'epoch': 0.33}
? Step 622  Epoch: 0.33  Loss: 1.3956
                                                          33%|███▎      | 622/1868 [32:16:14<64:49:39, 187.30s/it] 33%|███▎      | 623/1868 [32:19:21<64:45:46, 187.27s/it]{'loss': 1.3956, 'grad_norm': 0.0, 'learning_rate': 1.3351177730192722e-05, 'epoch': 0.33}
? Step 623  Epoch: 0.33  Loss: 1.4306
                                                          33%|███▎      | 623/1868 [32:19:21<64:45:46, 187.27s/it] 33%|███▎      | 624/1868 [32:22:28<64:42:05, 187.24s/it]{'loss': 1.4306, 'grad_norm': 0.0, 'learning_rate': 1.334047109207709e-05, 'epoch': 0.33}
? Step 624  Epoch: 0.33  Loss: 1.5429
                                                          33%|███▎      | 624/1868 [32:22:28<64:42:05, 187.24s/it] 33%|███▎      | 625/1868 [32:25:35<64:39:08, 187.25s/it]{'loss': 1.5429, 'grad_norm': 0.0, 'learning_rate': 1.3329764453961457e-05, 'epoch': 0.33}
? Step 625  Epoch: 0.33  Loss: 1.7224
                                                          33%|███▎      | 625/1868 [32:25:35<64:39:08, 187.25s/it] 34%|███▎      | 626/1868 [32:28:43<64:37:33, 187.32s/it]{'loss': 1.7224, 'grad_norm': 0.0, 'learning_rate': 1.3319057815845826e-05, 'epoch': 0.33}
? Step 626  Epoch: 0.34  Loss: 1.5283
                                                          34%|███▎      | 626/1868 [32:28:43<64:37:33, 187.32s/it] 34%|███▎      | 627/1868 [32:31:50<64:32:17, 187.22s/it]{'loss': 1.5283, 'grad_norm': 0.0, 'learning_rate': 1.3308351177730195e-05, 'epoch': 0.34}
? Step 627  Epoch: 0.34  Loss: 1.9680
                                                          34%|███▎      | 627/1868 [32:31:50<64:32:17, 187.22s/it] 34%|███▎      | 628/1868 [32:34:57<64:29:30, 187.23s/it]{'loss': 1.968, 'grad_norm': 0.0, 'learning_rate': 1.3297644539614563e-05, 'epoch': 0.34}
? Step 628  Epoch: 0.34  Loss: 1.5884
                                                          34%|███▎      | 628/1868 [32:34:57<64:29:30, 187.23s/it] 34%|███▎      | 629/1868 [32:38:04<64:26:03, 187.22s/it]{'loss': 1.5884, 'grad_norm': 0.0, 'learning_rate': 1.328693790149893e-05, 'epoch': 0.34}
? Step 629  Epoch: 0.34  Loss: 1.6139
                                                          34%|███▎      | 629/1868 [32:38:04<64:26:03, 187.22s/it] 34%|███▎      | 630/1868 [32:41:12<64:23:25, 187.24s/it]{'loss': 1.6139, 'grad_norm': 0.0, 'learning_rate': 1.3276231263383299e-05, 'epoch': 0.34}
? Step 630  Epoch: 0.34  Loss: 1.8925
                                                          34%|███▎      | 630/1868 [32:41:12<64:23:25, 187.24s/it] 34%|███▍      | 631/1868 [32:44:19<64:18:51, 187.17s/it]{'loss': 1.8925, 'grad_norm': 0.0, 'learning_rate': 1.3265524625267668e-05, 'epoch': 0.34}
? Step 631  Epoch: 0.34  Loss: 1.5451
                                                          34%|███▍      | 631/1868 [32:44:19<64:18:51, 187.17s/it] 34%|███▍      | 632/1868 [32:47:26<64:17:34, 187.26s/it]{'loss': 1.5451, 'grad_norm': 0.0, 'learning_rate': 1.3254817987152036e-05, 'epoch': 0.34}
? Step 632  Epoch: 0.34  Loss: 1.3975
                                                          34%|███▍      | 632/1868 [32:47:26<64:17:34, 187.26s/it] 34%|███▍      | 633/1868 [32:50:34<64:15:35, 187.32s/it]{'loss': 1.3975, 'grad_norm': 0.0, 'learning_rate': 1.3244111349036403e-05, 'epoch': 0.34}
? Step 633  Epoch: 0.34  Loss: 1.4955
                                                          34%|███▍      | 633/1868 [32:50:34<64:15:35, 187.32s/it] 34%|███▍      | 634/1868 [32:53:41<64:13:41, 187.38s/it]{'loss': 1.4955, 'grad_norm': 0.0, 'learning_rate': 1.3233404710920772e-05, 'epoch': 0.34}
? Step 634  Epoch: 0.34  Loss: 1.4001
                                                          34%|███▍      | 634/1868 [32:53:41<64:13:41, 187.38s/it] 34%|███▍      | 635/1868 [32:56:48<64:09:12, 187.31s/it]{'loss': 1.4001, 'grad_norm': 0.0, 'learning_rate': 1.322269807280514e-05, 'epoch': 0.34}
? Step 635  Epoch: 0.34  Loss: 1.8497
                                                          34%|███▍      | 635/1868 [32:56:48<64:09:12, 187.31s/it] 34%|███▍      | 636/1868 [32:59:56<64:06:49, 187.35s/it]{'loss': 1.8497, 'grad_norm': 0.0, 'learning_rate': 1.3211991434689508e-05, 'epoch': 0.34}
? Step 636  Epoch: 0.34  Loss: 1.3479
                                                          34%|███▍      | 636/1868 [32:59:56<64:06:49, 187.35s/it] 34%|███▍      | 637/1868 [33:03:03<64:04:45, 187.40s/it]{'loss': 1.3479, 'grad_norm': 0.0, 'learning_rate': 1.3201284796573876e-05, 'epoch': 0.34}
? Step 637  Epoch: 0.34  Loss: 1.4395
                                                          34%|███▍      | 637/1868 [33:03:03<64:04:45, 187.40s/it] 34%|███▍      | 638/1868 [33:06:10<64:00:50, 187.36s/it]{'loss': 1.4395, 'grad_norm': 0.0, 'learning_rate': 1.3190578158458245e-05, 'epoch': 0.34}
? Step 638  Epoch: 0.34  Loss: 1.6030
                                                          34%|███▍      | 638/1868 [33:06:10<64:00:50, 187.36s/it] 34%|███▍      | 639/1868 [33:09:18<63:58:47, 187.41s/it]{'loss': 1.603, 'grad_norm': 0.0, 'learning_rate': 1.3179871520342614e-05, 'epoch': 0.34}
? Step 639  Epoch: 0.34  Loss: 1.6425
                                                          34%|███▍      | 639/1868 [33:09:18<63:58:47, 187.41s/it] 34%|███▍      | 640/1868 [33:12:25<63:54:50, 187.37s/it]{'loss': 1.6425, 'grad_norm': 0.0, 'learning_rate': 1.316916488222698e-05, 'epoch': 0.34}
? Step 640  Epoch: 0.34  Loss: 1.4592
                                                          34%|███▍      | 640/1868 [33:12:25<63:54:50, 187.37s/it] 34%|███▍      | 641/1868 [33:15:32<63:50:00, 187.29s/it]{'loss': 1.4592, 'grad_norm': 0.0, 'learning_rate': 1.315845824411135e-05, 'epoch': 0.34}
? Step 641  Epoch: 0.34  Loss: 1.8531
                                                          34%|███▍      | 641/1868 [33:15:32<63:50:00, 187.29s/it] 34%|███▍      | 642/1868 [33:18:40<63:48:40, 187.37s/it]{'loss': 1.8531, 'grad_norm': 0.0, 'learning_rate': 1.3147751605995718e-05, 'epoch': 0.34}
? Step 642  Epoch: 0.34  Loss: 1.4615
                                                          34%|███▍      | 642/1868 [33:18:40<63:48:40, 187.37s/it] 34%|███▍      | 643/1868 [33:21:47<63:45:23, 187.37s/it]{'loss': 1.4615, 'grad_norm': 0.0, 'learning_rate': 1.3137044967880087e-05, 'epoch': 0.34}
? Step 643  Epoch: 0.34  Loss: 1.8434
                                                          34%|███▍      | 643/1868 [33:21:47<63:45:23, 187.37s/it] 34%|███▍      | 644/1868 [33:24:55<63:42:42, 187.39s/it]{'loss': 1.8434, 'grad_norm': 0.0, 'learning_rate': 1.3126338329764454e-05, 'epoch': 0.34}
? Step 644  Epoch: 0.34  Loss: 1.8018
                                                          34%|███▍      | 644/1868 [33:24:55<63:42:42, 187.39s/it] 35%|███▍      | 645/1868 [33:28:02<63:41:01, 187.46s/it]{'loss': 1.8018, 'grad_norm': 0.0, 'learning_rate': 1.3115631691648822e-05, 'epoch': 0.34}
? Step 645  Epoch: 0.35  Loss: 1.4317
                                                          35%|███▍      | 645/1868 [33:28:02<63:41:01, 187.46s/it] 35%|███▍      | 646/1868 [33:31:10<63:37:43, 187.45s/it]{'loss': 1.4317, 'grad_norm': 0.0, 'learning_rate': 1.3104925053533191e-05, 'epoch': 0.35}
? Step 646  Epoch: 0.35  Loss: 1.6168
                                                          35%|███▍      | 646/1868 [33:31:10<63:37:43, 187.45s/it] 35%|███▍      | 647/1868 [33:34:17<63:35:21, 187.49s/it]{'loss': 1.6168, 'grad_norm': 0.0, 'learning_rate': 1.309421841541756e-05, 'epoch': 0.35}
? Step 647  Epoch: 0.35  Loss: 1.6888
                                                          35%|███▍      | 647/1868 [33:34:17<63:35:21, 187.49s/it] 35%|███▍      | 648/1868 [33:37:25<63:30:40, 187.41s/it]{'loss': 1.6888, 'grad_norm': 0.0, 'learning_rate': 1.308351177730193e-05, 'epoch': 0.35}
? Step 648  Epoch: 0.35  Loss: 1.7940
                                                          35%|███▍      | 648/1868 [33:37:25<63:30:40, 187.41s/it] 35%|███▍      | 649/1868 [33:40:32<63:28:05, 187.44s/it]{'loss': 1.794, 'grad_norm': 0.0, 'learning_rate': 1.3072805139186297e-05, 'epoch': 0.35}
? Step 649  Epoch: 0.35  Loss: 1.7063
                                                          35%|███▍      | 649/1868 [33:40:32<63:28:05, 187.44s/it] 35%|███▍      | 650/1868 [33:43:39<63:24:27, 187.41s/it]{'loss': 1.7063, 'grad_norm': 0.0, 'learning_rate': 1.3062098501070666e-05, 'epoch': 0.35}
? Step 650  Epoch: 0.35  Loss: 1.4385
                                                          35%|███▍      | 650/1868 [33:43:39<63:24:27, 187.41s/it] 35%|███▍      | 651/1868 [33:46:47<63:20:52, 187.39s/it]{'loss': 1.4385, 'grad_norm': 0.0, 'learning_rate': 1.3051391862955034e-05, 'epoch': 0.35}
? Step 651  Epoch: 0.35  Loss: 1.4401
                                                          35%|███▍      | 651/1868 [33:46:47<63:20:52, 187.39s/it] 35%|███▍      | 652/1868 [33:49:54<63:17:48, 187.39s/it]{'loss': 1.4401, 'grad_norm': 0.0, 'learning_rate': 1.3040685224839403e-05, 'epoch': 0.35}
? Step 652  Epoch: 0.35  Loss: 1.4484
                                                          35%|███▍      | 652/1868 [33:49:54<63:17:48, 187.39s/it] 35%|███▍      | 653/1868 [33:53:02<63:16:00, 187.46s/it]{'loss': 1.4484, 'grad_norm': 0.0, 'learning_rate': 1.302997858672377e-05, 'epoch': 0.35}
? Step 653  Epoch: 0.35  Loss: 1.3961
                                                          35%|███▍      | 653/1868 [33:53:02<63:16:00, 187.46s/it] 35%|███▌      | 654/1868 [33:56:09<63:10:52, 187.36s/it]{'loss': 1.3961, 'grad_norm': 0.0, 'learning_rate': 1.3019271948608139e-05, 'epoch': 0.35}
? Step 654  Epoch: 0.35  Loss: 1.8393
                                                          35%|███▌      | 654/1868 [33:56:09<63:10:52, 187.36s/it] 35%|███▌      | 655/1868 [33:59:17<63:09:26, 187.44s/it]{'loss': 1.8393, 'grad_norm': 0.0, 'learning_rate': 1.3008565310492507e-05, 'epoch': 0.35}
? Step 655  Epoch: 0.35  Loss: 1.6868
                                                          35%|███▌      | 655/1868 [33:59:17<63:09:26, 187.44s/it] 35%|███▌      | 656/1868 [34:02:24<63:07:11, 187.48s/it]{'loss': 1.6868, 'grad_norm': 0.0, 'learning_rate': 1.2997858672376876e-05, 'epoch': 0.35}
? Step 656  Epoch: 0.35  Loss: 1.3385
                                                          35%|███▌      | 656/1868 [34:02:24<63:07:11, 187.48s/it] 35%|███▌      | 657/1868 [34:05:32<63:03:50, 187.47s/it]{'loss': 1.3385, 'grad_norm': 0.0, 'learning_rate': 1.2987152034261243e-05, 'epoch': 0.35}
? Step 657  Epoch: 0.35  Loss: 1.6929
                                                          35%|███▌      | 657/1868 [34:05:32<63:03:50, 187.47s/it] 35%|███▌      | 658/1868 [34:08:39<63:01:12, 187.50s/it]{'loss': 1.6929, 'grad_norm': 0.0, 'learning_rate': 1.2976445396145612e-05, 'epoch': 0.35}
? Step 658  Epoch: 0.35  Loss: 1.4930
                                                          35%|███▌      | 658/1868 [34:08:39<63:01:12, 187.50s/it] 35%|███▌      | 659/1868 [34:11:47<62:59:53, 187.59s/it]{'loss': 1.493, 'grad_norm': 0.0, 'learning_rate': 1.296573875802998e-05, 'epoch': 0.35}
? Step 659  Epoch: 0.35  Loss: 1.5214
                                                          35%|███▌      | 659/1868 [34:11:47<62:59:53, 187.59s/it] 35%|███▌      | 660/1868 [34:14:54<62:55:58, 187.55s/it]{'loss': 1.5214, 'grad_norm': 0.0, 'learning_rate': 1.2955032119914347e-05, 'epoch': 0.35}
? Step 660  Epoch: 0.35  Loss: 1.5809
                                                          35%|███▌      | 660/1868 [34:14:54<62:55:58, 187.55s/it] 35%|███▌      | 661/1868 [34:18:02<62:53:22, 187.57s/it]{'loss': 1.5809, 'grad_norm': 0.0, 'learning_rate': 1.2944325481798716e-05, 'epoch': 0.35}
? Step 661  Epoch: 0.35  Loss: 1.4349
                                                          35%|███▌      | 661/1868 [34:18:02<62:53:22, 187.57s/it] 35%|███▌      | 662/1868 [34:21:10<62:50:00, 187.56s/it]{'loss': 1.4349, 'grad_norm': 0.0, 'learning_rate': 1.2933618843683085e-05, 'epoch': 0.35}
? Step 662  Epoch: 0.35  Loss: 1.6021
                                                          35%|███▌      | 662/1868 [34:21:10<62:50:00, 187.56s/it] 35%|███▌      | 663/1868 [34:24:17<62:49:13, 187.68s/it]{'loss': 1.6021, 'grad_norm': 0.0, 'learning_rate': 1.2922912205567453e-05, 'epoch': 0.35}
? Step 663  Epoch: 0.35  Loss: 1.5417
                                                          35%|███▌      | 663/1868 [34:24:17<62:49:13, 187.68s/it] 36%|███▌      | 664/1868 [34:27:25<62:44:09, 187.58s/it]{'loss': 1.5417, 'grad_norm': 0.0, 'learning_rate': 1.291220556745182e-05, 'epoch': 0.35}
? Step 664  Epoch: 0.36  Loss: 1.8829
                                                          36%|███▌      | 664/1868 [34:27:25<62:44:09, 187.58s/it] 36%|███▌      | 665/1868 [34:30:32<62:39:46, 187.52s/it]{'loss': 1.8829, 'grad_norm': 0.0, 'learning_rate': 1.2901498929336189e-05, 'epoch': 0.36}
? Step 665  Epoch: 0.36  Loss: 1.5727
                                                          36%|███▌      | 665/1868 [34:30:32<62:39:46, 187.52s/it] 36%|███▌      | 666/1868 [34:33:40<62:38:04, 187.59s/it]{'loss': 1.5727, 'grad_norm': 0.0, 'learning_rate': 1.2890792291220558e-05, 'epoch': 0.36}
? Step 666  Epoch: 0.36  Loss: 1.6488
                                                          36%|███▌      | 666/1868 [34:33:40<62:38:04, 187.59s/it] 36%|███▌      | 667/1868 [34:36:48<62:36:51, 187.69s/it]{'loss': 1.6488, 'grad_norm': 0.0, 'learning_rate': 1.2880085653104926e-05, 'epoch': 0.36}
? Step 667  Epoch: 0.36  Loss: 1.4750
                                                          36%|███▌      | 667/1868 [34:36:48<62:36:51, 187.69s/it] 36%|███▌      | 668/1868 [34:39:55<62:33:18, 187.67s/it]{'loss': 1.475, 'grad_norm': 0.0, 'learning_rate': 1.2869379014989293e-05, 'epoch': 0.36}
? Step 668  Epoch: 0.36  Loss: 1.6069
                                                          36%|███▌      | 668/1868 [34:39:55<62:33:18, 187.67s/it] 36%|███▌      | 669/1868 [34:43:03<62:30:06, 187.66s/it]{'loss': 1.6069, 'grad_norm': 0.0, 'learning_rate': 1.2858672376873662e-05, 'epoch': 0.36}
? Step 669  Epoch: 0.36  Loss: 1.5961
                                                          36%|███▌      | 669/1868 [34:43:03<62:30:06, 187.66s/it] 36%|███▌      | 670/1868 [34:46:11<62:28:01, 187.71s/it]{'loss': 1.5961, 'grad_norm': 0.0, 'learning_rate': 1.284796573875803e-05, 'epoch': 0.36}
? Step 670  Epoch: 0.36  Loss: 1.5344
                                                          36%|███▌      | 670/1868 [34:46:11<62:28:01, 187.71s/it] 36%|███▌      | 671/1868 [34:49:19<62:25:16, 187.73s/it]{'loss': 1.5344, 'grad_norm': 0.0, 'learning_rate': 1.28372591006424e-05, 'epoch': 0.36}
? Step 671  Epoch: 0.36  Loss: 1.4805
                                                          36%|███▌      | 671/1868 [34:49:19<62:25:16, 187.73s/it] 36%|███▌      | 672/1868 [34:52:27<62:24:15, 187.84s/it]{'loss': 1.4805, 'grad_norm': 0.0, 'learning_rate': 1.2826552462526766e-05, 'epoch': 0.36}
? Step 672  Epoch: 0.36  Loss: 1.5566
                                                          36%|███▌      | 672/1868 [34:52:27<62:24:15, 187.84s/it] 36%|███▌      | 673/1868 [34:55:34<62:18:50, 187.72s/it]{'loss': 1.5566, 'grad_norm': 0.0, 'learning_rate': 1.2815845824411135e-05, 'epoch': 0.36}
? Step 673  Epoch: 0.36  Loss: 1.5058
                                                          36%|███▌      | 673/1868 [34:55:34<62:18:50, 187.72s/it] 36%|███▌      | 674/1868 [34:58:42<62:16:04, 187.74s/it]{'loss': 1.5058, 'grad_norm': 0.0, 'learning_rate': 1.2805139186295504e-05, 'epoch': 0.36}
? Step 674  Epoch: 0.36  Loss: 1.4391
                                                          36%|███▌      | 674/1868 [34:58:42<62:16:04, 187.74s/it] 36%|███▌      | 675/1868 [35:01:50<62:13:08, 187.75s/it]{'loss': 1.4391, 'grad_norm': 0.0, 'learning_rate': 1.2794432548179872e-05, 'epoch': 0.36}
? Step 675  Epoch: 0.36  Loss: 1.4542
                                                          36%|███▌      | 675/1868 [35:01:50<62:13:08, 187.75s/it] 36%|███▌      | 676/1868 [35:04:58<62:10:59, 187.80s/it]{'loss': 1.4542, 'grad_norm': 0.0, 'learning_rate': 1.2783725910064243e-05, 'epoch': 0.36}
? Step 676  Epoch: 0.36  Loss: 1.5500
                                                          36%|███▌      | 676/1868 [35:04:58<62:10:59, 187.80s/it] 36%|███▌      | 677/1868 [35:08:05<62:06:42, 187.74s/it]{'loss': 1.55, 'grad_norm': 0.0, 'learning_rate': 1.277301927194861e-05, 'epoch': 0.36}
? Step 677  Epoch: 0.36  Loss: 1.5930
                                                          36%|███▌      | 677/1868 [35:08:05<62:06:42, 187.74s/it] 36%|███▋      | 678/1868 [35:11:13<62:01:33, 187.64s/it]{'loss': 1.593, 'grad_norm': 0.0, 'learning_rate': 1.2762312633832978e-05, 'epoch': 0.36}
? Step 678  Epoch: 0.36  Loss: 1.7575
                                                          36%|███▋      | 678/1868 [35:11:13<62:01:33, 187.64s/it] 36%|███▋      | 679/1868 [35:14:21<62:00:22, 187.74s/it]{'loss': 1.7575, 'grad_norm': 0.0, 'learning_rate': 1.2751605995717347e-05, 'epoch': 0.36}
? Step 679  Epoch: 0.36  Loss: 1.3480
                                                          36%|███▋      | 679/1868 [35:14:21<62:00:22, 187.74s/it] 36%|███▋      | 680/1868 [35:17:28<61:56:43, 187.71s/it]{'loss': 1.348, 'grad_norm': 0.0, 'learning_rate': 1.2740899357601716e-05, 'epoch': 0.36}
? Step 680  Epoch: 0.36  Loss: 1.5185
                                                          36%|███▋      | 680/1868 [35:17:28<61:56:43, 187.71s/it] 36%|███▋      | 681/1868 [35:20:36<61:53:48, 187.72s/it]{'loss': 1.5185, 'grad_norm': 0.0, 'learning_rate': 1.2730192719486083e-05, 'epoch': 0.36}
? Step 681  Epoch: 0.36  Loss: 1.4327
                                                          36%|███▋      | 681/1868 [35:20:36<61:53:48, 187.72s/it] 37%|███▋      | 682/1868 [35:23:44<61:52:31, 187.82s/it]{'loss': 1.4327, 'grad_norm': 0.0, 'learning_rate': 1.2719486081370451e-05, 'epoch': 0.36}
? Step 682  Epoch: 0.37  Loss: 1.4222
                                                          37%|███▋      | 682/1868 [35:23:44<61:52:31, 187.82s/it] 37%|███▋      | 683/1868 [35:26:52<61:50:01, 187.85s/it]{'loss': 1.4222, 'grad_norm': 0.0, 'learning_rate': 1.270877944325482e-05, 'epoch': 0.37}
? Step 683  Epoch: 0.37  Loss: 1.4864
                                                          37%|███▋      | 683/1868 [35:26:52<61:50:01, 187.85s/it] 37%|███▋      | 684/1868 [35:30:00<61:46:11, 187.81s/it]{'loss': 1.4864, 'grad_norm': 0.0, 'learning_rate': 1.2698072805139187e-05, 'epoch': 0.37}
? Step 684  Epoch: 0.37  Loss: 1.4136
                                                          37%|███▋      | 684/1868 [35:30:00<61:46:11, 187.81s/it] 37%|███▋      | 685/1868 [35:33:07<61:41:14, 187.72s/it]{'loss': 1.4136, 'grad_norm': 0.0, 'learning_rate': 1.2687366167023556e-05, 'epoch': 0.37}
? Step 685  Epoch: 0.37  Loss: 1.6616
                                                          37%|███▋      | 685/1868 [35:33:07<61:41:14, 187.72s/it] 37%|███▋      | 686/1868 [35:36:15<61:38:46, 187.76s/it]{'loss': 1.6616, 'grad_norm': 0.0, 'learning_rate': 1.2676659528907924e-05, 'epoch': 0.37}
? Step 686  Epoch: 0.37  Loss: 1.4534
                                                          37%|███▋      | 686/1868 [35:36:15<61:38:46, 187.76s/it] 37%|███▋      | 687/1868 [35:39:23<61:33:43, 187.66s/it]{'loss': 1.4534, 'grad_norm': 0.0, 'learning_rate': 1.2665952890792293e-05, 'epoch': 0.37}
? Step 687  Epoch: 0.37  Loss: 1.6174
                                                          37%|███▋      | 687/1868 [35:39:23<61:33:43, 187.66s/it] 37%|███▋      | 688/1868 [35:42:31<61:32:14, 187.74s/it]{'loss': 1.6174, 'grad_norm': 0.0, 'learning_rate': 1.265524625267666e-05, 'epoch': 0.37}
? Step 688  Epoch: 0.37  Loss: 1.4645
                                                          37%|███▋      | 688/1868 [35:42:31<61:32:14, 187.74s/it] 37%|███▋      | 689/1868 [35:45:38<61:30:00, 187.79s/it]{'loss': 1.4645, 'grad_norm': 0.0, 'learning_rate': 1.2644539614561029e-05, 'epoch': 0.37}
? Step 689  Epoch: 0.37  Loss: 1.4159
                                                          37%|███▋      | 689/1868 [35:45:38<61:30:00, 187.79s/it] 37%|███▋      | 690/1868 [35:48:46<61:24:44, 187.68s/it]{'loss': 1.4159, 'grad_norm': 0.0, 'learning_rate': 1.2633832976445397e-05, 'epoch': 0.37}
? Step 690  Epoch: 0.37  Loss: 1.5735
                                                          37%|███▋      | 690/1868 [35:48:46<61:24:44, 187.68s/it] 37%|███▋      | 691/1868 [35:51:53<61:21:22, 187.67s/it]{'loss': 1.5735, 'grad_norm': 0.0, 'learning_rate': 1.2623126338329766e-05, 'epoch': 0.37}
? Step 691  Epoch: 0.37  Loss: 1.7357
                                                          37%|███▋      | 691/1868 [35:51:53<61:21:22, 187.67s/it] 37%|███▋      | 692/1868 [35:55:01<61:19:31, 187.73s/it]{'loss': 1.7357, 'grad_norm': 0.0, 'learning_rate': 1.2612419700214133e-05, 'epoch': 0.37}
? Step 692  Epoch: 0.37  Loss: 1.7632
                                                          37%|███▋      | 692/1868 [35:55:01<61:19:31, 187.73s/it] 37%|███▋      | 693/1868 [35:58:09<61:16:22, 187.73s/it]{'loss': 1.7632, 'grad_norm': 0.0, 'learning_rate': 1.2601713062098502e-05, 'epoch': 0.37}
? Step 693  Epoch: 0.37  Loss: 1.6528
                                                          37%|███▋      | 693/1868 [35:58:09<61:16:22, 187.73s/it] 37%|███▋      | 694/1868 [36:01:17<61:13:10, 187.73s/it]{'loss': 1.6528, 'grad_norm': 0.0, 'learning_rate': 1.259100642398287e-05, 'epoch': 0.37}
? Step 694  Epoch: 0.37  Loss: 1.8720
                                                          37%|███▋      | 694/1868 [36:01:17<61:13:10, 187.73s/it] 37%|███▋      | 695/1868 [36:04:25<61:11:46, 187.81s/it]{'loss': 1.872, 'grad_norm': 0.0, 'learning_rate': 1.2580299785867239e-05, 'epoch': 0.37}
? Step 695  Epoch: 0.37  Loss: 1.6637
                                                          37%|███▋      | 695/1868 [36:04:25<61:11:46, 187.81s/it] 37%|███▋      | 696/1868 [36:07:32<61:07:32, 187.76s/it]{'loss': 1.6637, 'grad_norm': 0.0, 'learning_rate': 1.2569593147751606e-05, 'epoch': 0.37}
? Step 696  Epoch: 0.37  Loss: 1.7693
                                                          37%|███▋      | 696/1868 [36:07:32<61:07:32, 187.76s/it] 37%|███▋      | 697/1868 [36:10:40<61:02:14, 187.65s/it]{'loss': 1.7693, 'grad_norm': 0.0, 'learning_rate': 1.2558886509635975e-05, 'epoch': 0.37}
? Step 697  Epoch: 0.37  Loss: 1.5142
                                                          37%|███▋      | 697/1868 [36:10:40<61:02:14, 187.65s/it] 37%|███▋      | 698/1868 [36:13:48<61:00:44, 187.73s/it]{'loss': 1.5142, 'grad_norm': 0.0, 'learning_rate': 1.2548179871520343e-05, 'epoch': 0.37}
? Step 698  Epoch: 0.37  Loss: 1.4415
                                                          37%|███▋      | 698/1868 [36:13:48<61:00:44, 187.73s/it] 37%|███▋      | 699/1868 [36:16:56<60:58:46, 187.79s/it]{'loss': 1.4415, 'grad_norm': 0.0, 'learning_rate': 1.2537473233404712e-05, 'epoch': 0.37}
? Step 699  Epoch: 0.37  Loss: 1.3550
                                                          37%|███▋      | 699/1868 [36:16:56<60:58:46, 187.79s/it] 37%|███▋      | 700/1868 [36:20:03<60:54:18, 187.72s/it]{'loss': 1.355, 'grad_norm': 0.0, 'learning_rate': 1.2526766595289079e-05, 'epoch': 0.37}
? Step 700  Epoch: 0.37  Loss: 1.7553
                                                          37%|███▋      | 700/1868 [36:20:03<60:54:18, 187.72s/it]/home/dev25-01/mistral-env/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 38%|███▊      | 701/1868 [36:23:12<60:56:22, 187.99s/it]{'loss': 1.7553, 'grad_norm': 0.0, 'learning_rate': 1.2516059957173448e-05, 'epoch': 0.37}
? Step 701  Epoch: 0.38  Loss: 1.5935
                                                          38%|███▊      | 701/1868 [36:23:12<60:56:22, 187.99s/it] 38%|███▊      | 702/1868 [36:26:20<60:53:18, 187.99s/it]{'loss': 1.5935, 'grad_norm': 0.0, 'learning_rate': 1.2505353319057816e-05, 'epoch': 0.38}
? Step 702  Epoch: 0.38  Loss: 1.4457
                                                          38%|███▊      | 702/1868 [36:26:20<60:53:18, 187.99s/it] 38%|███▊      | 703/1868 [36:29:28<60:49:35, 187.96s/it]{'loss': 1.4457, 'grad_norm': 0.0, 'learning_rate': 1.2494646680942187e-05, 'epoch': 0.38}
? Step 703  Epoch: 0.38  Loss: 1.6993
                                                          38%|███▊      | 703/1868 [36:29:28<60:49:35, 187.96s/it] 38%|███▊      | 704/1868 [36:32:36<60:45:51, 187.93s/it]{'loss': 1.6993, 'grad_norm': 0.0, 'learning_rate': 1.2483940042826555e-05, 'epoch': 0.38}
? Step 704  Epoch: 0.38  Loss: 1.5471
                                                          38%|███▊      | 704/1868 [36:32:36<60:45:51, 187.93s/it] 38%|███▊      | 705/1868 [36:35:44<60:42:40, 187.93s/it]{'loss': 1.5471, 'grad_norm': 0.0, 'learning_rate': 1.2473233404710922e-05, 'epoch': 0.38}
? Step 705  Epoch: 0.38  Loss: 1.7982
                                                          38%|███▊      | 705/1868 [36:35:44<60:42:40, 187.93s/it] 38%|███▊      | 706/1868 [36:38:52<60:39:36, 187.93s/it]{'loss': 1.7982, 'grad_norm': 0.0, 'learning_rate': 1.2462526766595291e-05, 'epoch': 0.38}
? Step 706  Epoch: 0.38  Loss: 1.4648
                                                          38%|███▊      | 706/1868 [36:38:52<60:39:36, 187.93s/it] 38%|███▊      | 707/1868 [36:41:59<60:33:59, 187.80s/it]{'loss': 1.4648, 'grad_norm': 0.0, 'learning_rate': 1.245182012847966e-05, 'epoch': 0.38}
? Step 707  Epoch: 0.38  Loss: 1.5797
                                                          38%|███▊      | 707/1868 [36:41:59<60:33:59, 187.80s/it] 38%|███▊      | 708/1868 [36:45:07<60:31:12, 187.82s/it]{'loss': 1.5797, 'grad_norm': 0.0, 'learning_rate': 1.2441113490364027e-05, 'epoch': 0.38}
? Step 708  Epoch: 0.38  Loss: 1.3652
                                                          38%|███▊      | 708/1868 [36:45:07<60:31:12, 187.82s/it] 38%|███▊      | 709/1868 [36:48:15<60:28:26, 187.84s/it]{'loss': 1.3652, 'grad_norm': 0.0, 'learning_rate': 1.2430406852248395e-05, 'epoch': 0.38}
? Step 709  Epoch: 0.38  Loss: 1.3771
                                                          38%|███▊      | 709/1868 [36:48:15<60:28:26, 187.84s/it] 38%|███▊      | 710/1868 [36:51:23<60:26:41, 187.91s/it]{'loss': 1.3771, 'grad_norm': 0.0, 'learning_rate': 1.2419700214132764e-05, 'epoch': 0.38}
? Step 710  Epoch: 0.38  Loss: 1.4090
                                                          38%|███▊      | 710/1868 [36:51:23<60:26:41, 187.91s/it] 38%|███▊      | 711/1868 [36:54:31<60:24:28, 187.96s/it]{'loss': 1.409, 'grad_norm': 0.0, 'learning_rate': 1.2408993576017133e-05, 'epoch': 0.38}
? Step 711  Epoch: 0.38  Loss: 1.5697
                                                          38%|███▊      | 711/1868 [36:54:31<60:24:28, 187.96s/it] 38%|███▊      | 712/1868 [36:57:39<60:21:24, 187.96s/it]{'loss': 1.5697, 'grad_norm': 0.0, 'learning_rate': 1.23982869379015e-05, 'epoch': 0.38}
? Step 712  Epoch: 0.38  Loss: 1.5075
                                                          38%|███▊      | 712/1868 [36:57:39<60:21:24, 187.96s/it] 38%|███▊      | 713/1868 [37:00:47<60:17:47, 187.94s/it]{'loss': 1.5075, 'grad_norm': 0.0, 'learning_rate': 1.2387580299785868e-05, 'epoch': 0.38}
? Step 713  Epoch: 0.38  Loss: 1.3940
                                                          38%|███▊      | 713/1868 [37:00:47<60:17:47, 187.94s/it] 38%|███▊      | 714/1868 [37:03:54<60:13:13, 187.86s/it]{'loss': 1.394, 'grad_norm': 0.0, 'learning_rate': 1.2376873661670237e-05, 'epoch': 0.38}
? Step 714  Epoch: 0.38  Loss: 1.7887
                                                          38%|███▊      | 714/1868 [37:03:54<60:13:13, 187.86s/it] 38%|███▊      | 715/1868 [37:07:02<60:08:31, 187.78s/it]{'loss': 1.7887, 'grad_norm': 0.0, 'learning_rate': 1.2366167023554606e-05, 'epoch': 0.38}
? Step 715  Epoch: 0.38  Loss: 1.5921
                                                          38%|███▊      | 715/1868 [37:07:02<60:08:31, 187.78s/it] 38%|███▊      | 716/1868 [37:10:10<60:06:19, 187.83s/it]{'loss': 1.5921, 'grad_norm': 0.0, 'learning_rate': 1.2355460385438973e-05, 'epoch': 0.38}
? Step 716  Epoch: 0.38  Loss: 1.3576
                                                          38%|███▊      | 716/1868 [37:10:10<60:06:19, 187.83s/it] 38%|███▊      | 717/1868 [37:13:18<60:03:34, 187.85s/it]{'loss': 1.3576, 'grad_norm': 0.0, 'learning_rate': 1.2344753747323341e-05, 'epoch': 0.38}
? Step 717  Epoch: 0.38  Loss: 1.4551
                                                          38%|███▊      | 717/1868 [37:13:18<60:03:34, 187.85s/it] 38%|███▊      | 718/1868 [37:16:25<59:58:27, 187.75s/it]{'loss': 1.4551, 'grad_norm': 0.0, 'learning_rate': 1.233404710920771e-05, 'epoch': 0.38}
? Step 718  Epoch: 0.38  Loss: 1.5269
                                                          38%|███▊      | 718/1868 [37:16:25<59:58:27, 187.75s/it] 38%|███▊      | 719/1868 [37:19:33<59:56:21, 187.80s/it]{'loss': 1.5269, 'grad_norm': 0.0, 'learning_rate': 1.2323340471092079e-05, 'epoch': 0.38}
? Step 719  Epoch: 0.38  Loss: 1.5284
                                                          38%|███▊      | 719/1868 [37:19:33<59:56:21, 187.80s/it] 39%|███▊      | 720/1868 [37:22:41<59:52:15, 187.75s/it]{'loss': 1.5284, 'grad_norm': 0.0, 'learning_rate': 1.2312633832976446e-05, 'epoch': 0.38}
? Step 720  Epoch: 0.39  Loss: 1.8778
                                                          39%|███▊      | 720/1868 [37:22:41<59:52:15, 187.75s/it] 39%|███▊      | 721/1868 [37:25:49<59:50:55, 187.84s/it]{'loss': 1.8778, 'grad_norm': 0.0, 'learning_rate': 1.2301927194860814e-05, 'epoch': 0.39}
? Step 721  Epoch: 0.39  Loss: 1.6316
                                                          39%|███▊      | 721/1868 [37:25:49<59:50:55, 187.84s/it] 39%|███▊      | 722/1868 [37:28:57<59:46:00, 187.75s/it]{'loss': 1.6316, 'grad_norm': 0.0, 'learning_rate': 1.2291220556745183e-05, 'epoch': 0.39}
? Step 722  Epoch: 0.39  Loss: 1.5883
                                                          39%|███▊      | 722/1868 [37:28:57<59:46:00, 187.75s/it] 39%|███▊      | 723/1868 [37:32:05<59:45:49, 187.90s/it]{'loss': 1.5883, 'grad_norm': 0.0, 'learning_rate': 1.2280513918629552e-05, 'epoch': 0.39}
? Step 723  Epoch: 0.39  Loss: 1.4983
                                                          39%|███▊      | 723/1868 [37:32:05<59:45:49, 187.90s/it] 39%|███▉      | 724/1868 [37:35:13<59:42:05, 187.87s/it]{'loss': 1.4983, 'grad_norm': 0.0, 'learning_rate': 1.2269807280513919e-05, 'epoch': 0.39}
? Step 724  Epoch: 0.39  Loss: 1.3253
                                                          39%|███▉      | 724/1868 [37:35:13<59:42:05, 187.87s/it] 39%|███▉      | 725/1868 [37:38:20<59:37:55, 187.82s/it]{'loss': 1.3253, 'grad_norm': 0.0, 'learning_rate': 1.2259100642398287e-05, 'epoch': 0.39}
? Step 725  Epoch: 0.39  Loss: 1.5989
                                                          39%|███▉      | 725/1868 [37:38:20<59:37:55, 187.82s/it] 39%|███▉      | 726/1868 [37:41:28<59:33:16, 187.74s/it]{'loss': 1.5989, 'grad_norm': 0.0, 'learning_rate': 1.2248394004282656e-05, 'epoch': 0.39}
? Step 726  Epoch: 0.39  Loss: 1.4927
                                                          39%|███▉      | 726/1868 [37:41:28<59:33:16, 187.74s/it] 39%|███▉      | 727/1868 [37:44:35<59:27:59, 187.62s/it]{'loss': 1.4927, 'grad_norm': 0.0, 'learning_rate': 1.2237687366167023e-05, 'epoch': 0.39}
? Step 727  Epoch: 0.39  Loss: 1.8165
                                                          39%|███▉      | 727/1868 [37:44:35<59:27:59, 187.62s/it] 39%|███▉      | 728/1868 [37:47:43<59:25:05, 187.64s/it]{'loss': 1.8165, 'grad_norm': 0.0, 'learning_rate': 1.2226980728051392e-05, 'epoch': 0.39}
? Step 728  Epoch: 0.39  Loss: 1.8202
                                                          39%|███▉      | 728/1868 [37:47:43<59:25:05, 187.64s/it] 39%|███▉      | 729/1868 [37:50:50<59:21:55, 187.63s/it]{'loss': 1.8202, 'grad_norm': 0.0, 'learning_rate': 1.221627408993576e-05, 'epoch': 0.39}
? Step 729  Epoch: 0.39  Loss: 1.6476
                                                          39%|███▉      | 729/1868 [37:50:50<59:21:55, 187.63s/it] 39%|███▉      | 730/1868 [37:53:58<59:19:24, 187.67s/it]{'loss': 1.6476, 'grad_norm': 0.0, 'learning_rate': 1.220556745182013e-05, 'epoch': 0.39}
? Step 730  Epoch: 0.39  Loss: 1.7894
                                                          39%|███▉      | 730/1868 [37:53:58<59:19:24, 187.67s/it] 39%|███▉      | 731/1868 [37:57:06<59:16:06, 187.66s/it]{'loss': 1.7894, 'grad_norm': 0.0, 'learning_rate': 1.21948608137045e-05, 'epoch': 0.39}
? Step 731  Epoch: 0.39  Loss: 1.4216
                                                          39%|███▉      | 731/1868 [37:57:06<59:16:06, 187.66s/it] 39%|███▉      | 732/1868 [38:00:14<59:13:17, 187.67s/it]{'loss': 1.4216, 'grad_norm': 0.0, 'learning_rate': 1.2184154175588866e-05, 'epoch': 0.39}
? Step 732  Epoch: 0.39  Loss: 1.4916
                                                          39%|███▉      | 732/1868 [38:00:14<59:13:17, 187.67s/it] 39%|███▉      | 733/1868 [38:03:21<59:09:21, 187.63s/it]{'loss': 1.4916, 'grad_norm': 0.0, 'learning_rate': 1.2173447537473235e-05, 'epoch': 0.39}
? Step 733  Epoch: 0.39  Loss: 1.4696
                                                          39%|███▉      | 733/1868 [38:03:21<59:09:21, 187.63s/it] 39%|███▉      | 734/1868 [38:06:29<59:06:14, 187.63s/it]{'loss': 1.4696, 'grad_norm': 0.0, 'learning_rate': 1.2162740899357604e-05, 'epoch': 0.39}
? Step 734  Epoch: 0.39  Loss: 1.7914
                                                          39%|███▉      | 734/1868 [38:06:29<59:06:14, 187.63s/it] 39%|███▉      | 735/1868 [38:09:36<59:03:25, 187.65s/it]{'loss': 1.7914, 'grad_norm': 0.0, 'learning_rate': 1.2152034261241972e-05, 'epoch': 0.39}
? Step 735  Epoch: 0.39  Loss: 1.9227
                                                          39%|███▉      | 735/1868 [38:09:36<59:03:25, 187.65s/it] 39%|███▉      | 736/1868 [38:12:44<58:59:46, 187.62s/it]{'loss': 1.9227, 'grad_norm': 0.0, 'learning_rate': 1.214132762312634e-05, 'epoch': 0.39}
? Step 736  Epoch: 0.39  Loss: 1.7634
                                                          39%|███▉      | 736/1868 [38:12:44<58:59:46, 187.62s/it] 39%|███▉      | 737/1868 [38:15:51<58:55:18, 187.55s/it]{'loss': 1.7634, 'grad_norm': 0.0, 'learning_rate': 1.2130620985010708e-05, 'epoch': 0.39}
? Step 737  Epoch: 0.39  Loss: 1.4678
                                                          39%|███▉      | 737/1868 [38:15:51<58:55:18, 187.55s/it] 40%|███▉      | 738/1868 [38:18:59<58:51:37, 187.52s/it]{'loss': 1.4678, 'grad_norm': 0.0, 'learning_rate': 1.2119914346895077e-05, 'epoch': 0.39}
? Step 738  Epoch: 0.40  Loss: 1.7492
                                                          40%|███▉      | 738/1868 [38:18:59<58:51:37, 187.52s/it] 40%|███▉      | 739/1868 [38:22:07<58:49:48, 187.59s/it]{'loss': 1.7492, 'grad_norm': 0.0, 'learning_rate': 1.2109207708779445e-05, 'epoch': 0.4}
? Step 739  Epoch: 0.40  Loss: 1.5315
                                                          40%|███▉      | 739/1868 [38:22:07<58:49:48, 187.59s/it] 40%|███▉      | 740/1868 [38:25:14<58:46:56, 187.60s/it]{'loss': 1.5315, 'grad_norm': 0.0, 'learning_rate': 1.2098501070663812e-05, 'epoch': 0.4}
? Step 740  Epoch: 0.40  Loss: 1.5746
                                                          40%|███▉      | 740/1868 [38:25:14<58:46:56, 187.60s/it] 40%|███▉      | 741/1868 [38:28:21<58:41:53, 187.50s/it]{'loss': 1.5746, 'grad_norm': 0.0, 'learning_rate': 1.2087794432548181e-05, 'epoch': 0.4}
? Step 741  Epoch: 0.40  Loss: 1.6186
                                                          40%|███▉      | 741/1868 [38:28:21<58:41:53, 187.50s/it] 40%|███▉      | 742/1868 [38:31:29<58:37:14, 187.42s/it]{'loss': 1.6186, 'grad_norm': 0.0, 'learning_rate': 1.207708779443255e-05, 'epoch': 0.4}
? Step 742  Epoch: 0.40  Loss: 1.8719
                                                          40%|███▉      | 742/1868 [38:31:29<58:37:14, 187.42s/it] 40%|███▉      | 743/1868 [38:34:36<58:34:07, 187.42s/it]{'loss': 1.8719, 'grad_norm': 0.0, 'learning_rate': 1.2066381156316918e-05, 'epoch': 0.4}
? Step 743  Epoch: 0.40  Loss: 1.5006
                                                          40%|███▉      | 743/1868 [38:34:36<58:34:07, 187.42s/it] 40%|███▉      | 744/1868 [38:37:44<58:32:26, 187.50s/it]{'loss': 1.5006, 'grad_norm': 0.0, 'learning_rate': 1.2055674518201285e-05, 'epoch': 0.4}
? Step 744  Epoch: 0.40  Loss: 1.4275
                                                          40%|███▉      | 744/1868 [38:37:44<58:32:26, 187.50s/it] 40%|███▉      | 745/1868 [38:40:51<58:29:59, 187.53s/it]{'loss': 1.4275, 'grad_norm': 0.0, 'learning_rate': 1.2044967880085654e-05, 'epoch': 0.4}
? Step 745  Epoch: 0.40  Loss: 1.5535
                                                          40%|███▉      | 745/1868 [38:40:51<58:29:59, 187.53s/it] 40%|███▉      | 746/1868 [38:43:59<58:26:31, 187.51s/it]{'loss': 1.5535, 'grad_norm': 0.0, 'learning_rate': 1.2034261241970023e-05, 'epoch': 0.4}
? Step 746  Epoch: 0.40  Loss: 1.5829
                                                          40%|███▉      | 746/1868 [38:43:59<58:26:31, 187.51s/it] 40%|███▉      | 747/1868 [38:47:06<58:23:04, 187.50s/it]{'loss': 1.5829, 'grad_norm': 0.0, 'learning_rate': 1.2023554603854391e-05, 'epoch': 0.4}
? Step 747  Epoch: 0.40  Loss: 1.8938
                                                          40%|███▉      | 747/1868 [38:47:06<58:23:04, 187.50s/it] 40%|████      | 748/1868 [38:50:14<58:20:23, 187.52s/it]{'loss': 1.8938, 'grad_norm': 0.0, 'learning_rate': 1.2012847965738758e-05, 'epoch': 0.4}
? Step 748  Epoch: 0.40  Loss: 1.7871
                                                          40%|████      | 748/1868 [38:50:14<58:20:23, 187.52s/it] 40%|████      | 749/1868 [38:53:22<58:18:31, 187.59s/it]{'loss': 1.7871, 'grad_norm': 0.0, 'learning_rate': 1.2002141327623127e-05, 'epoch': 0.4}
? Step 749  Epoch: 0.40  Loss: 1.5101
                                                          40%|████      | 749/1868 [38:53:22<58:18:31, 187.59s/it] 40%|████      | 750/1868 [38:56:29<58:15:47, 187.61s/it]{'loss': 1.5101, 'grad_norm': 0.0, 'learning_rate': 1.1991434689507496e-05, 'epoch': 0.4}
? Step 750  Epoch: 0.40  Loss: 1.5394
                                                          40%|████      | 750/1868 [38:56:29<58:15:47, 187.61s/it] 40%|████      | 751/1868 [38:59:37<58:11:58, 187.57s/it]{'loss': 1.5394, 'grad_norm': 0.0, 'learning_rate': 1.1980728051391863e-05, 'epoch': 0.4}
? Step 751  Epoch: 0.40  Loss: 1.7482
                                                          40%|████      | 751/1868 [38:59:37<58:11:58, 187.57s/it] 40%|████      | 752/1868 [39:02:44<58:09:14, 187.59s/it]{'loss': 1.7482, 'grad_norm': 0.0, 'learning_rate': 1.1970021413276231e-05, 'epoch': 0.4}
? Step 752  Epoch: 0.40  Loss: 1.4722
                                                          40%|████      | 752/1868 [39:02:44<58:09:14, 187.59s/it] 40%|████      | 753/1868 [39:05:52<58:05:36, 187.57s/it]{'loss': 1.4722, 'grad_norm': 0.0, 'learning_rate': 1.19593147751606e-05, 'epoch': 0.4}
? Step 753  Epoch: 0.40  Loss: 1.4246
                                                          40%|████      | 753/1868 [39:05:52<58:05:36, 187.57s/it] 40%|████      | 754/1868 [39:09:00<58:03:29, 187.62s/it]{'loss': 1.4246, 'grad_norm': 0.0, 'learning_rate': 1.1948608137044969e-05, 'epoch': 0.4}
? Step 754  Epoch: 0.40  Loss: 1.6282
                                                          40%|████      | 754/1868 [39:09:00<58:03:29, 187.62s/it] 40%|████      | 755/1868 [39:12:07<58:00:13, 187.61s/it]{'loss': 1.6282, 'grad_norm': 0.0, 'learning_rate': 1.1937901498929336e-05, 'epoch': 0.4}
? Step 755  Epoch: 0.40  Loss: 1.6602
                                                          40%|████      | 755/1868 [39:12:07<58:00:13, 187.61s/it] 40%|████      | 756/1868 [39:15:15<57:55:50, 187.55s/it]{'loss': 1.6602, 'grad_norm': 0.0, 'learning_rate': 1.1927194860813704e-05, 'epoch': 0.4}
? Step 756  Epoch: 0.40  Loss: 1.7031
                                                          40%|████      | 756/1868 [39:15:15<57:55:50, 187.55s/it] 41%|████      | 757/1868 [39:18:22<57:52:10, 187.52s/it]{'loss': 1.7031, 'grad_norm': 0.0, 'learning_rate': 1.1916488222698073e-05, 'epoch': 0.4}
? Step 757  Epoch: 0.41  Loss: 1.6707
                                                          41%|████      | 757/1868 [39:18:22<57:52:10, 187.52s/it] 41%|████      | 758/1868 [39:21:30<57:48:58, 187.51s/it]{'loss': 1.6707, 'grad_norm': 0.0, 'learning_rate': 1.1905781584582443e-05, 'epoch': 0.41}
? Step 758  Epoch: 0.41  Loss: 1.6092
                                                          41%|████      | 758/1868 [39:21:30<57:48:58, 187.51s/it] 41%|████      | 759/1868 [39:24:37<57:46:47, 187.56s/it]{'loss': 1.6092, 'grad_norm': 0.0, 'learning_rate': 1.1895074946466812e-05, 'epoch': 0.41}
? Step 759  Epoch: 0.41  Loss: 1.6925
                                                          41%|████      | 759/1868 [39:24:37<57:46:47, 187.56s/it] 41%|████      | 760/1868 [39:27:45<57:43:21, 187.55s/it]{'loss': 1.6925, 'grad_norm': 0.0, 'learning_rate': 1.1884368308351179e-05, 'epoch': 0.41}
? Step 760  Epoch: 0.41  Loss: 1.7837
                                                          41%|████      | 760/1868 [39:27:45<57:43:21, 187.55s/it] 41%|████      | 761/1868 [39:30:52<57:40:06, 187.54s/it]{'loss': 1.7837, 'grad_norm': 0.0, 'learning_rate': 1.1873661670235548e-05, 'epoch': 0.41}
? Step 761  Epoch: 0.41  Loss: 1.3560
                                                          41%|████      | 761/1868 [39:30:52<57:40:06, 187.54s/it] 41%|████      | 762/1868 [39:34:00<57:36:21, 187.51s/it]{'loss': 1.356, 'grad_norm': 0.0, 'learning_rate': 1.1862955032119916e-05, 'epoch': 0.41}
? Step 762  Epoch: 0.41  Loss: 1.7324
                                                          41%|████      | 762/1868 [39:34:00<57:36:21, 187.51s/it] 41%|████      | 763/1868 [39:37:07<57:34:10, 187.56s/it]{'loss': 1.7324, 'grad_norm': 0.0, 'learning_rate': 1.1852248394004285e-05, 'epoch': 0.41}
? Step 763  Epoch: 0.41  Loss: 1.7864
                                                          41%|████      | 763/1868 [39:37:07<57:34:10, 187.56s/it] 41%|████      | 764/1868 [39:40:15<57:31:22, 187.57s/it]{'loss': 1.7864, 'grad_norm': 0.0, 'learning_rate': 1.1841541755888652e-05, 'epoch': 0.41}
? Step 764  Epoch: 0.41  Loss: 1.6529
                                                          41%|████      | 764/1868 [39:40:15<57:31:22, 187.57s/it] 41%|████      | 765/1868 [39:43:23<57:28:17, 187.58s/it]{'loss': 1.6529, 'grad_norm': 0.0, 'learning_rate': 1.183083511777302e-05, 'epoch': 0.41}
? Step 765  Epoch: 0.41  Loss: 1.4487
                                                          41%|████      | 765/1868 [39:43:23<57:28:17, 187.58s/it] 41%|████      | 766/1868 [39:46:30<57:25:37, 187.60s/it]{'loss': 1.4487, 'grad_norm': 0.0, 'learning_rate': 1.182012847965739e-05, 'epoch': 0.41}
? Step 766  Epoch: 0.41  Loss: 1.5908
                                                          41%|████      | 766/1868 [39:46:30<57:25:37, 187.60s/it] 41%|████      | 767/1868 [39:49:38<57:21:42, 187.56s/it]{'loss': 1.5908, 'grad_norm': 0.0, 'learning_rate': 1.1809421841541758e-05, 'epoch': 0.41}
? Step 767  Epoch: 0.41  Loss: 1.5685
                                                          41%|████      | 767/1868 [39:49:38<57:21:42, 187.56s/it] 41%|████      | 768/1868 [39:52:45<57:18:11, 187.54s/it]{'loss': 1.5685, 'grad_norm': 0.0, 'learning_rate': 1.1798715203426125e-05, 'epoch': 0.41}
? Step 768  Epoch: 0.41  Loss: 1.5220
                                                          41%|████      | 768/1868 [39:52:45<57:18:11, 187.54s/it] 41%|████      | 769/1868 [39:55:53<57:14:27, 187.50s/it]{'loss': 1.522, 'grad_norm': 0.0, 'learning_rate': 1.1788008565310494e-05, 'epoch': 0.41}
? Step 769  Epoch: 0.41  Loss: 1.6907
                                                          41%|████      | 769/1868 [39:55:53<57:14:27, 187.50s/it] 41%|████      | 770/1868 [39:59:00<57:10:11, 187.44s/it]{'loss': 1.6907, 'grad_norm': 0.0, 'learning_rate': 1.1777301927194862e-05, 'epoch': 0.41}
? Step 770  Epoch: 0.41  Loss: 1.6148
                                                          41%|████      | 770/1868 [39:59:00<57:10:11, 187.44s/it] 41%|████▏     | 771/1868 [40:02:07<57:06:10, 187.39s/it]{'loss': 1.6148, 'grad_norm': 0.0, 'learning_rate': 1.1766595289079231e-05, 'epoch': 0.41}
? Step 771  Epoch: 0.41  Loss: 1.8629
                                                          41%|████▏     | 771/1868 [40:02:07<57:06:10, 187.39s/it] 41%|████▏     | 772/1868 [40:05:15<57:02:44, 187.38s/it]{'loss': 1.8629, 'grad_norm': 0.0, 'learning_rate': 1.1755888650963598e-05, 'epoch': 0.41}
? Step 772  Epoch: 0.41  Loss: 1.5897
                                                          41%|████▏     | 772/1868 [40:05:15<57:02:44, 187.38s/it] 41%|████▏     | 773/1868 [40:08:22<56:58:28, 187.31s/it]{'loss': 1.5897, 'grad_norm': 0.0, 'learning_rate': 1.1745182012847967e-05, 'epoch': 0.41}
? Step 773  Epoch: 0.41  Loss: 1.6120
                                                          41%|████▏     | 773/1868 [40:08:22<56:58:28, 187.31s/it] 41%|████▏     | 774/1868 [40:11:29<56:53:31, 187.21s/it]{'loss': 1.612, 'grad_norm': 0.0, 'learning_rate': 1.1734475374732335e-05, 'epoch': 0.41}
? Step 774  Epoch: 0.41  Loss: 1.9789
                                                          41%|████▏     | 774/1868 [40:11:29<56:53:31, 187.21s/it] 41%|████▏     | 775/1868 [40:14:36<56:52:31, 187.33s/it]{'loss': 1.9789, 'grad_norm': 0.0, 'learning_rate': 1.1723768736616702e-05, 'epoch': 0.41}
? Step 775  Epoch: 0.41  Loss: 1.6723
                                                          41%|████▏     | 775/1868 [40:14:36<56:52:31, 187.33s/it] 42%|████▏     | 776/1868 [40:17:44<56:49:42, 187.35s/it]{'loss': 1.6723, 'grad_norm': 0.0, 'learning_rate': 1.1713062098501071e-05, 'epoch': 0.41}
? Step 776  Epoch: 0.42  Loss: 1.6466
                                                          42%|████▏     | 776/1868 [40:17:44<56:49:42, 187.35s/it] 42%|████▏     | 777/1868 [40:20:51<56:44:56, 187.26s/it]{'loss': 1.6466, 'grad_norm': 0.0, 'learning_rate': 1.170235546038544e-05, 'epoch': 0.42}
? Step 777  Epoch: 0.42  Loss: 1.7833
                                                          42%|████▏     | 777/1868 [40:20:51<56:44:56, 187.26s/it] 42%|████▏     | 778/1868 [40:23:59<56:45:04, 187.44s/it]{'loss': 1.7833, 'grad_norm': 0.0, 'learning_rate': 1.1691648822269808e-05, 'epoch': 0.42}
? Step 778  Epoch: 0.42  Loss: 1.7427
                                                          42%|████▏     | 778/1868 [40:23:59<56:45:04, 187.44s/it] 42%|████▏     | 779/1868 [40:27:06<56:41:22, 187.40s/it]{'loss': 1.7427, 'grad_norm': 0.0, 'learning_rate': 1.1680942184154175e-05, 'epoch': 0.42}
? Step 779  Epoch: 0.42  Loss: 1.8225
                                                          42%|████▏     | 779/1868 [40:27:06<56:41:22, 187.40s/it] 42%|████▏     | 780/1868 [40:30:13<56:38:57, 187.44s/it]{'loss': 1.8225, 'grad_norm': 0.0, 'learning_rate': 1.1670235546038544e-05, 'epoch': 0.42}
? Step 780  Epoch: 0.42  Loss: 1.5748
                                                          42%|████▏     | 780/1868 [40:30:13<56:38:57, 187.44s/it] 42%|████▏     | 781/1868 [40:33:21<56:35:40, 187.43s/it]{'loss': 1.5748, 'grad_norm': 0.0, 'learning_rate': 1.1659528907922913e-05, 'epoch': 0.42}
? Step 781  Epoch: 0.42  Loss: 1.7525
                                                          42%|████▏     | 781/1868 [40:33:21<56:35:40, 187.43s/it] 42%|████▏     | 782/1868 [40:36:29<56:34:14, 187.53s/it]{'loss': 1.7525, 'grad_norm': 0.0, 'learning_rate': 1.1648822269807281e-05, 'epoch': 0.42}
? Step 782  Epoch: 0.42  Loss: 1.4690
                                                          42%|████▏     | 782/1868 [40:36:29<56:34:14, 187.53s/it] 42%|████▏     | 783/1868 [40:39:36<56:31:33, 187.55s/it]{'loss': 1.469, 'grad_norm': 0.0, 'learning_rate': 1.1638115631691648e-05, 'epoch': 0.42}
? Step 783  Epoch: 0.42  Loss: 1.5462
                                                          42%|████▏     | 783/1868 [40:39:36<56:31:33, 187.55s/it] 42%|████▏     | 784/1868 [40:42:44<56:27:22, 187.49s/it]{'loss': 1.5462, 'grad_norm': 0.0, 'learning_rate': 1.1627408993576017e-05, 'epoch': 0.42}
? Step 784  Epoch: 0.42  Loss: 1.7137
                                                          42%|████▏     | 784/1868 [40:42:44<56:27:22, 187.49s/it] 42%|████▏     | 785/1868 [40:45:51<56:22:57, 187.42s/it]{'loss': 1.7137, 'grad_norm': 0.0, 'learning_rate': 1.1616702355460387e-05, 'epoch': 0.42}
? Step 785  Epoch: 0.42  Loss: 1.5412
                                                          42%|████▏     | 785/1868 [40:45:51<56:22:57, 187.42s/it] 42%|████▏     | 786/1868 [40:48:58<56:17:59, 187.32s/it]{'loss': 1.5412, 'grad_norm': 0.0, 'learning_rate': 1.1605995717344756e-05, 'epoch': 0.42}
? Step 786  Epoch: 0.42  Loss: 1.4420
                                                          42%|████▏     | 786/1868 [40:48:58<56:17:59, 187.32s/it] 42%|████▏     | 787/1868 [40:52:05<56:14:29, 187.30s/it]{'loss': 1.442, 'grad_norm': 0.0, 'learning_rate': 1.1595289079229125e-05, 'epoch': 0.42}
? Step 787  Epoch: 0.42  Loss: 1.5357
                                                          42%|████▏     | 787/1868 [40:52:05<56:14:29, 187.30s/it] 42%|████▏     | 788/1868 [40:55:12<56:09:24, 187.19s/it]{'loss': 1.5357, 'grad_norm': 0.0, 'learning_rate': 1.1584582441113492e-05, 'epoch': 0.42}
? Step 788  Epoch: 0.42  Loss: 1.7190
                                                          42%|████▏     | 788/1868 [40:55:12<56:09:24, 187.19s/it] 42%|████▏     | 789/1868 [40:58:19<56:04:21, 187.08s/it]{'loss': 1.719, 'grad_norm': 0.0, 'learning_rate': 1.157387580299786e-05, 'epoch': 0.42}
? Step 789  Epoch: 0.42  Loss: 2.3323
                                                          42%|████▏     | 789/1868 [40:58:19<56:04:21, 187.08s/it] 42%|████▏     | 790/1868 [41:01:26<56:01:56, 187.12s/it]{'loss': 2.3323, 'grad_norm': 0.0, 'learning_rate': 1.1563169164882229e-05, 'epoch': 0.42}
? Step 790  Epoch: 0.42  Loss: 1.7193
                                                          42%|████▏     | 790/1868 [41:01:26<56:01:56, 187.12s/it] 42%|████▏     | 791/1868 [41:04:33<55:58:32, 187.11s/it]{'loss': 1.7193, 'grad_norm': 0.0, 'learning_rate': 1.1552462526766598e-05, 'epoch': 0.42}
? Step 791  Epoch: 0.42  Loss: 1.4185
                                                          42%|████▏     | 791/1868 [41:04:33<55:58:32, 187.11s/it] 42%|████▏     | 792/1868 [41:07:40<55:55:45, 187.12s/it]{'loss': 1.4185, 'grad_norm': 0.0, 'learning_rate': 1.1541755888650965e-05, 'epoch': 0.42}
? Step 792  Epoch: 0.42  Loss: 1.5112
                                                          42%|████▏     | 792/1868 [41:07:40<55:55:45, 187.12s/it] 42%|████▏     | 793/1868 [41:10:48<55:55:25, 187.28s/it]{'loss': 1.5112, 'grad_norm': 0.0, 'learning_rate': 1.1531049250535333e-05, 'epoch': 0.42}
? Step 793  Epoch: 0.42  Loss: 1.4734
                                                          42%|████▏     | 793/1868 [41:10:48<55:55:25, 187.28s/it] 43%|████▎     | 794/1868 [41:13:56<55:53:45, 187.36s/it]{'loss': 1.4734, 'grad_norm': 0.0, 'learning_rate': 1.1520342612419702e-05, 'epoch': 0.42}
? Step 794  Epoch: 0.43  Loss: 1.6898
                                                          43%|████▎     | 794/1868 [41:13:56<55:53:45, 187.36s/it] 43%|████▎     | 795/1868 [41:17:03<55:49:19, 187.29s/it]{'loss': 1.6898, 'grad_norm': 0.0, 'learning_rate': 1.150963597430407e-05, 'epoch': 0.43}
? Step 795  Epoch: 0.43  Loss: 1.4313
                                                          43%|████▎     | 795/1868 [41:17:03<55:49:19, 187.29s/it] 43%|████▎     | 796/1868 [41:20:10<55:45:53, 187.27s/it]{'loss': 1.4313, 'grad_norm': 0.0, 'learning_rate': 1.1498929336188438e-05, 'epoch': 0.43}
? Step 796  Epoch: 0.43  Loss: 1.5765
                                                          43%|████▎     | 796/1868 [41:20:10<55:45:53, 187.27s/it] 43%|████▎     | 797/1868 [41:23:17<55:41:55, 187.22s/it]{'loss': 1.5765, 'grad_norm': 0.0, 'learning_rate': 1.1488222698072806e-05, 'epoch': 0.43}
? Step 797  Epoch: 0.43  Loss: 1.6614
                                                          43%|████▎     | 797/1868 [41:23:17<55:41:55, 187.22s/it] 43%|████▎     | 798/1868 [41:26:24<55:38:43, 187.22s/it]{'loss': 1.6614, 'grad_norm': 0.0, 'learning_rate': 1.1477516059957175e-05, 'epoch': 0.43}
? Step 798  Epoch: 0.43  Loss: 1.5283
                                                          43%|████▎     | 798/1868 [41:26:24<55:38:43, 187.22s/it] 43%|████▎     | 799/1868 [41:29:31<55:34:28, 187.15s/it]{'loss': 1.5283, 'grad_norm': 0.0, 'learning_rate': 1.1466809421841542e-05, 'epoch': 0.43}
? Step 799  Epoch: 0.43  Loss: 1.5000
                                                          43%|████▎     | 799/1868 [41:29:31<55:34:28, 187.15s/it] 43%|████▎     | 800/1868 [41:32:38<55:30:12, 187.09s/it]{'loss': 1.5, 'grad_norm': 0.0, 'learning_rate': 1.145610278372591e-05, 'epoch': 0.43}
? Step 800  Epoch: 0.43  Loss: 1.6872
                                                          43%|████▎     | 800/1868 [41:32:38<55:30:12, 187.09s/it]/home/dev25-01/mistral-env/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 43%|████▎     | 801/1868 [41:35:46<55:30:06, 187.26s/it]{'loss': 1.6872, 'grad_norm': 0.0, 'learning_rate': 1.144539614561028e-05, 'epoch': 0.43}
? Step 801  Epoch: 0.43  Loss: 1.3975
                                                          43%|████▎     | 801/1868 [41:35:46<55:30:06, 187.26s/it] 43%|████▎     | 802/1868 [41:38:53<55:26:01, 187.21s/it]{'loss': 1.3975, 'grad_norm': 0.0, 'learning_rate': 1.1434689507494648e-05, 'epoch': 0.43}
? Step 802  Epoch: 0.43  Loss: 1.4534
                                                          43%|████▎     | 802/1868 [41:38:53<55:26:01, 187.21s/it] 43%|████▎     | 803/1868 [41:42:00<55:22:01, 187.16s/it]{'loss': 1.4534, 'grad_norm': 0.0, 'learning_rate': 1.1423982869379015e-05, 'epoch': 0.43}
? Step 803  Epoch: 0.43  Loss: 1.6870
                                                          43%|████▎     | 803/1868 [41:42:00<55:22:01, 187.16s/it] 43%|████▎     | 804/1868 [41:45:07<55:19:04, 187.17s/it]{'loss': 1.687, 'grad_norm': 0.0, 'learning_rate': 1.1413276231263384e-05, 'epoch': 0.43}
? Step 804  Epoch: 0.43  Loss: 1.3417
                                                          43%|████▎     | 804/1868 [41:45:07<55:19:04, 187.17s/it] 43%|████▎     | 805/1868 [41:48:15<55:17:04, 187.23s/it]{'loss': 1.3417, 'grad_norm': 0.0, 'learning_rate': 1.1402569593147752e-05, 'epoch': 0.43}
? Step 805  Epoch: 0.43  Loss: 1.6137
                                                          43%|████▎     | 805/1868 [41:48:15<55:17:04, 187.23s/it] 43%|████▎     | 806/1868 [41:51:22<55:14:59, 187.29s/it]{'loss': 1.6137, 'grad_norm': 0.0, 'learning_rate': 1.1391862955032121e-05, 'epoch': 0.43}
? Step 806  Epoch: 0.43  Loss: 1.4456
                                                          43%|████▎     | 806/1868 [41:51:22<55:14:59, 187.29s/it] 43%|████▎     | 807/1868 [41:54:29<55:11:28, 187.27s/it]{'loss': 1.4456, 'grad_norm': 0.0, 'learning_rate': 1.1381156316916488e-05, 'epoch': 0.43}
? Step 807  Epoch: 0.43  Loss: 1.6507
                                                          43%|████▎     | 807/1868 [41:54:29<55:11:28, 187.27s/it] 43%|████▎     | 808/1868 [41:57:36<55:08:17, 187.26s/it]{'loss': 1.6507, 'grad_norm': 0.0, 'learning_rate': 1.1370449678800857e-05, 'epoch': 0.43}
? Step 808  Epoch: 0.43  Loss: 1.5801
                                                          43%|████▎     | 808/1868 [41:57:36<55:08:17, 187.26s/it] 43%|████▎     | 809/1868 [42:00:44<55:05:21, 187.27s/it]{'loss': 1.5801, 'grad_norm': 0.0, 'learning_rate': 1.1359743040685225e-05, 'epoch': 0.43}
? Step 809  Epoch: 0.43  Loss: 1.5312
                                                          43%|████▎     | 809/1868 [42:00:44<55:05:21, 187.27s/it] 43%|████▎     | 810/1868 [42:03:51<55:02:35, 187.29s/it]{'loss': 1.5312, 'grad_norm': 0.0, 'learning_rate': 1.1349036402569594e-05, 'epoch': 0.43}
? Step 810  Epoch: 0.43  Loss: 1.2869
                                                          43%|████▎     | 810/1868 [42:03:51<55:02:35, 187.29s/it] 43%|████▎     | 811/1868 [42:06:58<54:59:26, 187.29s/it]{'loss': 1.2869, 'grad_norm': 0.0, 'learning_rate': 1.1338329764453961e-05, 'epoch': 0.43}
? Step 811  Epoch: 0.43  Loss: 1.4211
                                                          43%|████▎     | 811/1868 [42:06:58<54:59:26, 187.29s/it] 43%|████▎     | 812/1868 [42:10:06<54:56:26, 187.30s/it]{'loss': 1.4211, 'grad_norm': 0.0, 'learning_rate': 1.132762312633833e-05, 'epoch': 0.43}
? Step 812  Epoch: 0.43  Loss: 1.4868
                                                          43%|████▎     | 812/1868 [42:10:06<54:56:26, 187.30s/it] 44%|████▎     | 813/1868 [42:13:13<54:52:04, 187.23s/it]{'loss': 1.4868, 'grad_norm': 0.0, 'learning_rate': 1.13169164882227e-05, 'epoch': 0.43}
? Step 813  Epoch: 0.44  Loss: 1.7423
                                                          44%|████▎     | 813/1868 [42:13:13<54:52:04, 187.23s/it] 44%|████▎     | 814/1868 [42:16:20<54:48:09, 187.18s/it]{'loss': 1.7423, 'grad_norm': 0.0, 'learning_rate': 1.1306209850107069e-05, 'epoch': 0.44}
? Step 814  Epoch: 0.44  Loss: 1.6438
                                                          44%|████▎     | 814/1868 [42:16:20<54:48:09, 187.18s/it] 44%|████▎     | 815/1868 [42:19:27<54:46:09, 187.25s/it]{'loss': 1.6438, 'grad_norm': 0.0, 'learning_rate': 1.1295503211991437e-05, 'epoch': 0.44}
? Step 815  Epoch: 0.44  Loss: 1.5741
                                                          44%|████▎     | 815/1868 [42:19:27<54:46:09, 187.25s/it] 44%|████▎     | 816/1868 [42:22:34<54:42:55, 187.24s/it]{'loss': 1.5741, 'grad_norm': 0.0, 'learning_rate': 1.1284796573875804e-05, 'epoch': 0.44}
? Step 816  Epoch: 0.44  Loss: 1.7878
                                                          44%|████▎     | 816/1868 [42:22:34<54:42:55, 187.24s/it] 44%|████▎     | 817/1868 [42:25:41<54:38:12, 187.15s/it]{'loss': 1.7878, 'grad_norm': 0.0, 'learning_rate': 1.1274089935760173e-05, 'epoch': 0.44}
? Step 817  Epoch: 0.44  Loss: 1.5500
                                                          44%|████▎     | 817/1868 [42:25:41<54:38:12, 187.15s/it] 44%|████▍     | 818/1868 [42:28:49<54:35:25, 187.17s/it]{'loss': 1.55, 'grad_norm': 0.0, 'learning_rate': 1.1263383297644542e-05, 'epoch': 0.44}
? Step 818  Epoch: 0.44  Loss: 1.5922
                                                          44%|████▍     | 818/1868 [42:28:49<54:35:25, 187.17s/it] 44%|████▍     | 819/1868 [42:31:56<54:31:46, 187.14s/it]{'loss': 1.5922, 'grad_norm': 0.0, 'learning_rate': 1.125267665952891e-05, 'epoch': 0.44}
? Step 819  Epoch: 0.44  Loss: 1.7222
                                                          44%|████▍     | 819/1868 [42:31:56<54:31:46, 187.14s/it] 44%|████▍     | 820/1868 [42:35:03<54:30:30, 187.24s/it]{'loss': 1.7222, 'grad_norm': 0.0, 'learning_rate': 1.1241970021413277e-05, 'epoch': 0.44}
? Step 820  Epoch: 0.44  Loss: 1.4695
                                                          44%|████▍     | 820/1868 [42:35:03<54:30:30, 187.24s/it] 44%|████▍     | 821/1868 [42:38:10<54:26:53, 187.21s/it]{'loss': 1.4695, 'grad_norm': 0.0, 'learning_rate': 1.1231263383297646e-05, 'epoch': 0.44}
? Step 821  Epoch: 0.44  Loss: 1.8177
                                                          44%|████▍     | 821/1868 [42:38:10<54:26:53, 187.21s/it] 44%|████▍     | 822/1868 [42:41:18<54:24:09, 187.24s/it]{'loss': 1.8177, 'grad_norm': 0.0, 'learning_rate': 1.1220556745182015e-05, 'epoch': 0.44}
? Step 822  Epoch: 0.44  Loss: 1.8042
                                                          44%|████▍     | 822/1868 [42:41:18<54:24:09, 187.24s/it] 44%|████▍     | 823/1868 [42:44:25<54:20:43, 187.22s/it]{'loss': 1.8042, 'grad_norm': 0.0, 'learning_rate': 1.1209850107066382e-05, 'epoch': 0.44}
? Step 823  Epoch: 0.44  Loss: 1.5947
                                                          44%|████▍     | 823/1868 [42:44:25<54:20:43, 187.22s/it] 44%|████▍     | 824/1868 [42:47:32<54:16:55, 187.18s/it]{'loss': 1.5947, 'grad_norm': 0.0, 'learning_rate': 1.119914346895075e-05, 'epoch': 0.44}
? Step 824  Epoch: 0.44  Loss: 1.7252
                                                          44%|████▍     | 824/1868 [42:47:32<54:16:55, 187.18s/it] 44%|████▍     | 825/1868 [42:50:39<54:15:05, 187.25s/it]{'loss': 1.7252, 'grad_norm': 0.0, 'learning_rate': 1.1188436830835119e-05, 'epoch': 0.44}
? Step 825  Epoch: 0.44  Loss: 1.4169
                                                          44%|████▍     | 825/1868 [42:50:39<54:15:05, 187.25s/it] 44%|████▍     | 826/1868 [42:53:47<54:12:33, 187.29s/it]{'loss': 1.4169, 'grad_norm': 0.0, 'learning_rate': 1.1177730192719488e-05, 'epoch': 0.44}
? Step 826  Epoch: 0.44  Loss: 1.6727
                                                          44%|████▍     | 826/1868 [42:53:47<54:12:33, 187.29s/it] 44%|████▍     | 827/1868 [42:56:54<54:07:59, 187.20s/it]{'loss': 1.6727, 'grad_norm': 0.0, 'learning_rate': 1.1167023554603855e-05, 'epoch': 0.44}
? Step 827  Epoch: 0.44  Loss: 1.5896
                                                          44%|████▍     | 827/1868 [42:56:54<54:07:59, 187.20s/it] 44%|████▍     | 828/1868 [43:00:01<54:03:50, 187.15s/it]{'loss': 1.5896, 'grad_norm': 0.0, 'learning_rate': 1.1156316916488223e-05, 'epoch': 0.44}
? Step 828  Epoch: 0.44  Loss: 1.4571
                                                          44%|████▍     | 828/1868 [43:00:01<54:03:50, 187.15s/it] 44%|████▍     | 829/1868 [43:03:08<54:00:50, 187.15s/it]{'loss': 1.4571, 'grad_norm': 0.0, 'learning_rate': 1.1145610278372592e-05, 'epoch': 0.44}
? Step 829  Epoch: 0.44  Loss: 1.9422
                                                          44%|████▍     | 829/1868 [43:03:08<54:00:50, 187.15s/it] 44%|████▍     | 830/1868 [43:06:15<53:56:24, 187.08s/it]{'loss': 1.9422, 'grad_norm': 0.0, 'learning_rate': 1.113490364025696e-05, 'epoch': 0.44}
? Step 830  Epoch: 0.44  Loss: 1.8355
                                                          44%|████▍     | 830/1868 [43:06:15<53:56:24, 187.08s/it] 44%|████▍     | 831/1868 [43:09:22<53:54:52, 187.17s/it]{'loss': 1.8355, 'grad_norm': 0.0, 'learning_rate': 1.1124197002141328e-05, 'epoch': 0.44}
? Step 831  Epoch: 0.44  Loss: 1.6315
                                                          44%|████▍     | 831/1868 [43:09:22<53:54:52, 187.17s/it] 45%|████▍     | 832/1868 [43:12:29<53:52:01, 187.18s/it]{'loss': 1.6315, 'grad_norm': 0.0, 'learning_rate': 1.1113490364025696e-05, 'epoch': 0.44}
? Step 832  Epoch: 0.45  Loss: 1.4616
                                                          45%|████▍     | 832/1868 [43:12:29<53:52:01, 187.18s/it] 45%|████▍     | 833/1868 [43:15:37<53:49:01, 187.19s/it]{'loss': 1.4616, 'grad_norm': 0.0, 'learning_rate': 1.1102783725910065e-05, 'epoch': 0.45}
? Step 833  Epoch: 0.45  Loss: 1.7993
                                                          45%|████▍     | 833/1868 [43:15:37<53:49:01, 187.19s/it] 45%|████▍     | 834/1868 [43:18:44<53:45:33, 187.17s/it]{'loss': 1.7993, 'grad_norm': 0.0, 'learning_rate': 1.1092077087794434e-05, 'epoch': 0.45}
? Step 834  Epoch: 0.45  Loss: 1.6591
                                                          45%|████▍     | 834/1868 [43:18:44<53:45:33, 187.17s/it] 45%|████▍     | 835/1868 [43:21:51<53:43:45, 187.25s/it]{'loss': 1.6591, 'grad_norm': 0.0, 'learning_rate': 1.10813704496788e-05, 'epoch': 0.45}
? Step 835  Epoch: 0.45  Loss: 1.5917
                                                          45%|████▍     | 835/1868 [43:21:51<53:43:45, 187.25s/it] 45%|████▍     | 836/1868 [43:24:58<53:39:50, 187.20s/it]{'loss': 1.5917, 'grad_norm': 0.0, 'learning_rate': 1.107066381156317e-05, 'epoch': 0.45}
? Step 836  Epoch: 0.45  Loss: 1.7454
                                                          45%|████▍     | 836/1868 [43:24:58<53:39:50, 187.20s/it] 45%|████▍     | 837/1868 [43:28:05<53:36:13, 187.17s/it]{'loss': 1.7454, 'grad_norm': 0.0, 'learning_rate': 1.1059957173447538e-05, 'epoch': 0.45}
? Step 837  Epoch: 0.45  Loss: 1.9620
                                                          45%|████▍     | 837/1868 [43:28:05<53:36:13, 187.17s/it] 45%|████▍     | 838/1868 [43:31:13<53:33:39, 187.20s/it]{'loss': 1.962, 'grad_norm': 0.0, 'learning_rate': 1.1049250535331907e-05, 'epoch': 0.45}
? Step 838  Epoch: 0.45  Loss: 1.5713
                                                          45%|████▍     | 838/1868 [43:31:13<53:33:39, 187.20s/it] 45%|████▍     | 839/1868 [43:34:20<53:30:03, 187.18s/it]{'loss': 1.5713, 'grad_norm': 0.0, 'learning_rate': 1.1038543897216274e-05, 'epoch': 0.45}
? Step 839  Epoch: 0.45  Loss: 1.7067
                                                          45%|████▍     | 839/1868 [43:34:20<53:30:03, 187.18s/it] 45%|████▍     | 840/1868 [43:37:27<53:25:39, 187.10s/it]{'loss': 1.7067, 'grad_norm': 0.0, 'learning_rate': 1.1027837259100644e-05, 'epoch': 0.45}
? Step 840  Epoch: 0.45  Loss: 1.5829
                                                          45%|████▍     | 840/1868 [43:37:27<53:25:39, 187.10s/it] 45%|████▌     | 841/1868 [43:40:34<53:23:35, 187.16s/it]{'loss': 1.5829, 'grad_norm': 0.0, 'learning_rate': 1.1017130620985013e-05, 'epoch': 0.45}
? Step 841  Epoch: 0.45  Loss: 1.6506
                                                          45%|████▌     | 841/1868 [43:40:34<53:23:35, 187.16s/it] 45%|████▌     | 842/1868 [43:43:41<53:19:50, 187.13s/it]{'loss': 1.6506, 'grad_norm': 0.0, 'learning_rate': 1.1006423982869381e-05, 'epoch': 0.45}
? Step 842  Epoch: 0.45  Loss: 1.5593
                                                          45%|████▌     | 842/1868 [43:43:41<53:19:50, 187.13s/it] 45%|████▌     | 843/1868 [43:46:48<53:17:51, 187.19s/it]{'loss': 1.5593, 'grad_norm': 0.0, 'learning_rate': 1.099571734475375e-05, 'epoch': 0.45}
? Step 843  Epoch: 0.45  Loss: 1.6673
                                                          45%|████▌     | 843/1868 [43:46:48<53:17:51, 187.19s/it] 45%|████▌     | 844/1868 [43:49:55<53:13:31, 187.12s/it]{'loss': 1.6673, 'grad_norm': 0.0, 'learning_rate': 1.0985010706638117e-05, 'epoch': 0.45}
? Step 844  Epoch: 0.45  Loss: 1.6416
                                                          45%|████▌     | 844/1868 [43:49:55<53:13:31, 187.12s/it] 45%|████▌     | 845/1868 [43:53:03<53:11:34, 187.19s/it]{'loss': 1.6416, 'grad_norm': 0.0, 'learning_rate': 1.0974304068522486e-05, 'epoch': 0.45}
? Step 845  Epoch: 0.45  Loss: 1.6445
                                                          45%|████▌     | 845/1868 [43:53:03<53:11:34, 187.19s/it] 45%|████▌     | 846/1868 [43:56:09<53:06:57, 187.10s/it]{'loss': 1.6445, 'grad_norm': 0.0, 'learning_rate': 1.0963597430406854e-05, 'epoch': 0.45}
? Step 846  Epoch: 0.45  Loss: 1.5818
                                                          45%|████▌     | 846/1868 [43:56:09<53:06:57, 187.10s/it] 45%|████▌     | 847/1868 [43:59:16<53:03:07, 187.06s/it]{'loss': 1.5818, 'grad_norm': 0.0, 'learning_rate': 1.0952890792291221e-05, 'epoch': 0.45}
? Step 847  Epoch: 0.45  Loss: 1.6852
                                                          45%|████▌     | 847/1868 [43:59:16<53:03:07, 187.06s/it] 45%|████▌     | 848/1868 [44:02:24<53:01:34, 187.15s/it]{'loss': 1.6852, 'grad_norm': 0.0, 'learning_rate': 1.094218415417559e-05, 'epoch': 0.45}
? Step 848  Epoch: 0.45  Loss: 1.4655
                                                          45%|████▌     | 848/1868 [44:02:24<53:01:34, 187.15s/it] 45%|████▌     | 849/1868 [44:05:31<52:56:58, 187.06s/it]{'loss': 1.4655, 'grad_norm': 0.0, 'learning_rate': 1.0931477516059959e-05, 'epoch': 0.45}
? Step 849  Epoch: 0.45  Loss: 1.6492
                                                          45%|████▌     | 849/1868 [44:05:31<52:56:58, 187.06s/it] 46%|████▌     | 850/1868 [44:08:38<52:52:55, 187.01s/it]{'loss': 1.6492, 'grad_norm': 0.0, 'learning_rate': 1.0920770877944327e-05, 'epoch': 0.45}
? Step 850  Epoch: 0.46  Loss: 1.9131
                                                          46%|████▌     | 850/1868 [44:08:38<52:52:55, 187.01s/it] 46%|████▌     | 851/1868 [44:11:45<52:50:16, 187.04s/it]{'loss': 1.9131, 'grad_norm': 0.0, 'learning_rate': 1.0910064239828694e-05, 'epoch': 0.46}
? Step 851  Epoch: 0.46  Loss: 1.4540
                                                          46%|████▌     | 851/1868 [44:11:45<52:50:16, 187.04s/it] 46%|████▌     | 852/1868 [44:14:52<52:47:38, 187.07s/it]{'loss': 1.454, 'grad_norm': 0.0, 'learning_rate': 1.0899357601713063e-05, 'epoch': 0.46}
? Step 852  Epoch: 0.46  Loss: 1.4781
                                                          46%|████▌     | 852/1868 [44:14:52<52:47:38, 187.07s/it] 46%|████▌     | 853/1868 [44:17:59<52:46:02, 187.16s/it]{'loss': 1.4781, 'grad_norm': 0.0, 'learning_rate': 1.0888650963597432e-05, 'epoch': 0.46}
? Step 853  Epoch: 0.46  Loss: 1.5518
                                                          46%|████▌     | 853/1868 [44:17:59<52:46:02, 187.16s/it] 46%|████▌     | 854/1868 [44:21:06<52:41:04, 187.05s/it]{'loss': 1.5518, 'grad_norm': 0.0, 'learning_rate': 1.08779443254818e-05, 'epoch': 0.46}
? Step 854  Epoch: 0.46  Loss: 1.6605
                                                          46%|████▌     | 854/1868 [44:21:06<52:41:04, 187.05s/it] 46%|████▌     | 855/1868 [44:24:13<52:36:51, 186.98s/it]{'loss': 1.6605, 'grad_norm': 0.0, 'learning_rate': 1.0867237687366167e-05, 'epoch': 0.46}
? Step 855  Epoch: 0.46  Loss: 1.5285
                                                          46%|████▌     | 855/1868 [44:24:13<52:36:51, 186.98s/it] 46%|████▌     | 856/1868 [44:27:20<52:33:48, 186.98s/it]{'loss': 1.5285, 'grad_norm': 0.0, 'learning_rate': 1.0856531049250536e-05, 'epoch': 0.46}
? Step 856  Epoch: 0.46  Loss: 1.6586
                                                          46%|████▌     | 856/1868 [44:27:20<52:33:48, 186.98s/it] 46%|████▌     | 857/1868 [44:30:27<52:33:36, 187.16s/it]{'loss': 1.6586, 'grad_norm': 0.0, 'learning_rate': 1.0845824411134905e-05, 'epoch': 0.46}
? Step 857  Epoch: 0.46  Loss: 1.5421
                                                          46%|████▌     | 857/1868 [44:30:27<52:33:36, 187.16s/it] 46%|████▌     | 858/1868 [44:33:34<52:30:22, 187.15s/it]{'loss': 1.5421, 'grad_norm': 0.0, 'learning_rate': 1.0835117773019273e-05, 'epoch': 0.46}
? Step 858  Epoch: 0.46  Loss: 1.7562
                                                          46%|████▌     | 858/1868 [44:33:34<52:30:22, 187.15s/it] 46%|████▌     | 859/1868 [44:36:41<52:26:43, 187.12s/it]{'loss': 1.7562, 'grad_norm': 0.0, 'learning_rate': 1.082441113490364e-05, 'epoch': 0.46}
? Step 859  Epoch: 0.46  Loss: 2.0916
                                                          46%|████▌     | 859/1868 [44:36:41<52:26:43, 187.12s/it] 46%|████▌     | 860/1868 [44:39:49<52:24:14, 187.16s/it]{'loss': 2.0916, 'grad_norm': 0.0, 'learning_rate': 1.0813704496788009e-05, 'epoch': 0.46}
? Step 860  Epoch: 0.46  Loss: 1.5595
                                                          46%|████▌     | 860/1868 [44:39:49<52:24:14, 187.16s/it] 46%|████▌     | 861/1868 [44:42:56<52:19:11, 187.04s/it]{'loss': 1.5595, 'grad_norm': 0.0, 'learning_rate': 1.0802997858672378e-05, 'epoch': 0.46}
? Step 861  Epoch: 0.46  Loss: 1.6356
                                                          46%|████▌     | 861/1868 [44:42:56<52:19:11, 187.04s/it] 46%|████▌     | 862/1868 [44:46:03<52:16:28, 187.07s/it]{'loss': 1.6356, 'grad_norm': 0.0, 'learning_rate': 1.0792291220556746e-05, 'epoch': 0.46}
? Step 862  Epoch: 0.46  Loss: 1.5697
                                                          46%|████▌     | 862/1868 [44:46:03<52:16:28, 187.07s/it] 46%|████▌     | 863/1868 [44:49:09<52:12:00, 186.99s/it]{'loss': 1.5697, 'grad_norm': 0.0, 'learning_rate': 1.0781584582441113e-05, 'epoch': 0.46}
? Step 863  Epoch: 0.46  Loss: 1.6507
                                                          46%|████▌     | 863/1868 [44:49:09<52:12:00, 186.99s/it] 46%|████▋     | 864/1868 [44:52:17<52:10:03, 187.06s/it]{'loss': 1.6507, 'grad_norm': 0.0, 'learning_rate': 1.0770877944325482e-05, 'epoch': 0.46}
? Step 864  Epoch: 0.46  Loss: 1.6142
                                                          46%|████▋     | 864/1868 [44:52:17<52:10:03, 187.06s/it] 46%|████▋     | 865/1868 [44:55:23<52:05:52, 186.99s/it]{'loss': 1.6142, 'grad_norm': 0.0, 'learning_rate': 1.076017130620985e-05, 'epoch': 0.46}
? Step 865  Epoch: 0.46  Loss: 1.8088
                                                          46%|████▋     | 865/1868 [44:55:23<52:05:52, 186.99s/it] 46%|████▋     | 866/1868 [44:58:30<52:02:22, 186.97s/it]{'loss': 1.8088, 'grad_norm': 0.0, 'learning_rate': 1.0749464668094217e-05, 'epoch': 0.46}
? Step 866  Epoch: 0.46  Loss: 1.7795
                                                          46%|████▋     | 866/1868 [44:58:30<52:02:22, 186.97s/it] 46%|████▋     | 867/1868 [45:01:38<52:00:37, 187.05s/it]{'loss': 1.7795, 'grad_norm': 0.0, 'learning_rate': 1.0738758029978586e-05, 'epoch': 0.46}
? Step 867  Epoch: 0.46  Loss: 1.6409
                                                          46%|████▋     | 867/1868 [45:01:38<52:00:37, 187.05s/it] 46%|████▋     | 868/1868 [45:04:45<51:57:45, 187.07s/it]{'loss': 1.6409, 'grad_norm': 0.0, 'learning_rate': 1.0728051391862957e-05, 'epoch': 0.46}
? Step 868  Epoch: 0.46  Loss: 1.7834
                                                          46%|████▋     | 868/1868 [45:04:45<51:57:45, 187.07s/it] 47%|████▋     | 869/1868 [45:07:52<51:53:55, 187.02s/it]{'loss': 1.7834, 'grad_norm': 0.0, 'learning_rate': 1.0717344753747325e-05, 'epoch': 0.46}
? Step 869  Epoch: 0.47  Loss: 1.4630
                                                          47%|████▋     | 869/1868 [45:07:52<51:53:55, 187.02s/it] 47%|████▋     | 870/1868 [45:10:59<51:50:59, 187.03s/it]{'loss': 1.463, 'grad_norm': 0.0, 'learning_rate': 1.0706638115631694e-05, 'epoch': 0.47}
? Step 870  Epoch: 0.47  Loss: 1.6938
                                                          47%|████▋     | 870/1868 [45:10:59<51:50:59, 187.03s/it] 47%|████▋     | 871/1868 [45:14:05<51:45:45, 186.91s/it]{'loss': 1.6938, 'grad_norm': 0.0, 'learning_rate': 1.0695931477516061e-05, 'epoch': 0.47}
? Step 871  Epoch: 0.47  Loss: 1.9815
                                                          47%|████▋     | 871/1868 [45:14:05<51:45:45, 186.91s/it] 47%|████▋     | 872/1868 [45:17:13<51:44:11, 187.00s/it]{'loss': 1.9815, 'grad_norm': 0.0, 'learning_rate': 1.068522483940043e-05, 'epoch': 0.47}
? Step 872  Epoch: 0.47  Loss: 1.5574
                                                          47%|████▋     | 872/1868 [45:17:13<51:44:11, 187.00s/it] 47%|████▋     | 873/1868 [45:20:20<51:41:31, 187.03s/it]{'loss': 1.5574, 'grad_norm': 0.0, 'learning_rate': 1.0674518201284798e-05, 'epoch': 0.47}
? Step 873  Epoch: 0.47  Loss: 1.6716
                                                          47%|████▋     | 873/1868 [45:20:20<51:41:31, 187.03s/it] 47%|████▋     | 874/1868 [45:23:27<51:38:00, 187.00s/it]{'loss': 1.6716, 'grad_norm': 0.0, 'learning_rate': 1.0663811563169167e-05, 'epoch': 0.47}
? Step 874  Epoch: 0.47  Loss: 1.6374
                                                          47%|████▋     | 874/1868 [45:23:27<51:38:00, 187.00s/it] 47%|████▋     | 875/1868 [45:26:34<51:36:06, 187.08s/it]{'loss': 1.6374, 'grad_norm': 0.0, 'learning_rate': 1.0653104925053534e-05, 'epoch': 0.47}
? Step 875  Epoch: 0.47  Loss: 1.5007
                                                          47%|████▋     | 875/1868 [45:26:34<51:36:06, 187.08s/it] 47%|████▋     | 876/1868 [45:29:41<51:32:41, 187.06s/it]{'loss': 1.5007, 'grad_norm': 0.0, 'learning_rate': 1.0642398286937903e-05, 'epoch': 0.47}
? Step 876  Epoch: 0.47  Loss: 1.4806
                                                          47%|████▋     | 876/1868 [45:29:41<51:32:41, 187.06s/it] 47%|████▋     | 877/1868 [45:32:48<51:27:47, 186.95s/it]{'loss': 1.4806, 'grad_norm': 0.0, 'learning_rate': 1.0631691648822271e-05, 'epoch': 0.47}
? Step 877  Epoch: 0.47  Loss: 1.6403
                                                          47%|████▋     | 877/1868 [45:32:48<51:27:47, 186.95s/it] 47%|████▋     | 878/1868 [45:35:54<51:22:31, 186.82s/it]{'loss': 1.6403, 'grad_norm': 0.0, 'learning_rate': 1.062098501070664e-05, 'epoch': 0.47}
? Step 878  Epoch: 0.47  Loss: 2.0862
                                                          47%|████▋     | 878/1868 [45:35:54<51:22:31, 186.82s/it] 47%|████▋     | 879/1868 [45:39:01<51:20:55, 186.91s/it]{'loss': 2.0862, 'grad_norm': 0.0, 'learning_rate': 1.0610278372591007e-05, 'epoch': 0.47}
? Step 879  Epoch: 0.47  Loss: 1.4285
                                                          47%|████▋     | 879/1868 [45:39:01<51:20:55, 186.91s/it] 47%|████▋     | 880/1868 [45:42:08<51:18:29, 186.95s/it]{'loss': 1.4285, 'grad_norm': 0.0, 'learning_rate': 1.0599571734475376e-05, 'epoch': 0.47}
? Step 880  Epoch: 0.47  Loss: 1.6598
                                                          47%|████▋     | 880/1868 [45:42:08<51:18:29, 186.95s/it] 47%|████▋     | 881/1868 [45:45:15<51:15:07, 186.94s/it]{'loss': 1.6598, 'grad_norm': 0.0, 'learning_rate': 1.0588865096359744e-05, 'epoch': 0.47}
? Step 881  Epoch: 0.47  Loss: 1.7167
                                                          47%|████▋     | 881/1868 [45:45:15<51:15:07, 186.94s/it] 47%|████▋     | 882/1868 [45:48:22<51:12:00, 186.94s/it]{'loss': 1.7167, 'grad_norm': 0.0, 'learning_rate': 1.0578158458244113e-05, 'epoch': 0.47}
? Step 882  Epoch: 0.47  Loss: 1.4105
                                                          47%|████▋     | 882/1868 [45:48:22<51:12:00, 186.94s/it] 47%|████▋     | 883/1868 [45:51:29<51:08:12, 186.90s/it]{'loss': 1.4105, 'grad_norm': 0.0, 'learning_rate': 1.056745182012848e-05, 'epoch': 0.47}
? Step 883  Epoch: 0.47  Loss: 1.7556
                                                          47%|████▋     | 883/1868 [45:51:29<51:08:12, 186.90s/it] 47%|████▋     | 884/1868 [45:54:36<51:05:38, 186.93s/it]{'loss': 1.7556, 'grad_norm': 0.0, 'learning_rate': 1.0556745182012849e-05, 'epoch': 0.47}
? Step 884  Epoch: 0.47  Loss: 1.5693
                                                          47%|████▋     | 884/1868 [45:54:36<51:05:38, 186.93s/it] 47%|████▋     | 885/1868 [45:57:43<51:02:48, 186.95s/it]{'loss': 1.5693, 'grad_norm': 0.0, 'learning_rate': 1.0546038543897217e-05, 'epoch': 0.47}
? Step 885  Epoch: 0.47  Loss: 1.3625
                                                          47%|████▋     | 885/1868 [45:57:43<51:02:48, 186.95s/it] 47%|████▋     | 886/1868 [46:00:50<51:00:13, 186.98s/it]{'loss': 1.3625, 'grad_norm': 0.0, 'learning_rate': 1.0535331905781586e-05, 'epoch': 0.47}
? Step 886  Epoch: 0.47  Loss: 1.4641
                                                          47%|████▋     | 886/1868 [46:00:50<51:00:13, 186.98s/it] 47%|████▋     | 887/1868 [46:03:57<50:57:16, 186.99s/it]{'loss': 1.4641, 'grad_norm': 0.0, 'learning_rate': 1.0524625267665953e-05, 'epoch': 0.47}
? Step 887  Epoch: 0.47  Loss: 1.6461
                                                          47%|████▋     | 887/1868 [46:03:57<50:57:16, 186.99s/it] 48%|████▊     | 888/1868 [46:07:04<50:55:44, 187.09s/it]{'loss': 1.6461, 'grad_norm': 0.0, 'learning_rate': 1.0513918629550322e-05, 'epoch': 0.47}
? Step 888  Epoch: 0.48  Loss: 1.3911
                                                          48%|████▊     | 888/1868 [46:07:04<50:55:44, 187.09s/it] 48%|████▊     | 889/1868 [46:10:11<50:50:30, 186.96s/it]{'loss': 1.3911, 'grad_norm': 0.0, 'learning_rate': 1.050321199143469e-05, 'epoch': 0.48}
? Step 889  Epoch: 0.48  Loss: 1.5767
                                                          48%|████▊     | 889/1868 [46:10:11<50:50:30, 186.96s/it] 48%|████▊     | 890/1868 [46:13:18<50:46:31, 186.90s/it]{'loss': 1.5767, 'grad_norm': 0.0, 'learning_rate': 1.0492505353319057e-05, 'epoch': 0.48}
? Step 890  Epoch: 0.48  Loss: 1.6106
                                                          48%|████▊     | 890/1868 [46:13:18<50:46:31, 186.90s/it] 48%|████▊     | 891/1868 [46:16:25<50:44:06, 186.95s/it]{'loss': 1.6106, 'grad_norm': 0.0, 'learning_rate': 1.0481798715203426e-05, 'epoch': 0.48}
? Step 891  Epoch: 0.48  Loss: 1.5884
                                                          48%|████▊     | 891/1868 [46:16:25<50:44:06, 186.95s/it] 48%|████▊     | 892/1868 [46:19:32<50:43:27, 187.10s/it]{'loss': 1.5884, 'grad_norm': 0.0, 'learning_rate': 1.0471092077087794e-05, 'epoch': 0.48}
? Step 892  Epoch: 0.48  Loss: 1.4635
                                                          48%|████▊     | 892/1868 [46:19:32<50:43:27, 187.10s/it] 48%|████▊     | 893/1868 [46:22:39<50:40:15, 187.09s/it]{'loss': 1.4635, 'grad_norm': 0.0, 'learning_rate': 1.0460385438972163e-05, 'epoch': 0.48}
? Step 893  Epoch: 0.48  Loss: 1.5132
                                                          48%|████▊     | 893/1868 [46:22:39<50:40:15, 187.09s/it] 48%|████▊     | 894/1868 [46:25:46<50:34:04, 186.90s/it]{'loss': 1.5132, 'grad_norm': 0.0, 'learning_rate': 1.044967880085653e-05, 'epoch': 0.48}
? Step 894  Epoch: 0.48  Loss: 1.7199
                                                          48%|████▊     | 894/1868 [46:25:46<50:34:04, 186.90s/it] 48%|████▊     | 895/1868 [46:28:53<50:30:31, 186.88s/it]{'loss': 1.7199, 'grad_norm': 0.0, 'learning_rate': 1.04389721627409e-05, 'epoch': 0.48}
? Step 895  Epoch: 0.48  Loss: 1.6961
                                                          48%|████▊     | 895/1868 [46:28:53<50:30:31, 186.88s/it] 48%|████▊     | 896/1868 [46:31:59<50:26:05, 186.80s/it]{'loss': 1.6961, 'grad_norm': 0.0, 'learning_rate': 1.042826552462527e-05, 'epoch': 0.48}
? Step 896  Epoch: 0.48  Loss: 1.3737
                                                          48%|████▊     | 896/1868 [46:31:59<50:26:05, 186.80s/it] 48%|████▊     | 897/1868 [46:35:06<50:23:44, 186.84s/it]{'loss': 1.3737, 'grad_norm': 0.0, 'learning_rate': 1.0417558886509638e-05, 'epoch': 0.48}
? Step 897  Epoch: 0.48  Loss: 1.5914
                                                          48%|████▊     | 897/1868 [46:35:06<50:23:44, 186.84s/it] 48%|████▊     | 898/1868 [46:38:13<50:20:35, 186.84s/it]{'loss': 1.5914, 'grad_norm': 0.0, 'learning_rate': 1.0406852248394007e-05, 'epoch': 0.48}
? Step 898  Epoch: 0.48  Loss: 1.4891
                                                          48%|████▊     | 898/1868 [46:38:13<50:20:35, 186.84s/it] 48%|████▊     | 899/1868 [46:41:20<50:17:29, 186.84s/it]{'loss': 1.4891, 'grad_norm': 0.0, 'learning_rate': 1.0396145610278374e-05, 'epoch': 0.48}
? Step 899  Epoch: 0.48  Loss: 1.5778
                                                          48%|████▊     | 899/1868 [46:41:20<50:17:29, 186.84s/it] 48%|████▊     | 900/1868 [46:44:27<50:13:49, 186.81s/it]{'loss': 1.5778, 'grad_norm': 0.0, 'learning_rate': 1.0385438972162742e-05, 'epoch': 0.48}
? Step 900  Epoch: 0.48  Loss: 1.4593
                                                          48%|████▊     | 900/1868 [46:44:27<50:13:49, 186.81s/it]/home/dev25-01/mistral-env/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 48%|████▊     | 901/1868 [46:47:34<50:12:27, 186.92s/it]{'loss': 1.4593, 'grad_norm': 0.0, 'learning_rate': 1.0374732334047111e-05, 'epoch': 0.48}
? Step 901  Epoch: 0.48  Loss: 1.6077
                                                          48%|████▊     | 901/1868 [46:47:34<50:12:27, 186.92s/it] 48%|████▊     | 902/1868 [46:50:41<50:10:19, 186.98s/it]{'loss': 1.6077, 'grad_norm': 0.0, 'learning_rate': 1.036402569593148e-05, 'epoch': 0.48}
? Step 902  Epoch: 0.48  Loss: 1.4492
                                                          48%|████▊     | 902/1868 [46:50:41<50:10:19, 186.98s/it] 48%|████▊     | 903/1868 [46:53:48<50:07:20, 186.99s/it]{'loss': 1.4492, 'grad_norm': 0.0, 'learning_rate': 1.0353319057815846e-05, 'epoch': 0.48}
? Step 903  Epoch: 0.48  Loss: 1.7331
                                                          48%|████▊     | 903/1868 [46:53:48<50:07:20, 186.99s/it] 48%|████▊     | 904/1868 [46:56:55<50:06:42, 187.14s/it]{'loss': 1.7331, 'grad_norm': 0.0, 'learning_rate': 1.0342612419700215e-05, 'epoch': 0.48}
? Step 904  Epoch: 0.48  Loss: 1.6145
                                                          48%|████▊     | 904/1868 [46:56:55<50:06:42, 187.14s/it] 48%|████▊     | 905/1868 [47:00:02<50:02:58, 187.10s/it]{'loss': 1.6145, 'grad_norm': 0.0, 'learning_rate': 1.0331905781584584e-05, 'epoch': 0.48}
? Step 905  Epoch: 0.48  Loss: 1.4177
                                                          48%|████▊     | 905/1868 [47:00:02<50:02:58, 187.10s/it] 49%|████▊     | 906/1868 [47:03:09<49:58:57, 187.04s/it]{'loss': 1.4177, 'grad_norm': 0.0, 'learning_rate': 1.0321199143468953e-05, 'epoch': 0.48}
? Step 906  Epoch: 0.49  Loss: 1.5866
                                                          49%|████▊     | 906/1868 [47:03:09<49:58:57, 187.04s/it] 49%|████▊     | 907/1868 [47:06:16<49:55:39, 187.03s/it]{'loss': 1.5866, 'grad_norm': 0.0, 'learning_rate': 1.031049250535332e-05, 'epoch': 0.49}
? Step 907  Epoch: 0.49  Loss: 1.5771
                                                          49%|████▊     | 907/1868 [47:06:16<49:55:39, 187.03s/it] 49%|████▊     | 908/1868 [47:09:23<49:52:24, 187.03s/it]{'loss': 1.5771, 'grad_norm': 0.0, 'learning_rate': 1.0299785867237688e-05, 'epoch': 0.49}
? Step 908  Epoch: 0.49  Loss: 1.3450
                                                          49%|████▊     | 908/1868 [47:09:23<49:52:24, 187.03s/it] 49%|████▊     | 909/1868 [47:12:30<49:49:03, 187.01s/it]{'loss': 1.345, 'grad_norm': 0.0, 'learning_rate': 1.0289079229122057e-05, 'epoch': 0.49}
? Step 909  Epoch: 0.49  Loss: 1.4794
                                                          49%|████▊     | 909/1868 [47:12:30<49:49:03, 187.01s/it] 49%|████▊     | 910/1868 [47:15:37<49:46:01, 187.02s/it]{'loss': 1.4794, 'grad_norm': 0.0, 'learning_rate': 1.0278372591006426e-05, 'epoch': 0.49}
? Step 910  Epoch: 0.49  Loss: 1.6406
                                                          49%|████▊     | 910/1868 [47:15:37<49:46:01, 187.02s/it] 49%|████▉     | 911/1868 [47:18:44<49:42:14, 186.97s/it]{'loss': 1.6406, 'grad_norm': 0.0, 'learning_rate': 1.0267665952890792e-05, 'epoch': 0.49}
? Step 911  Epoch: 0.49  Loss: 1.3688
                                                          49%|████▉     | 911/1868 [47:18:44<49:42:14, 186.97s/it] 49%|████▉     | 912/1868 [47:21:51<49:38:44, 186.95s/it]{'loss': 1.3688, 'grad_norm': 0.0, 'learning_rate': 1.0256959314775161e-05, 'epoch': 0.49}
? Step 912  Epoch: 0.49  Loss: 1.6609
                                                          49%|████▉     | 912/1868 [47:21:51<49:38:44, 186.95s/it] 49%|████▉     | 913/1868 [47:24:58<49:33:52, 186.84s/it]{'loss': 1.6609, 'grad_norm': 0.0, 'learning_rate': 1.024625267665953e-05, 'epoch': 0.49}
? Step 913  Epoch: 0.49  Loss: 1.6633
                                                          49%|████▉     | 913/1868 [47:24:58<49:33:52, 186.84s/it] 49%|████▉     | 914/1868 [47:28:05<49:31:33, 186.89s/it]{'loss': 1.6633, 'grad_norm': 0.0, 'learning_rate': 1.0235546038543897e-05, 'epoch': 0.49}
? Step 914  Epoch: 0.49  Loss: 1.5071
                                                          49%|████▉     | 914/1868 [47:28:05<49:31:33, 186.89s/it] 49%|████▉     | 915/1868 [47:31:11<49:28:07, 186.87s/it]{'loss': 1.5071, 'grad_norm': 0.0, 'learning_rate': 1.0224839400428265e-05, 'epoch': 0.49}
? Step 915  Epoch: 0.49  Loss: 1.4674
                                                          49%|████▉     | 915/1868 [47:31:11<49:28:07, 186.87s/it] 49%|████▉     | 916/1868 [47:34:19<49:26:15, 186.95s/it]{'loss': 1.4674, 'grad_norm': 0.0, 'learning_rate': 1.0214132762312634e-05, 'epoch': 0.49}
? Step 916  Epoch: 0.49  Loss: 1.6469
                                                          49%|████▉     | 916/1868 [47:34:19<49:26:15, 186.95s/it] 49%|████▉     | 917/1868 [47:37:25<49:21:29, 186.84s/it]{'loss': 1.6469, 'grad_norm': 0.0, 'learning_rate': 1.0203426124197003e-05, 'epoch': 0.49}
? Step 917  Epoch: 0.49  Loss: 1.6997
                                                          49%|████▉     | 917/1868 [47:37:25<49:21:29, 186.84s/it] 49%|████▉     | 918/1868 [47:40:32<49:18:39, 186.86s/it]{'loss': 1.6997, 'grad_norm': 0.0, 'learning_rate': 1.019271948608137e-05, 'epoch': 0.49}
? Step 918  Epoch: 0.49  Loss: 1.5410
                                                          49%|████▉     | 918/1868 [47:40:32<49:18:39, 186.86s/it] 49%|████▉     | 919/1868 [47:43:39<49:15:50, 186.88s/it]{'loss': 1.541, 'grad_norm': 0.0, 'learning_rate': 1.0182012847965738e-05, 'epoch': 0.49}
? Step 919  Epoch: 0.49  Loss: 1.4857
                                                          49%|████▉     | 919/1868 [47:43:39<49:15:50, 186.88s/it] 49%|████▉     | 920/1868 [47:46:46<49:12:54, 186.89s/it]{'loss': 1.4857, 'grad_norm': 0.0, 'learning_rate': 1.0171306209850107e-05, 'epoch': 0.49}
? Step 920  Epoch: 0.49  Loss: 1.5329
                                                          49%|████▉     | 920/1868 [47:46:46<49:12:54, 186.89s/it] 49%|████▉     | 921/1868 [47:49:53<49:08:56, 186.84s/it]{'loss': 1.5329, 'grad_norm': 0.0, 'learning_rate': 1.0160599571734476e-05, 'epoch': 0.49}
? Step 921  Epoch: 0.49  Loss: 1.8132
                                                          49%|████▉     | 921/1868 [47:49:53<49:08:56, 186.84s/it] 49%|████▉     | 922/1868 [47:52:59<49:05:07, 186.79s/it]{'loss': 1.8132, 'grad_norm': 0.0, 'learning_rate': 1.0149892933618843e-05, 'epoch': 0.49}
? Step 922  Epoch: 0.49  Loss: 1.3857
                                                          49%|████▉     | 922/1868 [47:52:59<49:05:07, 186.79s/it] 49%|████▉     | 923/1868 [47:56:06<49:01:49, 186.78s/it]{'loss': 1.3857, 'grad_norm': 0.0, 'learning_rate': 1.0139186295503213e-05, 'epoch': 0.49}
? Step 923  Epoch: 0.49  Loss: 1.4131
                                                          49%|████▉     | 923/1868 [47:56:06<49:01:49, 186.78s/it] 49%|████▉     | 924/1868 [47:59:13<48:58:39, 186.78s/it]{'loss': 1.4131, 'grad_norm': 0.0, 'learning_rate': 1.0128479657387582e-05, 'epoch': 0.49}
? Step 924  Epoch: 0.49  Loss: 1.6714
                                                          49%|████▉     | 924/1868 [47:59:13<48:58:39, 186.78s/it] 50%|████▉     | 925/1868 [48:02:20<48:54:56, 186.74s/it]{'loss': 1.6714, 'grad_norm': 0.0, 'learning_rate': 1.011777301927195e-05, 'epoch': 0.49}
? Step 925  Epoch: 0.50  Loss: 1.5876
                                                          50%|████▉     | 925/1868 [48:02:20<48:54:56, 186.74s/it] 50%|████▉     | 926/1868 [48:05:27<48:53:37, 186.86s/it]{'loss': 1.5876, 'grad_norm': 0.0, 'learning_rate': 1.010706638115632e-05, 'epoch': 0.5}
? Step 926  Epoch: 0.50  Loss: 1.4354
                                                          50%|████▉     | 926/1868 [48:05:27<48:53:37, 186.86s/it] 50%|████▉     | 927/1868 [48:08:34<48:50:58, 186.88s/it]{'loss': 1.4354, 'grad_norm': 0.0, 'learning_rate': 1.0096359743040686e-05, 'epoch': 0.5}
? Step 927  Epoch: 0.50  Loss: 1.3149
                                                          50%|████▉     | 927/1868 [48:08:34<48:50:58, 186.88s/it] 50%|████▉     | 928/1868 [48:11:40<48:45:55, 186.76s/it]{'loss': 1.3149, 'grad_norm': 0.0, 'learning_rate': 1.0085653104925055e-05, 'epoch': 0.5}
? Step 928  Epoch: 0.50  Loss: 1.6517
                                                          50%|████▉     | 928/1868 [48:11:40<48:45:55, 186.76s/it] 50%|████▉     | 929/1868 [48:14:47<48:41:50, 186.70s/it]{'loss': 1.6517, 'grad_norm': 0.0, 'learning_rate': 1.0074946466809424e-05, 'epoch': 0.5}
? Step 929  Epoch: 0.50  Loss: 1.7736
                                                          50%|████▉     | 929/1868 [48:14:47<48:41:50, 186.70s/it] 50%|████▉     | 930/1868 [48:17:53<48:38:07, 186.66s/it]{'loss': 1.7736, 'grad_norm': 0.0, 'learning_rate': 1.0064239828693792e-05, 'epoch': 0.5}
? Step 930  Epoch: 0.50  Loss: 1.6910
                                                          50%|████▉     | 930/1868 [48:17:53<48:38:07, 186.66s/it] 50%|████▉     | 931/1868 [48:21:00<48:35:35, 186.70s/it]{'loss': 1.691, 'grad_norm': 0.0, 'learning_rate': 1.0053533190578159e-05, 'epoch': 0.5}
? Step 931  Epoch: 0.50  Loss: 1.7125
                                                          50%|████▉     | 931/1868 [48:21:00<48:35:35, 186.70s/it] 50%|████▉     | 932/1868 [48:24:07<48:32:40, 186.71s/it]{'loss': 1.7125, 'grad_norm': 0.0, 'learning_rate': 1.0042826552462528e-05, 'epoch': 0.5}
? Step 932  Epoch: 0.50  Loss: 1.6078
                                                          50%|████▉     | 932/1868 [48:24:07<48:32:40, 186.71s/it] 50%|████▉     | 933/1868 [48:27:13<48:29:00, 186.67s/it]{'loss': 1.6078, 'grad_norm': 0.0, 'learning_rate': 1.0032119914346896e-05, 'epoch': 0.5}
? Step 933  Epoch: 0.50  Loss: 1.5463
                                                          50%|████▉     | 933/1868 [48:27:13<48:29:00, 186.67s/it] 50%|█████     | 934/1868 [48:30:20<48:24:31, 186.59s/it]{'loss': 1.5463, 'grad_norm': 0.0, 'learning_rate': 1.0021413276231265e-05, 'epoch': 0.5}
? Step 934  Epoch: 0.50  Loss: 1.6491
                                                          50%|█████     | 934/1868 [48:30:20<48:24:31, 186.59s/it] 50%|█████     | 935/1868 [48:33:26<48:21:56, 186.62s/it]{'loss': 1.6491, 'grad_norm': 0.0, 'learning_rate': 1.0010706638115632e-05, 'epoch': 0.5}
? Step 935  Epoch: 0.50  Loss: 1.5218
                                                          50%|█████     | 935/1868 [48:33:26<48:21:56, 186.62s/it] 50%|█████     | 936/1868 [48:36:33<48:19:17, 186.65s/it]{'loss': 1.5218, 'grad_norm': 0.0, 'learning_rate': 1e-05, 'epoch': 0.5}
? Step 936  Epoch: 0.50  Loss: 1.4847
                                                          50%|█████     | 936/1868 [48:36:33<48:19:17, 186.65s/it] 50%|█████     | 937/1868 [48:39:40<48:15:23, 186.60s/it]{'loss': 1.4847, 'grad_norm': 0.0, 'learning_rate': 9.98929336188437e-06, 'epoch': 0.5}
? Step 937  Epoch: 0.50  Loss: 1.4998
                                                          50%|█████     | 937/1868 [48:39:40<48:15:23, 186.60s/it] 50%|█████     | 938/1868 [48:42:46<48:12:22, 186.60s/it]{'loss': 1.4998, 'grad_norm': 0.0, 'learning_rate': 9.978586723768736e-06, 'epoch': 0.5}
? Step 938  Epoch: 0.50  Loss: 1.6857
                                                          50%|█████     | 938/1868 [48:42:46<48:12:22, 186.60s/it] 50%|█████     | 939/1868 [48:45:53<48:12:13, 186.80s/it]{'loss': 1.6857, 'grad_norm': 0.0, 'learning_rate': 9.967880085653105e-06, 'epoch': 0.5}
? Step 939  Epoch: 0.50  Loss: 1.5733
                                                          50%|█████     | 939/1868 [48:45:53<48:12:13, 186.80s/it] 50%|█████     | 940/1868 [48:49:00<48:09:26, 186.82s/it]{'loss': 1.5733, 'grad_norm': 0.0, 'learning_rate': 9.957173447537474e-06, 'epoch': 0.5}
? Step 940  Epoch: 0.50  Loss: 1.6708
                                                          50%|█████     | 940/1868 [48:49:00<48:09:26, 186.82s/it] 50%|█████     | 941/1868 [48:52:07<48:06:11, 186.81s/it]{'loss': 1.6708, 'grad_norm': 0.0, 'learning_rate': 9.946466809421842e-06, 'epoch': 0.5}
? Step 941  Epoch: 0.50  Loss: 1.8011
                                                          50%|█████     | 941/1868 [48:52:07<48:06:11, 186.81s/it] 50%|█████     | 942/1868 [48:55:14<48:02:43, 186.79s/it]{'loss': 1.8011, 'grad_norm': 0.0, 'learning_rate': 9.93576017130621e-06, 'epoch': 0.5}
? Step 942  Epoch: 0.50  Loss: 1.4262
                                                          50%|█████     | 942/1868 [48:55:14<48:02:43, 186.79s/it] 50%|█████     | 943/1868 [48:58:21<47:59:31, 186.78s/it]{'loss': 1.4262, 'grad_norm': 0.0, 'learning_rate': 9.92505353319058e-06, 'epoch': 0.5}
? Step 943  Epoch: 0.50  Loss: 1.5495
                                                          50%|█████     | 943/1868 [48:58:21<47:59:31, 186.78s/it] 51%|█████     | 944/1868 [49:01:27<47:56:09, 186.76s/it]{'loss': 1.5495, 'grad_norm': 0.0, 'learning_rate': 9.914346895074949e-06, 'epoch': 0.5}
? Step 944  Epoch: 0.51  Loss: 1.7227
                                                          51%|█████     | 944/1868 [49:01:27<47:56:09, 186.76s/it] 51%|█████     | 945/1868 [49:04:34<47:52:39, 186.74s/it]{'loss': 1.7227, 'grad_norm': 0.0, 'learning_rate': 9.903640256959315e-06, 'epoch': 0.51}
? Step 945  Epoch: 0.51  Loss: 1.4665
                                                          51%|█████     | 945/1868 [49:04:34<47:52:39, 186.74s/it] 51%|█████     | 946/1868 [49:07:40<47:47:55, 186.63s/it]{'loss': 1.4665, 'grad_norm': 0.0, 'learning_rate': 9.892933618843684e-06, 'epoch': 0.51}
? Step 946  Epoch: 0.51  Loss: 1.4234
                                                          51%|█████     | 946/1868 [49:07:40<47:47:55, 186.63s/it] 51%|█████     | 947/1868 [49:10:47<47:43:22, 186.54s/it]{'loss': 1.4234, 'grad_norm': 0.0, 'learning_rate': 9.882226980728053e-06, 'epoch': 0.51}
? Step 947  Epoch: 0.51  Loss: 1.7861
                                                          51%|█████     | 947/1868 [49:10:47<47:43:22, 186.54s/it] 51%|█████     | 948/1868 [49:13:53<47:39:59, 186.52s/it]{'loss': 1.7861, 'grad_norm': 0.0, 'learning_rate': 9.871520342612421e-06, 'epoch': 0.51}
? Step 948  Epoch: 0.51  Loss: 1.5338
                                                          51%|█████     | 948/1868 [49:13:53<47:39:59, 186.52s/it] 51%|█████     | 949/1868 [49:17:00<47:36:33, 186.50s/it]{'loss': 1.5338, 'grad_norm': 0.0, 'learning_rate': 9.860813704496788e-06, 'epoch': 0.51}
? Step 949  Epoch: 0.51  Loss: 1.5664
                                                          51%|█████     | 949/1868 [49:17:00<47:36:33, 186.50s/it] 51%|█████     | 950/1868 [49:20:06<47:33:48, 186.52s/it]{'loss': 1.5664, 'grad_norm': 0.0, 'learning_rate': 9.850107066381157e-06, 'epoch': 0.51}
? Step 950  Epoch: 0.51  Loss: 1.6941
                                                          51%|█████     | 950/1868 [49:20:06<47:33:48, 186.52s/it] 51%|█████     | 951/1868 [49:23:13<47:30:24, 186.50s/it]{'loss': 1.6941, 'grad_norm': 0.0, 'learning_rate': 9.839400428265526e-06, 'epoch': 0.51}
? Step 951  Epoch: 0.51  Loss: 1.6582
                                                          51%|█████     | 951/1868 [49:23:13<47:30:24, 186.50s/it] 51%|█████     | 952/1868 [49:26:19<47:27:51, 186.54s/it]{'loss': 1.6582, 'grad_norm': 0.0, 'learning_rate': 9.828693790149893e-06, 'epoch': 0.51}
? Step 952  Epoch: 0.51  Loss: 1.3154
                                                          51%|█████     | 952/1868 [49:26:19<47:27:51, 186.54s/it] 51%|█████     | 953/1868 [49:29:26<47:25:54, 186.62s/it]{'loss': 1.3154, 'grad_norm': 0.0, 'learning_rate': 9.817987152034261e-06, 'epoch': 0.51}
? Step 953  Epoch: 0.51  Loss: 1.7649
                                                          51%|█████     | 953/1868 [49:29:26<47:25:54, 186.62s/it] 51%|█████     | 954/1868 [49:32:33<47:22:13, 186.58s/it]{'loss': 1.7649, 'grad_norm': 0.0, 'learning_rate': 9.80728051391863e-06, 'epoch': 0.51}
? Step 954  Epoch: 0.51  Loss: 1.8972
                                                          51%|█████     | 954/1868 [49:32:33<47:22:13, 186.58s/it] 51%|█████     | 955/1868 [49:35:39<47:19:06, 186.58s/it]{'loss': 1.8972, 'grad_norm': 0.0, 'learning_rate': 9.796573875802999e-06, 'epoch': 0.51}
? Step 955  Epoch: 0.51  Loss: 1.7344
                                                          51%|█████     | 955/1868 [49:35:39<47:19:06, 186.58s/it] 51%|█████     | 956/1868 [49:38:46<47:16:58, 186.64s/it]{'loss': 1.7344, 'grad_norm': 0.0, 'learning_rate': 9.785867237687366e-06, 'epoch': 0.51}
? Step 956  Epoch: 0.51  Loss: 1.3447
                                                          51%|█████     | 956/1868 [49:38:46<47:16:58, 186.64s/it] 51%|█████     | 957/1868 [49:41:52<47:13:27, 186.62s/it]{'loss': 1.3447, 'grad_norm': 0.0, 'learning_rate': 9.775160599571736e-06, 'epoch': 0.51}
? Step 957  Epoch: 0.51  Loss: 1.7061
                                                          51%|█████     | 957/1868 [49:41:53<47:13:27, 186.62s/it] 51%|█████▏    | 958/1868 [49:44:59<47:10:44, 186.64s/it]{'loss': 1.7061, 'grad_norm': 0.0, 'learning_rate': 9.764453961456105e-06, 'epoch': 0.51}
? Step 958  Epoch: 0.51  Loss: 1.5827
                                                          51%|█████▏    | 958/1868 [49:44:59<47:10:44, 186.64s/it] 51%|█████▏    | 959/1868 [49:48:06<47:07:51, 186.66s/it]{'loss': 1.5827, 'grad_norm': 0.0, 'learning_rate': 9.753747323340472e-06, 'epoch': 0.51}
? Step 959  Epoch: 0.51  Loss: 1.6146
                                                          51%|█████▏    | 959/1868 [49:48:06<47:07:51, 186.66s/it] 51%|█████▏    | 960/1868 [49:51:12<47:03:22, 186.57s/it]{'loss': 1.6146, 'grad_norm': 0.0, 'learning_rate': 9.74304068522484e-06, 'epoch': 0.51}
? Step 960  Epoch: 0.51  Loss: 1.7785
                                                          51%|█████▏    | 960/1868 [49:51:12<47:03:22, 186.57s/it] 51%|█████▏    | 961/1868 [49:54:19<47:00:25, 186.58s/it]{'loss': 1.7785, 'grad_norm': 0.0, 'learning_rate': 9.732334047109209e-06, 'epoch': 0.51}
? Step 961  Epoch: 0.51  Loss: 1.3938
                                                          51%|█████▏    | 961/1868 [49:54:19<47:00:25, 186.58s/it] 51%|█████▏    | 962/1868 [49:57:25<46:56:48, 186.54s/it]{'loss': 1.3938, 'grad_norm': 0.0, 'learning_rate': 9.721627408993576e-06, 'epoch': 0.51}
? Step 962  Epoch: 0.51  Loss: 1.4184
                                                          51%|█████▏    | 962/1868 [49:57:25<46:56:48, 186.54s/it] 52%|█████▏    | 963/1868 [50:00:32<46:54:32, 186.60s/it]{'loss': 1.4184, 'grad_norm': 0.0, 'learning_rate': 9.710920770877945e-06, 'epoch': 0.51}
? Step 963  Epoch: 0.52  Loss: 1.6415
                                                          52%|█████▏    | 963/1868 [50:00:32<46:54:32, 186.60s/it] 52%|█████▏    | 964/1868 [50:03:39<46:51:58, 186.64s/it]{'loss': 1.6415, 'grad_norm': 0.0, 'learning_rate': 9.700214132762313e-06, 'epoch': 0.52}
? Step 964  Epoch: 0.52  Loss: 1.5521
                                                          52%|█████▏    | 964/1868 [50:03:39<46:51:58, 186.64s/it] 52%|█████▏    | 965/1868 [50:06:45<46:48:57, 186.64s/it]{'loss': 1.5521, 'grad_norm': 0.0, 'learning_rate': 9.689507494646682e-06, 'epoch': 0.52}
? Step 965  Epoch: 0.52  Loss: 1.5326
                                                          52%|█████▏    | 965/1868 [50:06:45<46:48:57, 186.64s/it] 52%|█████▏    | 966/1868 [50:09:52<46:46:40, 186.70s/it]{'loss': 1.5326, 'grad_norm': 0.0, 'learning_rate': 9.678800856531049e-06, 'epoch': 0.52}
? Step 966  Epoch: 0.52  Loss: 1.4883
                                                          52%|█████▏    | 966/1868 [50:09:52<46:46:40, 186.70s/it] 52%|█████▏    | 967/1868 [50:12:59<46:43:07, 186.67s/it]{'loss': 1.4883, 'grad_norm': 0.0, 'learning_rate': 9.668094218415418e-06, 'epoch': 0.52}
? Step 967  Epoch: 0.52  Loss: 1.4464
                                                          52%|█████▏    | 967/1868 [50:12:59<46:43:07, 186.67s/it] 52%|█████▏    | 968/1868 [50:16:06<46:40:25, 186.70s/it]{'loss': 1.4464, 'grad_norm': 0.0, 'learning_rate': 9.657387580299786e-06, 'epoch': 0.52}
? Step 968  Epoch: 0.52  Loss: 1.5970
                                                          52%|█████▏    | 968/1868 [50:16:06<46:40:25, 186.70s/it] 52%|█████▏    | 969/1868 [50:19:12<46:36:34, 186.65s/it]{'loss': 1.597, 'grad_norm': 0.0, 'learning_rate': 9.646680942184155e-06, 'epoch': 0.52}
? Step 969  Epoch: 0.52  Loss: 1.5495
                                                          52%|█████▏    | 969/1868 [50:19:12<46:36:34, 186.65s/it] 52%|█████▏    | 970/1868 [50:22:19<46:34:17, 186.70s/it]{'loss': 1.5495, 'grad_norm': 0.0, 'learning_rate': 9.635974304068522e-06, 'epoch': 0.52}
? Step 970  Epoch: 0.52  Loss: 1.6289
                                                          52%|█████▏    | 970/1868 [50:22:19<46:34:17, 186.70s/it] 52%|█████▏    | 971/1868 [50:25:26<46:30:56, 186.68s/it]{'loss': 1.6289, 'grad_norm': 0.0, 'learning_rate': 9.625267665952892e-06, 'epoch': 0.52}
? Step 971  Epoch: 0.52  Loss: 1.7346
                                                          52%|█████▏    | 971/1868 [50:25:26<46:30:56, 186.68s/it] 52%|█████▏    | 972/1868 [50:28:32<46:27:59, 186.70s/it]{'loss': 1.7346, 'grad_norm': 0.0, 'learning_rate': 9.614561027837261e-06, 'epoch': 0.52}
? Step 972  Epoch: 0.52  Loss: 1.8141
                                                          52%|█████▏    | 972/1868 [50:28:32<46:27:59, 186.70s/it] 52%|█████▏    | 973/1868 [50:31:39<46:25:16, 186.72s/it]{'loss': 1.8141, 'grad_norm': 0.0, 'learning_rate': 9.603854389721628e-06, 'epoch': 0.52}
? Step 973  Epoch: 0.52  Loss: 1.3398
                                                          52%|█████▏    | 973/1868 [50:31:39<46:25:16, 186.72s/it] 52%|█████▏    | 974/1868 [50:34:46<46:22:04, 186.72s/it]{'loss': 1.3398, 'grad_norm': 0.0, 'learning_rate': 9.593147751605997e-06, 'epoch': 0.52}
? Step 974  Epoch: 0.52  Loss: 1.6966
                                                          52%|█████▏    | 974/1868 [50:34:46<46:22:04, 186.72s/it] 52%|█████▏    | 975/1868 [50:37:53<46:19:16, 186.74s/it]{'loss': 1.6966, 'grad_norm': 0.0, 'learning_rate': 9.582441113490365e-06, 'epoch': 0.52}
? Step 975  Epoch: 0.52  Loss: 1.5339
                                                          52%|█████▏    | 975/1868 [50:37:53<46:19:16, 186.74s/it] 52%|█████▏    | 976/1868 [50:41:00<46:17:32, 186.83s/it]{'loss': 1.5339, 'grad_norm': 0.0, 'learning_rate': 9.571734475374732e-06, 'epoch': 0.52}
? Step 976  Epoch: 0.52  Loss: 1.4384
                                                          52%|█████▏    | 976/1868 [50:41:00<46:17:32, 186.83s/it] 52%|█████▏    | 977/1868 [50:44:06<46:12:39, 186.71s/it]{'loss': 1.4384, 'grad_norm': 0.0, 'learning_rate': 9.561027837259101e-06, 'epoch': 0.52}
? Step 977  Epoch: 0.52  Loss: 1.6505
                                                          52%|█████▏    | 977/1868 [50:44:06<46:12:39, 186.71s/it] 52%|█████▏    | 978/1868 [50:47:13<46:09:25, 186.70s/it]{'loss': 1.6505, 'grad_norm': 0.0, 'learning_rate': 9.55032119914347e-06, 'epoch': 0.52}
? Step 978  Epoch: 0.52  Loss: 1.4518
                                                          52%|█████▏    | 978/1868 [50:47:13<46:09:25, 186.70s/it] 52%|█████▏    | 979/1868 [50:50:20<46:06:48, 186.74s/it]{'loss': 1.4518, 'grad_norm': 0.0, 'learning_rate': 9.539614561027838e-06, 'epoch': 0.52}
? Step 979  Epoch: 0.52  Loss: 1.5744
                                                          52%|█████▏    | 979/1868 [50:50:20<46:06:48, 186.74s/it] 52%|█████▏    | 980/1868 [50:53:26<46:03:55, 186.75s/it]{'loss': 1.5744, 'grad_norm': 0.0, 'learning_rate': 9.528907922912205e-06, 'epoch': 0.52}
? Step 980  Epoch: 0.52  Loss: 1.4906
                                                          52%|█████▏    | 980/1868 [50:53:26<46:03:55, 186.75s/it] 53%|█████▎    | 981/1868 [50:56:33<46:01:24, 186.79s/it]{'loss': 1.4906, 'grad_norm': 0.0, 'learning_rate': 9.518201284796574e-06, 'epoch': 0.52}
? Step 981  Epoch: 0.53  Loss: 1.5554
                                                          53%|█████▎    | 981/1868 [50:56:33<46:01:24, 186.79s/it] 53%|█████▎    | 982/1868 [50:59:40<45:58:05, 186.78s/it]{'loss': 1.5554, 'grad_norm': 0.0, 'learning_rate': 9.507494646680943e-06, 'epoch': 0.53}
? Step 982  Epoch: 0.53  Loss: 1.9383
                                                          53%|█████▎    | 982/1868 [50:59:40<45:58:05, 186.78s/it] 53%|█████▎    | 983/1868 [51:02:47<45:54:41, 186.76s/it]{'loss': 1.9383, 'grad_norm': 0.0, 'learning_rate': 9.496788008565311e-06, 'epoch': 0.53}
? Step 983  Epoch: 0.53  Loss: 1.5015
                                                          53%|█████▎    | 983/1868 [51:02:47<45:54:41, 186.76s/it] 53%|█████▎    | 984/1868 [51:05:54<45:52:02, 186.79s/it]{'loss': 1.5015, 'grad_norm': 0.0, 'learning_rate': 9.486081370449678e-06, 'epoch': 0.53}
? Step 984  Epoch: 0.53  Loss: 1.3598
                                                          53%|█████▎    | 984/1868 [51:05:54<45:52:02, 186.79s/it] 53%|█████▎    | 985/1868 [51:09:00<45:48:22, 186.75s/it]{'loss': 1.3598, 'grad_norm': 0.0, 'learning_rate': 9.475374732334049e-06, 'epoch': 0.53}
? Step 985  Epoch: 0.53  Loss: 1.6407
                                                          53%|█████▎    | 985/1868 [51:09:00<45:48:22, 186.75s/it] 53%|█████▎    | 986/1868 [51:12:07<45:46:29, 186.84s/it]{'loss': 1.6407, 'grad_norm': 0.0, 'learning_rate': 9.464668094218416e-06, 'epoch': 0.53}
? Step 986  Epoch: 0.53  Loss: 1.6001
                                                          53%|█████▎    | 986/1868 [51:12:07<45:46:29, 186.84s/it] 53%|█████▎    | 987/1868 [51:15:14<45:44:07, 186.89s/it]{'loss': 1.6001, 'grad_norm': 0.0, 'learning_rate': 9.453961456102784e-06, 'epoch': 0.53}
? Step 987  Epoch: 0.53  Loss: 1.6839
                                                          53%|█████▎    | 987/1868 [51:15:14<45:44:07, 186.89s/it] 53%|█████▎    | 988/1868 [51:18:21<45:41:09, 186.90s/it]{'loss': 1.6839, 'grad_norm': 0.0, 'learning_rate': 9.443254817987153e-06, 'epoch': 0.53}
? Step 988  Epoch: 0.53  Loss: 1.4258
                                                          53%|█████▎    | 988/1868 [51:18:21<45:41:09, 186.90s/it] 53%|█████▎    | 989/1868 [51:21:28<45:37:04, 186.83s/it]{'loss': 1.4258, 'grad_norm': 0.0, 'learning_rate': 9.432548179871522e-06, 'epoch': 0.53}
? Step 989  Epoch: 0.53  Loss: 1.6790
                                                          53%|█████▎    | 989/1868 [51:21:28<45:37:04, 186.83s/it] 53%|█████▎    | 990/1868 [51:24:35<45:33:53, 186.83s/it]{'loss': 1.679, 'grad_norm': 0.0, 'learning_rate': 9.421841541755889e-06, 'epoch': 0.53}
? Step 990  Epoch: 0.53  Loss: 1.4592
                                                          53%|█████▎    | 990/1868 [51:24:35<45:33:53, 186.83s/it] 53%|█████▎    | 991/1868 [51:27:42<45:31:41, 186.89s/it]{'loss': 1.4592, 'grad_norm': 0.0, 'learning_rate': 9.411134903640257e-06, 'epoch': 0.53}
? Step 991  Epoch: 0.53  Loss: 1.7231
                                                          53%|█████▎    | 991/1868 [51:27:42<45:31:41, 186.89s/it] 53%|█████▎    | 992/1868 [51:30:49<45:28:59, 186.92s/it]{'loss': 1.7231, 'grad_norm': 0.0, 'learning_rate': 9.400428265524626e-06, 'epoch': 0.53}
? Step 992  Epoch: 0.53  Loss: 1.5499
                                                          53%|█████▎    | 992/1868 [51:30:49<45:28:59, 186.92s/it] 53%|█████▎    | 993/1868 [51:33:56<45:25:44, 186.91s/it]{'loss': 1.5499, 'grad_norm': 0.0, 'learning_rate': 9.389721627408995e-06, 'epoch': 0.53}
? Step 993  Epoch: 0.53  Loss: 1.6379
                                                          53%|█████▎    | 993/1868 [51:33:56<45:25:44, 186.91s/it] 53%|█████▎    | 994/1868 [51:37:02<45:21:19, 186.82s/it]{'loss': 1.6379, 'grad_norm': 0.0, 'learning_rate': 9.379014989293362e-06, 'epoch': 0.53}
? Step 994  Epoch: 0.53  Loss: 1.6180
                                                          53%|█████▎    | 994/1868 [51:37:02<45:21:19, 186.82s/it] 53%|█████▎    | 995/1868 [51:40:09<45:16:44, 186.72s/it]{'loss': 1.618, 'grad_norm': 0.0, 'learning_rate': 9.36830835117773e-06, 'epoch': 0.53}
? Step 995  Epoch: 0.53  Loss: 1.7994
                                                          53%|█████▎    | 995/1868 [51:40:09<45:16:44, 186.72s/it] 53%|█████▎    | 996/1868 [51:43:15<45:13:13, 186.69s/it]{'loss': 1.7994, 'grad_norm': 0.0, 'learning_rate': 9.357601713062099e-06, 'epoch': 0.53}
? Step 996  Epoch: 0.53  Loss: 1.7764
                                                          53%|█████▎    | 996/1868 [51:43:15<45:13:13, 186.69s/it] 53%|█████▎    | 997/1868 [51:46:22<45:11:26, 186.78s/it]{'loss': 1.7764, 'grad_norm': 0.0, 'learning_rate': 9.346895074946468e-06, 'epoch': 0.53}
? Step 997  Epoch: 0.53  Loss: 1.4213
                                                          53%|█████▎    | 997/1868 [51:46:22<45:11:26, 186.78s/it] 53%|█████▎    | 998/1868 [51:49:29<45:09:12, 186.84s/it]{'loss': 1.4213, 'grad_norm': 0.0, 'learning_rate': 9.336188436830836e-06, 'epoch': 0.53}
? Step 998  Epoch: 0.53  Loss: 1.5646
                                                          53%|█████▎    | 998/1868 [51:49:29<45:09:12, 186.84s/it] 53%|█████▎    | 999/1868 [51:52:36<45:06:15, 186.85s/it]{'loss': 1.5646, 'grad_norm': 0.0, 'learning_rate': 9.325481798715205e-06, 'epoch': 0.53}
? Step 999  Epoch: 0.53  Loss: 1.8132
                                                          53%|█████▎    | 999/1868 [51:52:36<45:06:15, 186.85s/it] 54%|█████▎    | 1000/1868 [51:55:43<45:03:17, 186.86s/it]{'loss': 1.8132, 'grad_norm': 0.0, 'learning_rate': 9.314775160599572e-06, 'epoch': 0.53}
? Step 1000  Epoch: 0.54  Loss: 1.5420
                                                           54%|█████▎    | 1000/1868 [51:55:43<45:03:17, 186.86s/it]/home/dev25-01/mistral-env/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 54%|█████▎    | 1001/1868 [51:58:50<45:00:35, 186.89s/it]{'loss': 1.542, 'grad_norm': 0.0, 'learning_rate': 9.30406852248394e-06, 'epoch': 0.54}
? Step 1001  Epoch: 0.54  Loss: 2.0765
                                                           54%|█████▎    | 1001/1868 [51:58:50<45:00:35, 186.89s/it] 54%|█████▎    | 1002/1868 [52:01:57<44:56:31, 186.83s/it]{'loss': 2.0765, 'grad_norm': 0.0, 'learning_rate': 9.29336188436831e-06, 'epoch': 0.54}
? Step 1002  Epoch: 0.54  Loss: 1.7239
                                                           54%|█████▎    | 1002/1868 [52:01:57<44:56:31, 186.83s/it] 54%|█████▎    | 1003/1868 [52:05:04<44:53:35, 186.84s/it]{'loss': 1.7239, 'grad_norm': 0.0, 'learning_rate': 9.282655246252678e-06, 'epoch': 0.54}
? Step 1003  Epoch: 0.54  Loss: 1.7047
                                                           54%|█████▎    | 1003/1868 [52:05:04<44:53:35, 186.84s/it] 54%|█████▎    | 1004/1868 [52:08:11<44:52:07, 186.95s/it]{'loss': 1.7047, 'grad_norm': 0.0, 'learning_rate': 9.271948608137045e-06, 'epoch': 0.54}
? Step 1004  Epoch: 0.54  Loss: 1.5864
                                                           54%|█████▎    | 1004/1868 [52:08:11<44:52:07, 186.95s/it] 54%|█████▍    | 1005/1868 [52:11:18<44:48:32, 186.92s/it]{'loss': 1.5864, 'grad_norm': 0.0, 'learning_rate': 9.261241970021414e-06, 'epoch': 0.54}
? Step 1005  Epoch: 0.54  Loss: 1.8977
                                                           54%|█████▍    | 1005/1868 [52:11:18<44:48:32, 186.92s/it] 54%|█████▍    | 1006/1868 [52:14:24<44:44:36, 186.86s/it]{'loss': 1.8977, 'grad_norm': 0.0, 'learning_rate': 9.250535331905782e-06, 'epoch': 0.54}
? Step 1006  Epoch: 0.54  Loss: 1.5456
                                                           54%|█████▍    | 1006/1868 [52:14:24<44:44:36, 186.86s/it] 54%|█████▍    | 1007/1868 [52:17:31<44:41:10, 186.84s/it]{'loss': 1.5456, 'grad_norm': 0.0, 'learning_rate': 9.239828693790151e-06, 'epoch': 0.54}
? Step 1007  Epoch: 0.54  Loss: 1.7739
                                                           54%|█████▍    | 1007/1868 [52:17:31<44:41:10, 186.84s/it] 54%|█████▍    | 1008/1868 [52:20:38<44:38:41, 186.89s/it]{'loss': 1.7739, 'grad_norm': 0.0, 'learning_rate': 9.229122055674518e-06, 'epoch': 0.54}
? Step 1008  Epoch: 0.54  Loss: 1.7427
                                                           54%|█████▍    | 1008/1868 [52:20:38<44:38:41, 186.89s/it] 54%|█████▍    | 1009/1868 [52:23:45<44:36:04, 186.92s/it]{'loss': 1.7427, 'grad_norm': 0.0, 'learning_rate': 9.218415417558887e-06, 'epoch': 0.54}
? Step 1009  Epoch: 0.54  Loss: 1.6236
                                                           54%|█████▍    | 1009/1868 [52:23:45<44:36:04, 186.92s/it] 54%|█████▍    | 1010/1868 [52:26:53<44:34:51, 187.05s/it]{'loss': 1.6236, 'grad_norm': 0.0, 'learning_rate': 9.207708779443255e-06, 'epoch': 0.54}
? Step 1010  Epoch: 0.54  Loss: 1.5372
                                                           54%|█████▍    | 1010/1868 [52:26:53<44:34:51, 187.05s/it] 54%|█████▍    | 1011/1868 [52:29:59<44:30:18, 186.95s/it]{'loss': 1.5372, 'grad_norm': 0.0, 'learning_rate': 9.197002141327624e-06, 'epoch': 0.54}
? Step 1011  Epoch: 0.54  Loss: 1.3978
                                                           54%|█████▍    | 1011/1868 [52:29:59<44:30:18, 186.95s/it] 54%|█████▍    | 1012/1868 [52:33:06<44:26:56, 186.93s/it]{'loss': 1.3978, 'grad_norm': 0.0, 'learning_rate': 9.186295503211993e-06, 'epoch': 0.54}
? Step 1012  Epoch: 0.54  Loss: 1.7619
                                                           54%|█████▍    | 1012/1868 [52:33:06<44:26:56, 186.93s/it] 54%|█████▍    | 1013/1868 [52:36:13<44:23:12, 186.89s/it]{'loss': 1.7619, 'grad_norm': 0.0, 'learning_rate': 9.175588865096361e-06, 'epoch': 0.54}
? Step 1013  Epoch: 0.54  Loss: 1.4183
                                                           54%|█████▍    | 1013/1868 [52:36:13<44:23:12, 186.89s/it] 54%|█████▍    | 1014/1868 [52:39:20<44:20:56, 186.95s/it]{'loss': 1.4183, 'grad_norm': 0.0, 'learning_rate': 9.164882226980728e-06, 'epoch': 0.54}
? Step 1014  Epoch: 0.54  Loss: 1.6149
                                                           54%|█████▍    | 1014/1868 [52:39:20<44:20:56, 186.95s/it] 54%|█████▍    | 1015/1868 [52:42:27<44:17:23, 186.92s/it]{'loss': 1.6149, 'grad_norm': 0.0, 'learning_rate': 9.154175588865097e-06, 'epoch': 0.54}
? Step 1015  Epoch: 0.54  Loss: 1.8369
                                                           54%|█████▍    | 1015/1868 [52:42:27<44:17:23, 186.92s/it] 54%|█████▍    | 1016/1868 [52:45:34<44:14:14, 186.92s/it]{'loss': 1.8369, 'grad_norm': 0.0, 'learning_rate': 9.143468950749466e-06, 'epoch': 0.54}
? Step 1016  Epoch: 0.54  Loss: 2.0436
                                                           54%|█████▍    | 1016/1868 [52:45:34<44:14:14, 186.92s/it] 54%|█████▍    | 1017/1868 [52:48:41<44:13:07, 187.06s/it]{'loss': 2.0436, 'grad_norm': 0.0, 'learning_rate': 9.132762312633834e-06, 'epoch': 0.54}
? Step 1017  Epoch: 0.54  Loss: 1.4665
                                                           54%|█████▍    | 1017/1868 [52:48:41<44:13:07, 187.06s/it] 54%|█████▍    | 1018/1868 [52:51:48<44:09:57, 187.06s/it]{'loss': 1.4665, 'grad_norm': 0.0, 'learning_rate': 9.122055674518201e-06, 'epoch': 0.54}
? Step 1018  Epoch: 0.54  Loss: 1.4078
                                                           54%|█████▍    | 1018/1868 [52:51:48<44:09:57, 187.06s/it] 55%|█████▍    | 1019/1868 [52:54:55<44:07:18, 187.09s/it]{'loss': 1.4078, 'grad_norm': 0.0, 'learning_rate': 9.11134903640257e-06, 'epoch': 0.54}
? Step 1019  Epoch: 0.55  Loss: 1.5758
                                                           55%|█████▍    | 1019/1868 [52:54:55<44:07:18, 187.09s/it] 55%|█████▍    | 1020/1868 [52:58:02<44:03:18, 187.03s/it]{'loss': 1.5758, 'grad_norm': 0.0, 'learning_rate': 9.100642398286939e-06, 'epoch': 0.55}
? Step 1020  Epoch: 0.55  Loss: 1.5545
                                                           55%|█████▍    | 1020/1868 [52:58:02<44:03:18, 187.03s/it] 55%|█████▍    | 1021/1868 [53:01:09<43:59:18, 186.96s/it]{'loss': 1.5545, 'grad_norm': 0.0, 'learning_rate': 9.089935760171307e-06, 'epoch': 0.55}
? Step 1021  Epoch: 0.55  Loss: 2.2052
                                                           55%|█████▍    | 1021/1868 [53:01:09<43:59:18, 186.96s/it] 55%|█████▍    | 1022/1868 [53:04:16<43:54:59, 186.88s/it]{'loss': 2.2052, 'grad_norm': 0.0, 'learning_rate': 9.079229122055674e-06, 'epoch': 0.55}
? Step 1022  Epoch: 0.55  Loss: 1.4087
                                                           55%|█████▍    | 1022/1868 [53:04:16<43:54:59, 186.88s/it] 55%|█████▍    | 1023/1868 [53:07:23<43:52:09, 186.90s/it]{'loss': 1.4087, 'grad_norm': 0.0, 'learning_rate': 9.068522483940043e-06, 'epoch': 0.55}
? Step 1023  Epoch: 0.55  Loss: 1.7154
                                                           55%|█████▍    | 1023/1868 [53:07:23<43:52:09, 186.90s/it] 55%|█████▍    | 1024/1868 [53:10:30<43:49:11, 186.91s/it]{'loss': 1.7154, 'grad_norm': 0.0, 'learning_rate': 9.057815845824412e-06, 'epoch': 0.55}
? Step 1024  Epoch: 0.55  Loss: 1.9607
                                                           55%|█████▍    | 1024/1868 [53:10:30<43:49:11, 186.91s/it] 55%|█████▍    | 1025/1868 [53:13:37<43:46:35, 186.95s/it]{'loss': 1.9607, 'grad_norm': 0.0, 'learning_rate': 9.04710920770878e-06, 'epoch': 0.55}
? Step 1025  Epoch: 0.55  Loss: 1.4892
                                                           55%|█████▍    | 1025/1868 [53:13:37<43:46:35, 186.95s/it] 55%|█████▍    | 1026/1868 [53:16:44<43:44:44, 187.04s/it]{'loss': 1.4892, 'grad_norm': 0.0, 'learning_rate': 9.036402569593149e-06, 'epoch': 0.55}
? Step 1026  Epoch: 0.55  Loss: 1.6000
                                                           55%|█████▍    | 1026/1868 [53:16:44<43:44:44, 187.04s/it] 55%|█████▍    | 1027/1868 [53:19:51<43:42:24, 187.09s/it]{'loss': 1.6, 'grad_norm': 0.0, 'learning_rate': 9.025695931477518e-06, 'epoch': 0.55}
? Step 1027  Epoch: 0.55  Loss: 1.4280
                                                           55%|█████▍    | 1027/1868 [53:19:51<43:42:24, 187.09s/it] 55%|█████▌    | 1028/1868 [53:22:58<43:38:35, 187.04s/it]{'loss': 1.428, 'grad_norm': 0.0, 'learning_rate': 9.014989293361885e-06, 'epoch': 0.55}
? Step 1028  Epoch: 0.55  Loss: 1.7059
                                                           55%|█████▌    | 1028/1868 [53:22:58<43:38:35, 187.04s/it] 55%|█████▌    | 1029/1868 [53:26:05<43:35:38, 187.05s/it]{'loss': 1.7059, 'grad_norm': 0.0, 'learning_rate': 9.004282655246253e-06, 'epoch': 0.55}
? Step 1029  Epoch: 0.55  Loss: 1.6420
                                                           55%|█████▌    | 1029/1868 [53:26:05<43:35:38, 187.05s/it] 55%|█████▌    | 1030/1868 [53:29:12<43:31:55, 187.01s/it]{'loss': 1.642, 'grad_norm': 0.0, 'learning_rate': 8.993576017130622e-06, 'epoch': 0.55}
? Step 1030  Epoch: 0.55  Loss: 1.5119
                                                           55%|█████▌    | 1030/1868 [53:29:12<43:31:55, 187.01s/it] 55%|█████▌    | 1031/1868 [53:32:19<43:29:30, 187.06s/it]{'loss': 1.5119, 'grad_norm': 0.0, 'learning_rate': 8.98286937901499e-06, 'epoch': 0.55}
? Step 1031  Epoch: 0.55  Loss: 1.7653
                                                           55%|█████▌    | 1031/1868 [53:32:19<43:29:30, 187.06s/it] 55%|█████▌    | 1032/1868 [53:35:26<43:25:44, 187.01s/it]{'loss': 1.7653, 'grad_norm': 0.0, 'learning_rate': 8.972162740899358e-06, 'epoch': 0.55}
? Step 1032  Epoch: 0.55  Loss: 1.6568
                                                           55%|█████▌    | 1032/1868 [53:35:26<43:25:44, 187.01s/it] 55%|█████▌    | 1033/1868 [53:38:33<43:22:47, 187.03s/it]{'loss': 1.6568, 'grad_norm': 0.0, 'learning_rate': 8.961456102783726e-06, 'epoch': 0.55}
? Step 1033  Epoch: 0.55  Loss: 1.4331
                                                           55%|█████▌    | 1033/1868 [53:38:33<43:22:47, 187.03s/it] 55%|█████▌    | 1034/1868 [53:41:40<43:20:15, 187.07s/it]{'loss': 1.4331, 'grad_norm': 0.0, 'learning_rate': 8.950749464668095e-06, 'epoch': 0.55}
? Step 1034  Epoch: 0.55  Loss: 1.6258
                                                           55%|█████▌    | 1034/1868 [53:41:40<43:20:15, 187.07s/it] 55%|█████▌    | 1035/1868 [53:44:48<43:17:39, 187.11s/it]{'loss': 1.6258, 'grad_norm': 0.0, 'learning_rate': 8.940042826552464e-06, 'epoch': 0.55}
? Step 1035  Epoch: 0.55  Loss: 1.4886
                                                           55%|█████▌    | 1035/1868 [53:44:48<43:17:39, 187.11s/it] 55%|█████▌    | 1036/1868 [53:47:55<43:14:36, 187.11s/it]{'loss': 1.4886, 'grad_norm': 0.0, 'learning_rate': 8.92933618843683e-06, 'epoch': 0.55}
? Step 1036  Epoch: 0.55  Loss: 1.5125
                                                           55%|█████▌    | 1036/1868 [53:47:55<43:14:36, 187.11s/it] 56%|█████▌    | 1037/1868 [53:51:02<43:10:44, 187.06s/it]{'loss': 1.5125, 'grad_norm': 0.0, 'learning_rate': 8.9186295503212e-06, 'epoch': 0.55}
? Step 1037  Epoch: 0.56  Loss: 1.4788
                                                           56%|█████▌    | 1037/1868 [53:51:02<43:10:44, 187.06s/it] 56%|█████▌    | 1038/1868 [53:54:08<43:06:19, 186.96s/it]{'loss': 1.4788, 'grad_norm': 0.0, 'learning_rate': 8.907922912205568e-06, 'epoch': 0.56}
? Step 1038  Epoch: 0.56  Loss: 1.6591
                                                           56%|█████▌    | 1038/1868 [53:54:08<43:06:19, 186.96s/it] 56%|█████▌    | 1039/1868 [53:57:15<43:03:23, 186.98s/it]{'loss': 1.6591, 'grad_norm': 0.0, 'learning_rate': 8.897216274089937e-06, 'epoch': 0.56}
? Step 1039  Epoch: 0.56  Loss: 1.8788
                                                           56%|█████▌    | 1039/1868 [53:57:15<43:03:23, 186.98s/it] 56%|█████▌    | 1040/1868 [54:00:23<43:01:58, 187.10s/it]{'loss': 1.8788, 'grad_norm': 0.0, 'learning_rate': 8.886509635974305e-06, 'epoch': 0.56}
? Step 1040  Epoch: 0.56  Loss: 1.6807
                                                           56%|█████▌    | 1040/1868 [54:00:23<43:01:58, 187.10s/it] 56%|█████▌    | 1041/1868 [54:03:30<42:57:39, 187.01s/it]{'loss': 1.6807, 'grad_norm': 0.0, 'learning_rate': 8.875802997858674e-06, 'epoch': 0.56}
? Step 1041  Epoch: 0.56  Loss: 1.5620
                                                           56%|█████▌    | 1041/1868 [54:03:30<42:57:39, 187.01s/it] 56%|█████▌    | 1042/1868 [54:06:37<42:54:52, 187.04s/it]{'loss': 1.562, 'grad_norm': 0.0, 'learning_rate': 8.865096359743041e-06, 'epoch': 0.56}
? Step 1042  Epoch: 0.56  Loss: 1.4317
                                                           56%|█████▌    | 1042/1868 [54:06:37<42:54:52, 187.04s/it] 56%|█████▌    | 1043/1868 [54:09:44<42:53:18, 187.15s/it]{'loss': 1.4317, 'grad_norm': 0.0, 'learning_rate': 8.85438972162741e-06, 'epoch': 0.56}
? Step 1043  Epoch: 0.56  Loss: 1.6157
                                                           56%|█████▌    | 1043/1868 [54:09:44<42:53:18, 187.15s/it] 56%|█████▌    | 1044/1868 [54:12:52<42:51:59, 187.28s/it]{'loss': 1.6157, 'grad_norm': 0.0, 'learning_rate': 8.843683083511778e-06, 'epoch': 0.56}
? Step 1044  Epoch: 0.56  Loss: 1.6123
                                                           56%|█████▌    | 1044/1868 [54:12:52<42:51:59, 187.28s/it] 56%|█████▌    | 1045/1868 [54:15:59<42:48:19, 187.24s/it]{'loss': 1.6123, 'grad_norm': 0.0, 'learning_rate': 8.832976445396147e-06, 'epoch': 0.56}
? Step 1045  Epoch: 0.56  Loss: 1.6277
                                                           56%|█████▌    | 1045/1868 [54:15:59<42:48:19, 187.24s/it] 56%|█████▌    | 1046/1868 [54:19:06<42:45:08, 187.24s/it]{'loss': 1.6277, 'grad_norm': 0.0, 'learning_rate': 8.822269807280514e-06, 'epoch': 0.56}
? Step 1046  Epoch: 0.56  Loss: 1.4136
                                                           56%|█████▌    | 1046/1868 [54:19:06<42:45:08, 187.24s/it] 56%|█████▌    | 1047/1868 [54:22:13<42:40:54, 187.15s/it]{'loss': 1.4136, 'grad_norm': 0.0, 'learning_rate': 8.811563169164883e-06, 'epoch': 0.56}
? Step 1047  Epoch: 0.56  Loss: 1.6818
                                                           56%|█████▌    | 1047/1868 [54:22:13<42:40:54, 187.15s/it] 56%|█████▌    | 1048/1868 [54:25:20<42:37:34, 187.14s/it]{'loss': 1.6818, 'grad_norm': 0.0, 'learning_rate': 8.800856531049251e-06, 'epoch': 0.56}
? Step 1048  Epoch: 0.56  Loss: 1.4944
                                                           56%|█████▌    | 1048/1868 [54:25:20<42:37:34, 187.14s/it] 56%|█████▌    | 1049/1868 [54:28:27<42:34:02, 187.11s/it]{'loss': 1.4944, 'grad_norm': 0.0, 'learning_rate': 8.79014989293362e-06, 'epoch': 0.56}
? Step 1049  Epoch: 0.56  Loss: 1.5330
                                                           56%|█████▌    | 1049/1868 [54:28:27<42:34:02, 187.11s/it] 56%|█████▌    | 1050/1868 [54:31:34<42:31:38, 187.16s/it]{'loss': 1.533, 'grad_norm': 0.0, 'learning_rate': 8.779443254817987e-06, 'epoch': 0.56}
? Step 1050  Epoch: 0.56  Loss: 1.6960
                                                           56%|█████▌    | 1050/1868 [54:31:34<42:31:38, 187.16s/it] 56%|█████▋    | 1051/1868 [54:34:42<42:29:28, 187.23s/it]{'loss': 1.696, 'grad_norm': 0.0, 'learning_rate': 8.768736616702356e-06, 'epoch': 0.56}
? Step 1051  Epoch: 0.56  Loss: 1.4873
                                                           56%|█████▋    | 1051/1868 [54:34:42<42:29:28, 187.23s/it] 56%|█████▋    | 1052/1868 [54:37:49<42:25:09, 187.14s/it]{'loss': 1.4873, 'grad_norm': 0.0, 'learning_rate': 8.758029978586724e-06, 'epoch': 0.56}
? Step 1052  Epoch: 0.56  Loss: 1.8972
                                                           56%|█████▋    | 1052/1868 [54:37:49<42:25:09, 187.14s/it] 56%|█████▋    | 1053/1868 [54:40:56<42:22:05, 187.15s/it]{'loss': 1.8972, 'grad_norm': 0.0, 'learning_rate': 8.747323340471093e-06, 'epoch': 0.56}
? Step 1053  Epoch: 0.56  Loss: 1.5439
                                                           56%|█████▋    | 1053/1868 [54:40:56<42:22:05, 187.15s/it] 56%|█████▋    | 1054/1868 [54:44:03<42:18:32, 187.12s/it]{'loss': 1.5439, 'grad_norm': 0.0, 'learning_rate': 8.736616702355462e-06, 'epoch': 0.56}
? Step 1054  Epoch: 0.56  Loss: 1.6108
                                                           56%|█████▋    | 1054/1868 [54:44:03<42:18:32, 187.12s/it] 56%|█████▋    | 1055/1868 [54:47:10<42:16:07, 187.17s/it]{'loss': 1.6108, 'grad_norm': 0.0, 'learning_rate': 8.72591006423983e-06, 'epoch': 0.56}
? Step 1055  Epoch: 0.56  Loss: 1.6383
                                                           56%|█████▋    | 1055/1868 [54:47:10<42:16:07, 187.17s/it] 57%|█████▋    | 1056/1868 [54:50:18<42:13:45, 187.22s/it]{'loss': 1.6383, 'grad_norm': 0.0, 'learning_rate': 8.715203426124197e-06, 'epoch': 0.56}
? Step 1056  Epoch: 0.57  Loss: 1.5645
                                                           57%|█████▋    | 1056/1868 [54:50:18<42:13:45, 187.22s/it] 57%|█████▋    | 1057/1868 [54:53:25<42:10:31, 187.22s/it]{'loss': 1.5645, 'grad_norm': 0.0, 'learning_rate': 8.704496788008566e-06, 'epoch': 0.57}
? Step 1057  Epoch: 0.57  Loss: 1.4701
                                                           57%|█████▋    | 1057/1868 [54:53:25<42:10:31, 187.22s/it] 57%|█████▋    | 1058/1868 [54:56:32<42:08:00, 187.26s/it]{'loss': 1.4701, 'grad_norm': 0.0, 'learning_rate': 8.693790149892935e-06, 'epoch': 0.57}
? Step 1058  Epoch: 0.57  Loss: 1.5008
                                                           57%|█████▋    | 1058/1868 [54:56:32<42:08:00, 187.26s/it] 57%|█████▋    | 1059/1868 [54:59:39<42:04:22, 187.22s/it]{'loss': 1.5008, 'grad_norm': 0.0, 'learning_rate': 8.683083511777303e-06, 'epoch': 0.57}
? Step 1059  Epoch: 0.57  Loss: 1.4526
                                                           57%|█████▋    | 1059/1868 [54:59:39<42:04:22, 187.22s/it] 57%|█████▋    | 1060/1868 [55:02:46<42:00:36, 187.17s/it]{'loss': 1.4526, 'grad_norm': 0.0, 'learning_rate': 8.67237687366167e-06, 'epoch': 0.57}
? Step 1060  Epoch: 0.57  Loss: 1.4347
                                                           57%|█████▋    | 1060/1868 [55:02:46<42:00:36, 187.17s/it] 57%|█████▋    | 1061/1868 [55:05:54<41:58:15, 187.23s/it]{'loss': 1.4347, 'grad_norm': 0.0, 'learning_rate': 8.661670235546039e-06, 'epoch': 0.57}
? Step 1061  Epoch: 0.57  Loss: 1.4381
                                                           57%|█████▋    | 1061/1868 [55:05:54<41:58:15, 187.23s/it] 57%|█████▋    | 1062/1868 [55:09:01<41:54:19, 187.17s/it]{'loss': 1.4381, 'grad_norm': 0.0, 'learning_rate': 8.650963597430408e-06, 'epoch': 0.57}
? Step 1062  Epoch: 0.57  Loss: 1.5784
                                                           57%|█████▋    | 1062/1868 [55:09:01<41:54:19, 187.17s/it] 57%|█████▋    | 1063/1868 [55:12:08<41:51:40, 187.21s/it]{'loss': 1.5784, 'grad_norm': 0.0, 'learning_rate': 8.640256959314776e-06, 'epoch': 0.57}
? Step 1063  Epoch: 0.57  Loss: 1.6555
                                                           57%|█████▋    | 1063/1868 [55:12:08<41:51:40, 187.21s/it] 57%|█████▋    | 1064/1868 [55:15:15<41:48:53, 187.23s/it]{'loss': 1.6555, 'grad_norm': 0.0, 'learning_rate': 8.629550321199143e-06, 'epoch': 0.57}
? Step 1064  Epoch: 0.57  Loss: 1.3669
                                                           57%|█████▋    | 1064/1868 [55:15:15<41:48:53, 187.23s/it] 57%|█████▋    | 1065/1868 [55:18:22<41:45:27, 187.21s/it]{'loss': 1.3669, 'grad_norm': 0.0, 'learning_rate': 8.618843683083512e-06, 'epoch': 0.57}
? Step 1065  Epoch: 0.57  Loss: 1.3951
                                                           57%|█████▋    | 1065/1868 [55:18:22<41:45:27, 187.21s/it] 57%|█████▋    | 1066/1868 [55:21:30<41:42:17, 187.20s/it]{'loss': 1.3951, 'grad_norm': 0.0, 'learning_rate': 8.60813704496788e-06, 'epoch': 0.57}
? Step 1066  Epoch: 0.57  Loss: 1.3721
                                                           57%|█████▋    | 1066/1868 [55:21:30<41:42:17, 187.20s/it] 57%|█████▋    | 1067/1868 [55:24:37<41:39:36, 187.24s/it]{'loss': 1.3721, 'grad_norm': 0.0, 'learning_rate': 8.59743040685225e-06, 'epoch': 0.57}
? Step 1067  Epoch: 0.57  Loss: 1.5291
                                                           57%|█████▋    | 1067/1868 [55:24:37<41:39:36, 187.24s/it] 57%|█████▋    | 1068/1868 [55:27:44<41:36:53, 187.27s/it]{'loss': 1.5291, 'grad_norm': 0.0, 'learning_rate': 8.586723768736618e-06, 'epoch': 0.57}
? Step 1068  Epoch: 0.57  Loss: 1.4911
                                                           57%|█████▋    | 1068/1868 [55:27:44<41:36:53, 187.27s/it] 57%|█████▋    | 1069/1868 [55:30:51<41:31:50, 187.12s/it]{'loss': 1.4911, 'grad_norm': 0.0, 'learning_rate': 8.576017130620987e-06, 'epoch': 0.57}
? Step 1069  Epoch: 0.57  Loss: 1.6945
                                                           57%|█████▋    | 1069/1868 [55:30:51<41:31:50, 187.12s/it] 57%|█████▋    | 1070/1868 [55:33:58<41:28:29, 187.10s/it]{'loss': 1.6945, 'grad_norm': 0.0, 'learning_rate': 8.565310492505354e-06, 'epoch': 0.57}
? Step 1070  Epoch: 0.57  Loss: 1.6458
                                                           57%|█████▋    | 1070/1868 [55:33:58<41:28:29, 187.10s/it] 57%|█████▋    | 1071/1868 [55:37:05<41:25:29, 187.11s/it]{'loss': 1.6458, 'grad_norm': 0.0, 'learning_rate': 8.554603854389722e-06, 'epoch': 0.57}
? Step 1071  Epoch: 0.57  Loss: 1.4674
                                                           57%|█████▋    | 1071/1868 [55:37:05<41:25:29, 187.11s/it] 57%|█████▋    | 1072/1868 [55:40:13<41:23:06, 187.17s/it]{'loss': 1.4674, 'grad_norm': 0.0, 'learning_rate': 8.543897216274091e-06, 'epoch': 0.57}
? Step 1072  Epoch: 0.57  Loss: 1.6927
                                                           57%|█████▋    | 1072/1868 [55:40:13<41:23:06, 187.17s/it] 57%|█████▋    | 1073/1868 [55:43:20<41:20:10, 187.18s/it]{'loss': 1.6927, 'grad_norm': 0.0, 'learning_rate': 8.53319057815846e-06, 'epoch': 0.57}
? Step 1073  Epoch: 0.57  Loss: 1.5174
                                                           57%|█████▋    | 1073/1868 [55:43:20<41:20:10, 187.18s/it] 57%|█████▋    | 1074/1868 [55:46:27<41:17:22, 187.21s/it]{'loss': 1.5174, 'grad_norm': 0.0, 'learning_rate': 8.522483940042827e-06, 'epoch': 0.57}
? Step 1074  Epoch: 0.57  Loss: 1.6792
                                                           57%|█████▋    | 1074/1868 [55:46:27<41:17:22, 187.21s/it] 58%|█████▊    | 1075/1868 [55:49:34<41:14:10, 187.20s/it]{'loss': 1.6792, 'grad_norm': 0.0, 'learning_rate': 8.511777301927195e-06, 'epoch': 0.57}
? Step 1075  Epoch: 0.58  Loss: 1.9480
                                                           58%|█████▊    | 1075/1868 [55:49:34<41:14:10, 187.20s/it] 58%|█████▊    | 1076/1868 [55:52:41<41:10:38, 187.17s/it]{'loss': 1.948, 'grad_norm': 0.0, 'learning_rate': 8.501070663811564e-06, 'epoch': 0.58}
? Step 1076  Epoch: 0.58  Loss: 1.4270
                                                           58%|█████▊    | 1076/1868 [55:52:41<41:10:38, 187.17s/it] 58%|█████▊    | 1077/1868 [55:55:49<41:08:14, 187.22s/it]{'loss': 1.427, 'grad_norm': 0.0, 'learning_rate': 8.490364025695931e-06, 'epoch': 0.58}
? Step 1077  Epoch: 0.58  Loss: 1.5658
                                                           58%|█████▊    | 1077/1868 [55:55:49<41:08:14, 187.22s/it] 58%|█████▊    | 1078/1868 [55:58:56<41:04:33, 187.18s/it]{'loss': 1.5658, 'grad_norm': 0.0, 'learning_rate': 8.4796573875803e-06, 'epoch': 0.58}
? Step 1078  Epoch: 0.58  Loss: 1.7389
                                                           58%|█████▊    | 1078/1868 [55:58:56<41:04:33, 187.18s/it] 58%|█████▊    | 1079/1868 [56:02:03<41:01:18, 187.17s/it]{'loss': 1.7389, 'grad_norm': 0.0, 'learning_rate': 8.468950749464668e-06, 'epoch': 0.58}
? Step 1079  Epoch: 0.58  Loss: 1.3821
                                                           58%|█████▊    | 1079/1868 [56:02:03<41:01:18, 187.17s/it] 58%|█████▊    | 1080/1868 [56:05:10<40:58:40, 187.21s/it]{'loss': 1.3821, 'grad_norm': 0.0, 'learning_rate': 8.458244111349037e-06, 'epoch': 0.58}
? Step 1080  Epoch: 0.58  Loss: 1.6328
                                                           58%|█████▊    | 1080/1868 [56:05:10<40:58:40, 187.21s/it] 58%|█████▊    | 1081/1868 [56:08:18<40:56:49, 187.30s/it]{'loss': 1.6328, 'grad_norm': 0.0, 'learning_rate': 8.447537473233406e-06, 'epoch': 0.58}
? Step 1081  Epoch: 0.58  Loss: 1.7506
                                                           58%|█████▊    | 1081/1868 [56:08:18<40:56:49, 187.30s/it] 58%|█████▊    | 1082/1868 [56:11:25<40:54:18, 187.35s/it]{'loss': 1.7506, 'grad_norm': 0.0, 'learning_rate': 8.436830835117774e-06, 'epoch': 0.58}
? Step 1082  Epoch: 0.58  Loss: 1.6022
                                                           58%|█████▊    | 1082/1868 [56:11:25<40:54:18, 187.35s/it] 58%|█████▊    | 1083/1868 [56:14:32<40:50:38, 187.31s/it]{'loss': 1.6022, 'grad_norm': 0.0, 'learning_rate': 8.426124197002143e-06, 'epoch': 0.58}
? Step 1083  Epoch: 0.58  Loss: 1.9627
                                                           58%|█████▊    | 1083/1868 [56:14:32<40:50:38, 187.31s/it] 58%|█████▊    | 1084/1868 [56:17:40<40:48:21, 187.37s/it]{'loss': 1.9627, 'grad_norm': 0.0, 'learning_rate': 8.41541755888651e-06, 'epoch': 0.58}
? Step 1084  Epoch: 0.58  Loss: 1.5827
                                                           58%|█████▊    | 1084/1868 [56:17:40<40:48:21, 187.37s/it] 58%|█████▊    | 1085/1868 [56:20:48<40:46:22, 187.46s/it]{'loss': 1.5827, 'grad_norm': 0.0, 'learning_rate': 8.404710920770879e-06, 'epoch': 0.58}
? Step 1085  Epoch: 0.58  Loss: 1.4690
                                                           58%|█████▊    | 1085/1868 [56:20:48<40:46:22, 187.46s/it] 58%|█████▊    | 1086/1868 [56:23:55<40:44:08, 187.53s/it]{'loss': 1.469, 'grad_norm': 0.0, 'learning_rate': 8.394004282655247e-06, 'epoch': 0.58}
? Step 1086  Epoch: 0.58  Loss: 1.3906
                                                           58%|█████▊    | 1086/1868 [56:23:55<40:44:08, 187.53s/it] 58%|█████▊    | 1087/1868 [56:27:03<40:41:50, 187.59s/it]{'loss': 1.3906, 'grad_norm': 0.0, 'learning_rate': 8.383297644539616e-06, 'epoch': 0.58}
? Step 1087  Epoch: 0.58  Loss: 1.3084
                                                           58%|█████▊    | 1087/1868 [56:27:03<40:41:50, 187.59s/it] 58%|█████▊    | 1088/1868 [56:30:11<40:38:34, 187.58s/it]{'loss': 1.3084, 'grad_norm': 0.0, 'learning_rate': 8.372591006423983e-06, 'epoch': 0.58}
? Step 1088  Epoch: 0.58  Loss: 1.3878
                                                           58%|█████▊    | 1088/1868 [56:30:11<40:38:34, 187.58s/it] 58%|█████▊    | 1089/1868 [56:33:18<40:35:16, 187.57s/it]{'loss': 1.3878, 'grad_norm': 0.0, 'learning_rate': 8.361884368308352e-06, 'epoch': 0.58}
? Step 1089  Epoch: 0.58  Loss: 1.5569
                                                           58%|█████▊    | 1089/1868 [56:33:18<40:35:16, 187.57s/it] 58%|█████▊    | 1090/1868 [56:36:26<40:32:29, 187.60s/it]{'loss': 1.5569, 'grad_norm': 0.0, 'learning_rate': 8.35117773019272e-06, 'epoch': 0.58}
? Step 1090  Epoch: 0.58  Loss: 1.5094
                                                           58%|█████▊    | 1090/1868 [56:36:26<40:32:29, 187.60s/it] 58%|█████▊    | 1091/1868 [56:39:33<40:28:35, 187.54s/it]{'loss': 1.5094, 'grad_norm': 0.0, 'learning_rate': 8.340471092077087e-06, 'epoch': 0.58}
? Step 1091  Epoch: 0.58  Loss: 1.7762
                                                           58%|█████▊    | 1091/1868 [56:39:33<40:28:35, 187.54s/it] 58%|█████▊    | 1092/1868 [56:42:41<40:26:07, 187.59s/it]{'loss': 1.7762, 'grad_norm': 0.0, 'learning_rate': 8.329764453961456e-06, 'epoch': 0.58}
? Step 1092  Epoch: 0.58  Loss: 1.5203
                                                           58%|█████▊    | 1092/1868 [56:42:41<40:26:07, 187.59s/it] 59%|█████▊    | 1093/1868 [56:45:48<40:22:38, 187.56s/it]{'loss': 1.5203, 'grad_norm': 0.0, 'learning_rate': 8.319057815845825e-06, 'epoch': 0.58}
? Step 1093  Epoch: 0.59  Loss: 1.7615
                                                           59%|█████▊    | 1093/1868 [56:45:48<40:22:38, 187.56s/it] 59%|█████▊    | 1094/1868 [56:48:56<40:19:33, 187.56s/it]{'loss': 1.7615, 'grad_norm': 0.0, 'learning_rate': 8.308351177730193e-06, 'epoch': 0.59}
? Step 1094  Epoch: 0.59  Loss: 1.6203
                                                           59%|█████▊    | 1094/1868 [56:48:56<40:19:33, 187.56s/it] 59%|█████▊    | 1095/1868 [56:52:04<40:17:06, 187.62s/it]{'loss': 1.6203, 'grad_norm': 0.0, 'learning_rate': 8.297644539614562e-06, 'epoch': 0.59}
? Step 1095  Epoch: 0.59  Loss: 1.2811
                                                           59%|█████▊    | 1095/1868 [56:52:04<40:17:06, 187.62s/it] 59%|█████▊    | 1096/1868 [56:55:11<40:13:37, 187.59s/it]{'loss': 1.2811, 'grad_norm': 0.0, 'learning_rate': 8.28693790149893e-06, 'epoch': 0.59}
? Step 1096  Epoch: 0.59  Loss: 1.6743
                                                           59%|█████▊    | 1096/1868 [56:55:11<40:13:37, 187.59s/it] 59%|█████▊    | 1097/1868 [56:58:19<40:11:01, 187.63s/it]{'loss': 1.6743, 'grad_norm': 0.0, 'learning_rate': 8.2762312633833e-06, 'epoch': 0.59}
? Step 1097  Epoch: 0.59  Loss: 1.5450
                                                           59%|█████▊    | 1097/1868 [56:58:19<40:11:01, 187.63s/it] 59%|█████▉    | 1098/1868 [57:01:27<40:08:15, 187.66s/it]{'loss': 1.545, 'grad_norm': 0.0, 'learning_rate': 8.265524625267666e-06, 'epoch': 0.59}
? Step 1098  Epoch: 0.59  Loss: 1.5895
                                                           59%|█████▉    | 1098/1868 [57:01:27<40:08:15, 187.66s/it] 59%|█████▉    | 1099/1868 [57:04:35<40:06:00, 187.73s/it]{'loss': 1.5895, 'grad_norm': 0.0, 'learning_rate': 8.254817987152035e-06, 'epoch': 0.59}
? Step 1099  Epoch: 0.59  Loss: 1.6037
                                                           59%|█████▉    | 1099/1868 [57:04:35<40:06:00, 187.73s/it] 59%|█████▉    | 1100/1868 [57:07:42<40:02:20, 187.68s/it]{'loss': 1.6037, 'grad_norm': 0.0, 'learning_rate': 8.244111349036404e-06, 'epoch': 0.59}
? Step 1100  Epoch: 0.59  Loss: 1.5611
                                                           59%|█████▉    | 1100/1868 [57:07:42<40:02:20, 187.68s/it]/home/dev25-01/mistral-env/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 59%|█████▉    | 1101/1868 [57:10:50<39:59:34, 187.71s/it]{'loss': 1.5611, 'grad_norm': 0.0, 'learning_rate': 8.23340471092077e-06, 'epoch': 0.59}
? Step 1101  Epoch: 0.59  Loss: 1.5126
                                                           59%|█████▉    | 1101/1868 [57:10:50<39:59:34, 187.71s/it] 59%|█████▉    | 1102/1868 [57:13:58<39:57:22, 187.78s/it]{'loss': 1.5126, 'grad_norm': 0.0, 'learning_rate': 8.22269807280514e-06, 'epoch': 0.59}
? Step 1102  Epoch: 0.59  Loss: 1.6282
                                                           59%|█████▉    | 1102/1868 [57:13:58<39:57:22, 187.78s/it] 59%|█████▉    | 1103/1868 [57:17:06<39:55:01, 187.84s/it]{'loss': 1.6282, 'grad_norm': 0.0, 'learning_rate': 8.211991434689508e-06, 'epoch': 0.59}
? Step 1103  Epoch: 0.59  Loss: 1.5122
                                                           59%|█████▉    | 1103/1868 [57:17:06<39:55:01, 187.84s/it] 59%|█████▉    | 1104/1868 [57:20:14<39:51:31, 187.82s/it]{'loss': 1.5122, 'grad_norm': 0.0, 'learning_rate': 8.201284796573877e-06, 'epoch': 0.59}
? Step 1104  Epoch: 0.59  Loss: 1.5120
                                                           59%|█████▉    | 1104/1868 [57:20:14<39:51:31, 187.82s/it] 59%|█████▉    | 1105/1868 [57:23:21<39:47:13, 187.72s/it]{'loss': 1.512, 'grad_norm': 0.0, 'learning_rate': 8.190578158458244e-06, 'epoch': 0.59}
? Step 1105  Epoch: 0.59  Loss: 1.4737
                                                           59%|█████▉    | 1105/1868 [57:23:21<39:47:13, 187.72s/it] 59%|█████▉    | 1106/1868 [57:26:28<39:42:46, 187.62s/it]{'loss': 1.4737, 'grad_norm': 0.0, 'learning_rate': 8.179871520342612e-06, 'epoch': 0.59}
? Step 1106  Epoch: 0.59  Loss: 1.5799
                                                           59%|█████▉    | 1106/1868 [57:26:28<39:42:46, 187.62s/it] 59%|█████▉    | 1107/1868 [57:29:36<39:39:12, 187.59s/it]{'loss': 1.5799, 'grad_norm': 0.0, 'learning_rate': 8.169164882226981e-06, 'epoch': 0.59}
? Step 1107  Epoch: 0.59  Loss: 1.9440
                                                           59%|█████▉    | 1107/1868 [57:29:36<39:39:12, 187.59s/it] 59%|█████▉    | 1108/1868 [57:32:44<39:35:56, 187.57s/it]{'loss': 1.944, 'grad_norm': 0.0, 'learning_rate': 8.15845824411135e-06, 'epoch': 0.59}
? Step 1108  Epoch: 0.59  Loss: 1.5444
                                                           59%|█████▉    | 1108/1868 [57:32:44<39:35:56, 187.57s/it] 59%|█████▉    | 1109/1868 [57:35:51<39:33:43, 187.65s/it]{'loss': 1.5444, 'grad_norm': 0.0, 'learning_rate': 8.147751605995718e-06, 'epoch': 0.59}
? Step 1109  Epoch: 0.59  Loss: 1.6168
                                                           59%|█████▉    | 1109/1868 [57:35:51<39:33:43, 187.65s/it] 59%|█████▉    | 1110/1868 [57:38:59<39:31:25, 187.71s/it]{'loss': 1.6168, 'grad_norm': 0.0, 'learning_rate': 8.137044967880087e-06, 'epoch': 0.59}
? Step 1110  Epoch: 0.59  Loss: 1.5696
                                                           59%|█████▉    | 1110/1868 [57:38:59<39:31:25, 187.71s/it] 59%|█████▉    | 1111/1868 [57:42:07<39:27:47, 187.67s/it]{'loss': 1.5696, 'grad_norm': 0.0, 'learning_rate': 8.126338329764456e-06, 'epoch': 0.59}
? Step 1111  Epoch: 0.59  Loss: 1.6037
                                                           59%|█████▉    | 1111/1868 [57:42:07<39:27:47, 187.67s/it] 60%|█████▉    | 1112/1868 [57:45:15<39:25:28, 187.74s/it]{'loss': 1.6037, 'grad_norm': 0.0, 'learning_rate': 8.115631691648823e-06, 'epoch': 0.59}
? Step 1112  Epoch: 0.60  Loss: 1.6196
                                                           60%|█████▉    | 1112/1868 [57:45:15<39:25:28, 187.74s/it] 60%|█████▉    | 1113/1868 [57:48:22<39:21:48, 187.69s/it]{'loss': 1.6196, 'grad_norm': 0.0, 'learning_rate': 8.104925053533191e-06, 'epoch': 0.6}
? Step 1113  Epoch: 0.60  Loss: 1.4141
                                                           60%|█████▉    | 1113/1868 [57:48:22<39:21:48, 187.69s/it] 60%|█████▉    | 1114/1868 [57:51:30<39:17:59, 187.64s/it]{'loss': 1.4141, 'grad_norm': 0.0, 'learning_rate': 8.09421841541756e-06, 'epoch': 0.6}
? Step 1114  Epoch: 0.60  Loss: 1.6473
                                                           60%|█████▉    | 1114/1868 [57:51:30<39:17:59, 187.64s/it] 60%|█████▉    | 1115/1868 [57:54:38<39:15:15, 187.67s/it]{'loss': 1.6473, 'grad_norm': 0.0, 'learning_rate': 8.083511777301927e-06, 'epoch': 0.6}
? Step 1115  Epoch: 0.60  Loss: 1.5228
                                                           60%|█████▉    | 1115/1868 [57:54:38<39:15:15, 187.67s/it] 60%|█████▉    | 1116/1868 [57:57:45<39:12:30, 187.70s/it]{'loss': 1.5228, 'grad_norm': 0.0, 'learning_rate': 8.072805139186296e-06, 'epoch': 0.6}
? Step 1116  Epoch: 0.60  Loss: 1.6033
                                                           60%|█████▉    | 1116/1868 [57:57:45<39:12:30, 187.70s/it] 60%|█████▉    | 1117/1868 [58:00:53<39:09:21, 187.70s/it]{'loss': 1.6033, 'grad_norm': 0.0, 'learning_rate': 8.062098501070664e-06, 'epoch': 0.6}
? Step 1117  Epoch: 0.60  Loss: 1.5553
                                                           60%|█████▉    | 1117/1868 [58:00:53<39:09:21, 187.70s/it] 60%|█████▉    | 1118/1868 [58:04:01<39:07:21, 187.79s/it]{'loss': 1.5553, 'grad_norm': 0.0, 'learning_rate': 8.051391862955033e-06, 'epoch': 0.6}
? Step 1118  Epoch: 0.60  Loss: 1.5511
                                                           60%|█████▉    | 1118/1868 [58:04:01<39:07:21, 187.79s/it] 60%|█████▉    | 1119/1868 [58:07:09<39:03:44, 187.75s/it]{'loss': 1.5511, 'grad_norm': 0.0, 'learning_rate': 8.0406852248394e-06, 'epoch': 0.6}
? Step 1119  Epoch: 0.60  Loss: 1.5220
                                                           60%|█████▉    | 1119/1868 [58:07:09<39:03:44, 187.75s/it] 60%|█████▉    | 1120/1868 [58:10:16<39:00:11, 187.72s/it]{'loss': 1.522, 'grad_norm': 0.0, 'learning_rate': 8.029978586723769e-06, 'epoch': 0.6}
? Step 1120  Epoch: 0.60  Loss: 1.5440
                                                           60%|█████▉    | 1120/1868 [58:10:16<39:00:11, 187.72s/it] 60%|██████    | 1121/1868 [58:13:24<38:57:09, 187.72s/it]{'loss': 1.544, 'grad_norm': 0.0, 'learning_rate': 8.019271948608137e-06, 'epoch': 0.6}
? Step 1121  Epoch: 0.60  Loss: 1.9720
                                                           60%|██████    | 1121/1868 [58:13:24<38:57:09, 187.72s/it] 60%|██████    | 1122/1868 [58:16:32<38:53:14, 187.66s/it]{'loss': 1.972, 'grad_norm': 0.0, 'learning_rate': 8.008565310492506e-06, 'epoch': 0.6}
? Step 1122  Epoch: 0.60  Loss: 1.8202
                                                           60%|██████    | 1122/1868 [58:16:32<38:53:14, 187.66s/it] 60%|██████    | 1123/1868 [58:19:39<38:50:25, 187.69s/it]{'loss': 1.8202, 'grad_norm': 0.0, 'learning_rate': 7.997858672376875e-06, 'epoch': 0.6}
? Step 1123  Epoch: 0.60  Loss: 1.4548
                                                           60%|██████    | 1123/1868 [58:19:39<38:50:25, 187.69s/it] 60%|██████    | 1124/1868 [58:22:47<38:48:07, 187.75s/it]{'loss': 1.4548, 'grad_norm': 0.0, 'learning_rate': 7.987152034261243e-06, 'epoch': 0.6}
? Step 1124  Epoch: 0.60  Loss: 1.7399
                                                           60%|██████    | 1124/1868 [58:22:47<38:48:07, 187.75s/it] 60%|██████    | 1125/1868 [58:25:55<38:45:44, 187.81s/it]{'loss': 1.7399, 'grad_norm': 0.0, 'learning_rate': 7.97644539614561e-06, 'epoch': 0.6}
? Step 1125  Epoch: 0.60  Loss: 1.5667
                                                           60%|██████    | 1125/1868 [58:25:55<38:45:44, 187.81s/it] 60%|██████    | 1126/1868 [58:29:03<38:42:18, 187.79s/it]{'loss': 1.5667, 'grad_norm': 0.0, 'learning_rate': 7.965738758029979e-06, 'epoch': 0.6}
? Step 1126  Epoch: 0.60  Loss: 1.7324
                                                           60%|██████    | 1126/1868 [58:29:03<38:42:18, 187.79s/it] 60%|██████    | 1127/1868 [58:32:11<38:38:34, 187.74s/it]{'loss': 1.7324, 'grad_norm': 0.0, 'learning_rate': 7.955032119914348e-06, 'epoch': 0.6}
? Step 1127  Epoch: 0.60  Loss: 1.9158
                                                           60%|██████    | 1127/1868 [58:32:11<38:38:34, 187.74s/it] 60%|██████    | 1128/1868 [58:35:18<38:34:33, 187.67s/it]{'loss': 1.9158, 'grad_norm': 0.0, 'learning_rate': 7.944325481798716e-06, 'epoch': 0.6}
? Step 1128  Epoch: 0.60  Loss: 1.6805
                                                           60%|██████    | 1128/1868 [58:35:18<38:34:33, 187.67s/it] 60%|██████    | 1129/1868 [58:38:26<38:32:28, 187.75s/it]{'loss': 1.6805, 'grad_norm': 0.0, 'learning_rate': 7.933618843683083e-06, 'epoch': 0.6}
? Step 1129  Epoch: 0.60  Loss: 1.6929
                                                           60%|██████    | 1129/1868 [58:38:26<38:32:28, 187.75s/it] 60%|██████    | 1130/1868 [58:41:34<38:29:16, 187.75s/it]{'loss': 1.6929, 'grad_norm': 0.0, 'learning_rate': 7.922912205567452e-06, 'epoch': 0.6}
? Step 1130  Epoch: 0.60  Loss: 1.5867
                                                           60%|██████    | 1130/1868 [58:41:34<38:29:16, 187.75s/it] 61%|██████    | 1131/1868 [58:44:41<38:25:28, 187.69s/it]{'loss': 1.5867, 'grad_norm': 0.0, 'learning_rate': 7.91220556745182e-06, 'epoch': 0.6}
? Step 1131  Epoch: 0.61  Loss: 1.7996
                                                           61%|██████    | 1131/1868 [58:44:41<38:25:28, 187.69s/it] 61%|██████    | 1132/1868 [58:47:49<38:22:25, 187.70s/it]{'loss': 1.7996, 'grad_norm': 0.0, 'learning_rate': 7.90149892933619e-06, 'epoch': 0.61}
? Step 1132  Epoch: 0.61  Loss: 1.4743
                                                           61%|██████    | 1132/1868 [58:47:49<38:22:25, 187.70s/it] 61%|██████    | 1133/1868 [58:50:57<38:19:25, 187.71s/it]{'loss': 1.4743, 'grad_norm': 0.0, 'learning_rate': 7.890792291220556e-06, 'epoch': 0.61}
? Step 1133  Epoch: 0.61  Loss: 1.7278
                                                           61%|██████    | 1133/1868 [58:50:57<38:19:25, 187.71s/it] 61%|██████    | 1134/1868 [58:54:04<38:16:18, 187.71s/it]{'loss': 1.7278, 'grad_norm': 0.0, 'learning_rate': 7.880085653104925e-06, 'epoch': 0.61}
? Step 1134  Epoch: 0.61  Loss: 1.6534
                                                           61%|██████    | 1134/1868 [58:54:04<38:16:18, 187.71s/it] 61%|██████    | 1135/1868 [58:57:12<38:12:46, 187.68s/it]{'loss': 1.6534, 'grad_norm': 0.0, 'learning_rate': 7.869379014989294e-06, 'epoch': 0.61}
? Step 1135  Epoch: 0.61  Loss: 1.8749
                                                           61%|██████    | 1135/1868 [58:57:12<38:12:46, 187.68s/it] 61%|██████    | 1136/1868 [59:00:20<38:09:51, 187.69s/it]{'loss': 1.8749, 'grad_norm': 0.0, 'learning_rate': 7.858672376873662e-06, 'epoch': 0.61}
? Step 1136  Epoch: 0.61  Loss: 1.5868
                                                           61%|██████    | 1136/1868 [59:00:20<38:09:51, 187.69s/it] 61%|██████    | 1137/1868 [59:03:27<38:05:46, 187.62s/it]{'loss': 1.5868, 'grad_norm': 0.0, 'learning_rate': 7.847965738758031e-06, 'epoch': 0.61}
? Step 1137  Epoch: 0.61  Loss: 1.6494
                                                           61%|██████    | 1137/1868 [59:03:27<38:05:46, 187.62s/it] 61%|██████    | 1138/1868 [59:06:35<38:03:20, 187.67s/it]{'loss': 1.6494, 'grad_norm': 0.0, 'learning_rate': 7.8372591006424e-06, 'epoch': 0.61}
? Step 1138  Epoch: 0.61  Loss: 1.5224
                                                           61%|██████    | 1138/1868 [59:06:35<38:03:20, 187.67s/it] 61%|██████    | 1139/1868 [59:09:43<38:00:21, 187.68s/it]{'loss': 1.5224, 'grad_norm': 0.0, 'learning_rate': 7.826552462526767e-06, 'epoch': 0.61}
? Step 1139  Epoch: 0.61  Loss: 1.2811
                                                           61%|██████    | 1139/1868 [59:09:43<38:00:21, 187.68s/it] 61%|██████    | 1140/1868 [59:12:50<37:56:58, 187.66s/it]{'loss': 1.2811, 'grad_norm': 0.0, 'learning_rate': 7.815845824411135e-06, 'epoch': 0.61}
? Step 1140  Epoch: 0.61  Loss: 1.5639
                                                           61%|██████    | 1140/1868 [59:12:50<37:56:58, 187.66s/it] 61%|██████    | 1141/1868 [59:15:58<37:53:57, 187.67s/it]{'loss': 1.5639, 'grad_norm': 0.0, 'learning_rate': 7.805139186295504e-06, 'epoch': 0.61}
? Step 1141  Epoch: 0.61  Loss: 1.6688
                                                           61%|██████    | 1141/1868 [59:15:58<37:53:57, 187.67s/it] 61%|██████    | 1142/1868 [59:19:06<37:50:19, 187.63s/it]{'loss': 1.6688, 'grad_norm': 0.0, 'learning_rate': 7.794432548179873e-06, 'epoch': 0.61}
? Step 1142  Epoch: 0.61  Loss: 1.8475
                                                           61%|██████    | 1142/1868 [59:19:06<37:50:19, 187.63s/it] 61%|██████    | 1143/1868 [59:22:13<37:47:51, 187.69s/it]{'loss': 1.8475, 'grad_norm': 0.0, 'learning_rate': 7.78372591006424e-06, 'epoch': 0.61}
? Step 1143  Epoch: 0.61  Loss: 1.4099
                                                           61%|██████    | 1143/1868 [59:22:13<37:47:51, 187.69s/it] 61%|██████    | 1144/1868 [59:25:21<37:45:04, 187.71s/it]{'loss': 1.4099, 'grad_norm': 0.0, 'learning_rate': 7.773019271948608e-06, 'epoch': 0.61}
? Step 1144  Epoch: 0.61  Loss: 1.5382
                                                           61%|██████    | 1144/1868 [59:25:21<37:45:04, 187.71s/it] 61%|██████▏   | 1145/1868 [59:28:29<37:42:09, 187.73s/it]{'loss': 1.5382, 'grad_norm': 0.0, 'learning_rate': 7.762312633832977e-06, 'epoch': 0.61}
? Step 1145  Epoch: 0.61  Loss: 1.5775
                                                           61%|██████▏   | 1145/1868 [59:28:29<37:42:09, 187.73s/it] 61%|██████▏   | 1146/1868 [59:31:37<37:39:02, 187.73s/it]{'loss': 1.5775, 'grad_norm': 0.0, 'learning_rate': 7.751605995717346e-06, 'epoch': 0.61}
? Step 1146  Epoch: 0.61  Loss: 1.6399
                                                           61%|██████▏   | 1146/1868 [59:31:37<37:39:02, 187.73s/it] 61%|██████▏   | 1147/1868 [59:34:44<37:35:34, 187.70s/it]{'loss': 1.6399, 'grad_norm': 0.0, 'learning_rate': 7.740899357601713e-06, 'epoch': 0.61}
? Step 1147  Epoch: 0.61  Loss: 1.7746
                                                           61%|██████▏   | 1147/1868 [59:34:44<37:35:34, 187.70s/it] 61%|██████▏   | 1148/1868 [59:37:52<37:33:36, 187.80s/it]{'loss': 1.7746, 'grad_norm': 0.0, 'learning_rate': 7.730192719486081e-06, 'epoch': 0.61}
? Step 1148  Epoch: 0.61  Loss: 1.6333
                                                           61%|██████▏   | 1148/1868 [59:37:52<37:33:36, 187.80s/it] 62%|██████▏   | 1149/1868 [59:41:00<37:29:54, 187.75s/it]{'loss': 1.6333, 'grad_norm': 0.0, 'learning_rate': 7.71948608137045e-06, 'epoch': 0.61}
? Step 1149  Epoch: 0.62  Loss: 1.7807
                                                           62%|██████▏   | 1149/1868 [59:41:00<37:29:54, 187.75s/it] 62%|██████▏   | 1150/1868 [59:44:08<37:26:40, 187.75s/it]{'loss': 1.7807, 'grad_norm': 0.0, 'learning_rate': 7.708779443254819e-06, 'epoch': 0.62}
? Step 1150  Epoch: 0.62  Loss: 1.6632
                                                           62%|██████▏   | 1150/1868 [59:44:08<37:26:40, 187.75s/it] 62%|██████▏   | 1151/1868 [59:47:15<37:23:40, 187.76s/it]{'loss': 1.6632, 'grad_norm': 0.0, 'learning_rate': 7.698072805139187e-06, 'epoch': 0.62}
? Step 1151  Epoch: 0.62  Loss: 1.5072
                                                           62%|██████▏   | 1151/1868 [59:47:15<37:23:40, 187.76s/it] 62%|██████▏   | 1152/1868 [59:50:23<37:20:37, 187.76s/it]{'loss': 1.5072, 'grad_norm': 0.0, 'learning_rate': 7.687366167023556e-06, 'epoch': 0.62}
? Step 1152  Epoch: 0.62  Loss: 1.4496
                                                           62%|██████▏   | 1152/1868 [59:50:23<37:20:37, 187.76s/it] 62%|██████▏   | 1153/1868 [59:53:31<37:16:41, 187.69s/it]{'loss': 1.4496, 'grad_norm': 0.0, 'learning_rate': 7.676659528907923e-06, 'epoch': 0.62}
? Step 1153  Epoch: 0.62  Loss: 1.7868
                                                           62%|██████▏   | 1153/1868 [59:53:31<37:16:41, 187.69s/it] 62%|██████▏   | 1154/1868 [59:56:38<37:12:49, 187.63s/it]{'loss': 1.7868, 'grad_norm': 0.0, 'learning_rate': 7.665952890792292e-06, 'epoch': 0.62}
? Step 1154  Epoch: 0.62  Loss: 1.3486
                                                           62%|██████▏   | 1154/1868 [59:56:38<37:12:49, 187.63s/it] 62%|██████▏   | 1155/1868 [59:59:46<37:10:48, 187.73s/it]{'loss': 1.3486, 'grad_norm': 0.0, 'learning_rate': 7.65524625267666e-06, 'epoch': 0.62}
? Step 1155  Epoch: 0.62  Loss: 1.6824
                                                           62%|██████▏   | 1155/1868 [59:59:46<37:10:48, 187.73s/it] 62%|██████▏   | 1156/1868 [60:02:54<37:07:28, 187.71s/it]{'loss': 1.6824, 'grad_norm': 0.0, 'learning_rate': 7.644539614561029e-06, 'epoch': 0.62}
? Step 1156  Epoch: 0.62  Loss: 1.6713
                                                           62%|██████▏   | 1156/1868 [60:02:54<37:07:28, 187.71s/it] 62%|██████▏   | 1157/1868 [60:06:02<37:04:38, 187.73s/it]{'loss': 1.6713, 'grad_norm': 0.0, 'learning_rate': 7.633832976445396e-06, 'epoch': 0.62}
? Step 1157  Epoch: 0.62  Loss: 1.6026
                                                           62%|██████▏   | 1157/1868 [60:06:02<37:04:38, 187.73s/it] 62%|██████▏   | 1158/1868 [60:09:09<37:01:11, 187.71s/it]{'loss': 1.6026, 'grad_norm': 0.0, 'learning_rate': 7.623126338329765e-06, 'epoch': 0.62}
? Step 1158  Epoch: 0.62  Loss: 1.6534
                                                           62%|██████▏   | 1158/1868 [60:09:09<37:01:11, 187.71s/it] 62%|██████▏   | 1159/1868 [60:12:17<36:58:34, 187.75s/it]{'loss': 1.6534, 'grad_norm': 0.0, 'learning_rate': 7.612419700214133e-06, 'epoch': 0.62}
? Step 1159  Epoch: 0.62  Loss: 1.5770
                                                           62%|██████▏   | 1159/1868 [60:12:17<36:58:34, 187.75s/it] 62%|██████▏   | 1160/1868 [60:15:25<36:54:57, 187.71s/it]{'loss': 1.577, 'grad_norm': 0.0, 'learning_rate': 7.601713062098501e-06, 'epoch': 0.62}
? Step 1160  Epoch: 0.62  Loss: 1.5069
                                                           62%|██████▏   | 1160/1868 [60:15:25<36:54:57, 187.71s/it] 62%|██████▏   | 1161/1868 [60:18:33<36:52:56, 187.80s/it]{'loss': 1.5069, 'grad_norm': 0.0, 'learning_rate': 7.59100642398287e-06, 'epoch': 0.62}
? Step 1161  Epoch: 0.62  Loss: 1.5140
                                                           62%|██████▏   | 1161/1868 [60:18:33<36:52:56, 187.80s/it] 62%|██████▏   | 1162/1868 [60:21:41<36:49:44, 187.80s/it]{'loss': 1.514, 'grad_norm': 0.0, 'learning_rate': 7.580299785867238e-06, 'epoch': 0.62}
? Step 1162  Epoch: 0.62  Loss: 1.5746
                                                           62%|██████▏   | 1162/1868 [60:21:41<36:49:44, 187.80s/it] 62%|██████▏   | 1163/1868 [60:24:48<36:45:50, 187.73s/it]{'loss': 1.5746, 'grad_norm': 0.0, 'learning_rate': 7.569593147751607e-06, 'epoch': 0.62}
? Step 1163  Epoch: 0.62  Loss: 1.5544
                                                           62%|██████▏   | 1163/1868 [60:24:48<36:45:50, 187.73s/it] 62%|██████▏   | 1164/1868 [60:27:56<36:42:58, 187.75s/it]{'loss': 1.5544, 'grad_norm': 0.0, 'learning_rate': 7.558886509635975e-06, 'epoch': 0.62}
? Step 1164  Epoch: 0.62  Loss: 1.5172
                                                           62%|██████▏   | 1164/1868 [60:27:56<36:42:58, 187.75s/it] 62%|██████▏   | 1165/1868 [60:31:04<36:40:29, 187.81s/it]{'loss': 1.5172, 'grad_norm': 0.0, 'learning_rate': 7.548179871520344e-06, 'epoch': 0.62}
? Step 1165  Epoch: 0.62  Loss: 1.7104
                                                           62%|██████▏   | 1165/1868 [60:31:04<36:40:29, 187.81s/it] 62%|██████▏   | 1166/1868 [60:34:12<36:38:25, 187.90s/it]{'loss': 1.7104, 'grad_norm': 0.0, 'learning_rate': 7.5374732334047115e-06, 'epoch': 0.62}
? Step 1166  Epoch: 0.62  Loss: 1.4192
                                                           62%|██████▏   | 1166/1868 [60:34:12<36:38:25, 187.90s/it] 62%|██████▏   | 1167/1868 [60:37:20<36:34:20, 187.82s/it]{'loss': 1.4192, 'grad_norm': 0.0, 'learning_rate': 7.52676659528908e-06, 'epoch': 0.62}
? Step 1167  Epoch: 0.62  Loss: 1.7018
                                                           62%|██████▏   | 1167/1868 [60:37:20<36:34:20, 187.82s/it] 63%|██████▎   | 1168/1868 [60:40:28<36:31:30, 187.84s/it]{'loss': 1.7018, 'grad_norm': 0.0, 'learning_rate': 7.516059957173448e-06, 'epoch': 0.62}
? Step 1168  Epoch: 0.63  Loss: 1.5202
                                                           63%|██████▎   | 1168/1868 [60:40:28<36:31:30, 187.84s/it] 63%|██████▎   | 1169/1868 [60:43:35<36:28:20, 187.84s/it]{'loss': 1.5202, 'grad_norm': 0.0, 'learning_rate': 7.505353319057817e-06, 'epoch': 0.63}
? Step 1169  Epoch: 0.63  Loss: 1.6834
                                                           63%|██████▎   | 1169/1868 [60:43:35<36:28:20, 187.84s/it] 63%|██████▎   | 1170/1868 [60:46:43<36:24:59, 187.82s/it]{'loss': 1.6834, 'grad_norm': 0.0, 'learning_rate': 7.4946466809421845e-06, 'epoch': 0.63}
? Step 1170  Epoch: 0.63  Loss: 1.8246
                                                           63%|██████▎   | 1170/1868 [60:46:43<36:24:59, 187.82s/it] 63%|██████▎   | 1171/1868 [60:49:51<36:21:32, 187.79s/it]{'loss': 1.8246, 'grad_norm': 0.0, 'learning_rate': 7.483940042826553e-06, 'epoch': 0.63}
? Step 1171  Epoch: 0.63  Loss: 1.8189
                                                           63%|██████▎   | 1171/1868 [60:49:51<36:21:32, 187.79s/it] 63%|██████▎   | 1172/1868 [60:52:59<36:17:54, 187.75s/it]{'loss': 1.8189, 'grad_norm': 0.0, 'learning_rate': 7.473233404710921e-06, 'epoch': 0.63}
? Step 1172  Epoch: 0.63  Loss: 1.4728
                                                           63%|██████▎   | 1172/1868 [60:52:59<36:17:54, 187.75s/it] 63%|██████▎   | 1173/1868 [60:56:06<36:14:41, 187.74s/it]{'loss': 1.4728, 'grad_norm': 0.0, 'learning_rate': 7.46252676659529e-06, 'epoch': 0.63}
? Step 1173  Epoch: 0.63  Loss: 1.5043
                                                           63%|██████▎   | 1173/1868 [60:56:06<36:14:41, 187.74s/it] 63%|██████▎   | 1174/1868 [60:59:14<36:12:23, 187.81s/it]{'loss': 1.5043, 'grad_norm': 0.0, 'learning_rate': 7.4518201284796575e-06, 'epoch': 0.63}
? Step 1174  Epoch: 0.63  Loss: 1.7812
                                                           63%|██████▎   | 1174/1868 [60:59:14<36:12:23, 187.81s/it] 63%|██████▎   | 1175/1868 [61:02:22<36:08:13, 187.73s/it]{'loss': 1.7812, 'grad_norm': 0.0, 'learning_rate': 7.441113490364026e-06, 'epoch': 0.63}
? Step 1175  Epoch: 0.63  Loss: 1.8327
                                                           63%|██████▎   | 1175/1868 [61:02:22<36:08:13, 187.73s/it] 63%|██████▎   | 1176/1868 [61:05:30<36:05:36, 187.77s/it]{'loss': 1.8327, 'grad_norm': 0.0, 'learning_rate': 7.430406852248394e-06, 'epoch': 0.63}
? Step 1176  Epoch: 0.63  Loss: 1.5232
                                                           63%|██████▎   | 1176/1868 [61:05:30<36:05:36, 187.77s/it] 63%|██████▎   | 1177/1868 [61:08:37<36:02:37, 187.78s/it]{'loss': 1.5232, 'grad_norm': 0.0, 'learning_rate': 7.4197002141327635e-06, 'epoch': 0.63}
? Step 1177  Epoch: 0.63  Loss: 1.5915
                                                           63%|██████▎   | 1177/1868 [61:08:37<36:02:37, 187.78s/it] 63%|██████▎   | 1178/1868 [61:11:45<35:59:25, 187.78s/it]{'loss': 1.5915, 'grad_norm': 0.0, 'learning_rate': 7.408993576017131e-06, 'epoch': 0.63}
? Step 1178  Epoch: 0.63  Loss: 1.6312
                                                           63%|██████▎   | 1178/1868 [61:11:45<35:59:25, 187.78s/it] 63%|██████▎   | 1179/1868 [61:14:53<35:55:53, 187.74s/it]{'loss': 1.6312, 'grad_norm': 0.0, 'learning_rate': 7.3982869379015e-06, 'epoch': 0.63}
? Step 1179  Epoch: 0.63  Loss: 1.7008
                                                           63%|██████▎   | 1179/1868 [61:14:53<35:55:53, 187.74s/it] 63%|██████▎   | 1180/1868 [61:18:01<35:52:41, 187.73s/it]{'loss': 1.7008, 'grad_norm': 0.0, 'learning_rate': 7.387580299785868e-06, 'epoch': 0.63}
? Step 1180  Epoch: 0.63  Loss: 1.4051
                                                           63%|██████▎   | 1180/1868 [61:18:01<35:52:41, 187.73s/it] 63%|██████▎   | 1181/1868 [61:21:08<35:49:51, 187.76s/it]{'loss': 1.4051, 'grad_norm': 0.0, 'learning_rate': 7.3768736616702365e-06, 'epoch': 0.63}
? Step 1181  Epoch: 0.63  Loss: 1.4656
                                                           63%|██████▎   | 1181/1868 [61:21:08<35:49:51, 187.76s/it] 63%|██████▎   | 1182/1868 [61:24:16<35:46:23, 187.73s/it]{'loss': 1.4656, 'grad_norm': 0.0, 'learning_rate': 7.366167023554604e-06, 'epoch': 0.63}
? Step 1182  Epoch: 0.63  Loss: 1.5608
                                                           63%|██████▎   | 1182/1868 [61:24:16<35:46:23, 187.73s/it] 63%|██████▎   | 1183/1868 [61:27:24<35:43:27, 187.75s/it]{'loss': 1.5608, 'grad_norm': 0.0, 'learning_rate': 7.355460385438973e-06, 'epoch': 0.63}
? Step 1183  Epoch: 0.63  Loss: 1.6204
                                                           63%|██████▎   | 1183/1868 [61:27:24<35:43:27, 187.75s/it] 63%|██████▎   | 1184/1868 [61:30:32<35:40:21, 187.75s/it]{'loss': 1.6204, 'grad_norm': 0.0, 'learning_rate': 7.344753747323341e-06, 'epoch': 0.63}
? Step 1184  Epoch: 0.63  Loss: 1.5835
                                                           63%|██████▎   | 1184/1868 [61:30:32<35:40:21, 187.75s/it] 63%|██████▎   | 1185/1868 [61:33:39<35:37:22, 187.76s/it]{'loss': 1.5835, 'grad_norm': 0.0, 'learning_rate': 7.3340471092077095e-06, 'epoch': 0.63}
? Step 1185  Epoch: 0.63  Loss: 1.5416
                                                           63%|██████▎   | 1185/1868 [61:33:39<35:37:22, 187.76s/it] 63%|██████▎   | 1186/1868 [61:36:47<35:34:26, 187.78s/it]{'loss': 1.5416, 'grad_norm': 0.0, 'learning_rate': 7.323340471092077e-06, 'epoch': 0.63}
? Step 1186  Epoch: 0.63  Loss: 1.6059
                                                           63%|██████▎   | 1186/1868 [61:36:47<35:34:26, 187.78s/it] 64%|██████▎   | 1187/1868 [61:39:55<35:31:11, 187.77s/it]{'loss': 1.6059, 'grad_norm': 0.0, 'learning_rate': 7.312633832976446e-06, 'epoch': 0.63}
? Step 1187  Epoch: 0.64  Loss: 1.4569
                                                           64%|██████▎   | 1187/1868 [61:39:55<35:31:11, 187.77s/it] 64%|██████▎   | 1188/1868 [61:43:03<35:28:15, 187.79s/it]{'loss': 1.4569, 'grad_norm': 0.0, 'learning_rate': 7.301927194860814e-06, 'epoch': 0.64}
? Step 1188  Epoch: 0.64  Loss: 1.6077
                                                           64%|██████▎   | 1188/1868 [61:43:03<35:28:15, 187.79s/it] 64%|██████▎   | 1189/1868 [61:46:11<35:25:25, 187.81s/it]{'loss': 1.6077, 'grad_norm': 0.0, 'learning_rate': 7.2912205567451825e-06, 'epoch': 0.64}
? Step 1189  Epoch: 0.64  Loss: 1.7132
                                                           64%|██████▎   | 1189/1868 [61:46:11<35:25:25, 187.81s/it] 64%|██████▎   | 1190/1868 [61:49:19<35:22:34, 187.84s/it]{'loss': 1.7132, 'grad_norm': 0.0, 'learning_rate': 7.28051391862955e-06, 'epoch': 0.64}
? Step 1190  Epoch: 0.64  Loss: 1.5996
                                                           64%|██████▎   | 1190/1868 [61:49:19<35:22:34, 187.84s/it] 64%|██████▍   | 1191/1868 [61:52:27<35:20:42, 187.95s/it]{'loss': 1.5996, 'grad_norm': 0.0, 'learning_rate': 7.26980728051392e-06, 'epoch': 0.64}
? Step 1191  Epoch: 0.64  Loss: 1.4249
                                                           64%|██████▍   | 1191/1868 [61:52:27<35:20:42, 187.95s/it] 64%|██████▍   | 1192/1868 [61:55:35<35:17:31, 187.95s/it]{'loss': 1.4249, 'grad_norm': 0.0, 'learning_rate': 7.259100642398288e-06, 'epoch': 0.64}
? Step 1192  Epoch: 0.64  Loss: 1.6132
                                                           64%|██████▍   | 1192/1868 [61:55:35<35:17:31, 187.95s/it] 64%|██████▍   | 1193/1868 [61:58:42<35:13:34, 187.87s/it]{'loss': 1.6132, 'grad_norm': 0.0, 'learning_rate': 7.248394004282656e-06, 'epoch': 0.64}
? Step 1193  Epoch: 0.64  Loss: 1.7164
                                                           64%|██████▍   | 1193/1868 [61:58:42<35:13:34, 187.87s/it] 64%|██████▍   | 1194/1868 [62:01:50<35:10:18, 187.86s/it]{'loss': 1.7164, 'grad_norm': 0.0, 'learning_rate': 7.237687366167024e-06, 'epoch': 0.64}
? Step 1194  Epoch: 0.64  Loss: 1.8151
                                                           64%|██████▍   | 1194/1868 [62:01:50<35:10:18, 187.86s/it] 64%|██████▍   | 1195/1868 [62:04:58<35:07:43, 187.91s/it]{'loss': 1.8151, 'grad_norm': 0.0, 'learning_rate': 7.226980728051393e-06, 'epoch': 0.64}
? Step 1195  Epoch: 0.64  Loss: 1.5855
                                                           64%|██████▍   | 1195/1868 [62:04:58<35:07:43, 187.91s/it] 64%|██████▍   | 1196/1868 [62:08:06<35:04:17, 187.88s/it]{'loss': 1.5855, 'grad_norm': 0.0, 'learning_rate': 7.216274089935761e-06, 'epoch': 0.64}
? Step 1196  Epoch: 0.64  Loss: 1.4652
                                                           64%|██████▍   | 1196/1868 [62:08:06<35:04:17, 187.88s/it] 64%|██████▍   | 1197/1868 [62:11:14<35:01:25, 187.91s/it]{'loss': 1.4652, 'grad_norm': 0.0, 'learning_rate': 7.205567451820129e-06, 'epoch': 0.64}
? Step 1197  Epoch: 0.64  Loss: 1.8420
                                                           64%|██████▍   | 1197/1868 [62:11:14<35:01:25, 187.91s/it] 64%|██████▍   | 1198/1868 [62:14:22<34:58:12, 187.90s/it]{'loss': 1.842, 'grad_norm': 0.0, 'learning_rate': 7.194860813704497e-06, 'epoch': 0.64}
? Step 1198  Epoch: 0.64  Loss: 1.5638
                                                           64%|██████▍   | 1198/1868 [62:14:22<34:58:12, 187.90s/it] 64%|██████▍   | 1199/1868 [62:17:30<34:54:09, 187.82s/it]{'loss': 1.5638, 'grad_norm': 0.0, 'learning_rate': 7.184154175588866e-06, 'epoch': 0.64}
? Step 1199  Epoch: 0.64  Loss: 1.5801
                                                           64%|██████▍   | 1199/1868 [62:17:30<34:54:09, 187.82s/it] 64%|██████▍   | 1200/1868 [62:20:37<34:50:02, 187.73s/it]{'loss': 1.5801, 'grad_norm': 0.0, 'learning_rate': 7.173447537473234e-06, 'epoch': 0.64}
? Step 1200  Epoch: 0.64  Loss: 1.8322
                                                           64%|██████▍   | 1200/1868 [62:20:37<34:50:02, 187.73s/it]/home/dev25-01/mistral-env/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 64%|██████▍   | 1201/1868 [62:23:45<34:48:20, 187.86s/it]{'loss': 1.8322, 'grad_norm': 0.0, 'learning_rate': 7.162740899357602e-06, 'epoch': 0.64}
? Step 1201  Epoch: 0.64  Loss: 1.5415
                                                           64%|██████▍   | 1201/1868 [62:23:45<34:48:20, 187.86s/it] 64%|██████▍   | 1202/1868 [62:26:53<34:44:25, 187.79s/it]{'loss': 1.5415, 'grad_norm': 0.0, 'learning_rate': 7.15203426124197e-06, 'epoch': 0.64}
? Step 1202  Epoch: 0.64  Loss: 1.5149
                                                           64%|██████▍   | 1202/1868 [62:26:53<34:44:25, 187.79s/it] 64%|██████▍   | 1203/1868 [62:30:01<34:41:29, 187.80s/it]{'loss': 1.5149, 'grad_norm': 0.0, 'learning_rate': 7.141327623126339e-06, 'epoch': 0.64}
? Step 1203  Epoch: 0.64  Loss: 1.5714
                                                           64%|██████▍   | 1203/1868 [62:30:01<34:41:29, 187.80s/it] 64%|██████▍   | 1204/1868 [62:33:09<34:39:05, 187.87s/it]{'loss': 1.5714, 'grad_norm': 0.0, 'learning_rate': 7.1306209850107075e-06, 'epoch': 0.64}
? Step 1204  Epoch: 0.64  Loss: 1.5436
                                                           64%|██████▍   | 1204/1868 [62:33:09<34:39:05, 187.87s/it] 65%|██████▍   | 1205/1868 [62:36:17<34:36:08, 187.89s/it]{'loss': 1.5436, 'grad_norm': 0.0, 'learning_rate': 7.119914346895076e-06, 'epoch': 0.64}
? Step 1205  Epoch: 0.65  Loss: 1.6650
                                                           65%|██████▍   | 1205/1868 [62:36:17<34:36:08, 187.89s/it] 65%|██████▍   | 1206/1868 [62:39:25<34:33:11, 187.90s/it]{'loss': 1.665, 'grad_norm': 0.0, 'learning_rate': 7.109207708779444e-06, 'epoch': 0.65}
? Step 1206  Epoch: 0.65  Loss: 1.6379
                                                           65%|██████▍   | 1206/1868 [62:39:25<34:33:11, 187.90s/it] 65%|██████▍   | 1207/1868 [62:42:32<34:29:12, 187.82s/it]{'loss': 1.6379, 'grad_norm': 0.0, 'learning_rate': 7.098501070663813e-06, 'epoch': 0.65}
? Step 1207  Epoch: 0.65  Loss: 1.6612
                                                           65%|██████▍   | 1207/1868 [62:42:32<34:29:12, 187.82s/it] 65%|██████▍   | 1208/1868 [62:45:40<34:25:29, 187.77s/it]{'loss': 1.6612, 'grad_norm': 0.0, 'learning_rate': 7.0877944325481805e-06, 'epoch': 0.65}
? Step 1208  Epoch: 0.65  Loss: 1.7775
                                                           65%|██████▍   | 1208/1868 [62:45:40<34:25:29, 187.77s/it] 65%|██████▍   | 1209/1868 [62:48:48<34:22:48, 187.81s/it]{'loss': 1.7775, 'grad_norm': 0.0, 'learning_rate': 7.077087794432549e-06, 'epoch': 0.65}
? Step 1209  Epoch: 0.65  Loss: 1.4250
                                                           65%|██████▍   | 1209/1868 [62:48:48<34:22:48, 187.81s/it] 65%|██████▍   | 1210/1868 [62:51:56<34:19:46, 187.82s/it]{'loss': 1.425, 'grad_norm': 0.0, 'learning_rate': 7.066381156316917e-06, 'epoch': 0.65}
? Step 1210  Epoch: 0.65  Loss: 1.5397
                                                           65%|██████▍   | 1210/1868 [62:51:56<34:19:46, 187.82s/it] 65%|██████▍   | 1211/1868 [62:55:03<34:16:09, 187.78s/it]{'loss': 1.5397, 'grad_norm': 0.0, 'learning_rate': 7.055674518201286e-06, 'epoch': 0.65}
? Step 1211  Epoch: 0.65  Loss: 1.7458
                                                           65%|██████▍   | 1211/1868 [62:55:03<34:16:09, 187.78s/it] 65%|██████▍   | 1212/1868 [62:58:11<34:12:18, 187.71s/it]{'loss': 1.7458, 'grad_norm': 0.0, 'learning_rate': 7.0449678800856535e-06, 'epoch': 0.65}
? Step 1212  Epoch: 0.65  Loss: 1.4677
                                                           65%|██████▍   | 1212/1868 [62:58:11<34:12:18, 187.71s/it] 65%|██████▍   | 1213/1868 [63:01:18<34:08:30, 187.65s/it]{'loss': 1.4677, 'grad_norm': 0.0, 'learning_rate': 7.034261241970022e-06, 'epoch': 0.65}
? Step 1213  Epoch: 0.65  Loss: 1.8171
                                                           65%|██████▍   | 1213/1868 [63:01:18<34:08:30, 187.65s/it] 65%|██████▍   | 1214/1868 [63:04:26<34:05:17, 187.64s/it]{'loss': 1.8171, 'grad_norm': 0.0, 'learning_rate': 7.02355460385439e-06, 'epoch': 0.65}
? Step 1214  Epoch: 0.65  Loss: 1.5683
                                                           65%|██████▍   | 1214/1868 [63:04:26<34:05:17, 187.64s/it] 65%|██████▌   | 1215/1868 [63:07:34<34:03:18, 187.75s/it]{'loss': 1.5683, 'grad_norm': 0.0, 'learning_rate': 7.012847965738759e-06, 'epoch': 0.65}
? Step 1215  Epoch: 0.65  Loss: 1.5137
                                                           65%|██████▌   | 1215/1868 [63:07:34<34:03:18, 187.75s/it] 65%|██████▌   | 1216/1868 [63:10:42<34:00:45, 187.80s/it]{'loss': 1.5137, 'grad_norm': 0.0, 'learning_rate': 7.0021413276231265e-06, 'epoch': 0.65}
? Step 1216  Epoch: 0.65  Loss: 1.5424
                                                           65%|██████▌   | 1216/1868 [63:10:42<34:00:45, 187.80s/it] 65%|██████▌   | 1217/1868 [63:13:50<33:57:51, 187.82s/it]{'loss': 1.5424, 'grad_norm': 0.0, 'learning_rate': 6.991434689507494e-06, 'epoch': 0.65}
? Step 1217  Epoch: 0.65  Loss: 1.5876
                                                           65%|██████▌   | 1217/1868 [63:13:50<33:57:51, 187.82s/it] 65%|██████▌   | 1218/1868 [63:16:58<33:55:11, 187.86s/it]{'loss': 1.5876, 'grad_norm': 0.0, 'learning_rate': 6.980728051391864e-06, 'epoch': 0.65}
? Step 1218  Epoch: 0.65  Loss: 1.3907
                                                           65%|██████▌   | 1218/1868 [63:16:58<33:55:11, 187.86s/it] 65%|██████▌   | 1219/1868 [63:20:05<33:50:59, 187.76s/it]{'loss': 1.3907, 'grad_norm': 0.0, 'learning_rate': 6.9700214132762325e-06, 'epoch': 0.65}
? Step 1219  Epoch: 0.65  Loss: 1.4034
                                                           65%|██████▌   | 1219/1868 [63:20:05<33:50:59, 187.76s/it] 65%|██████▌   | 1220/1868 [63:23:13<33:47:52, 187.77s/it]{'loss': 1.4034, 'grad_norm': 0.0, 'learning_rate': 6.9593147751606e-06, 'epoch': 0.65}
? Step 1220  Epoch: 0.65  Loss: 1.5190
                                                           65%|██████▌   | 1220/1868 [63:23:13<33:47:52, 187.77s/it] 65%|██████▌   | 1221/1868 [63:26:21<33:45:19, 187.82s/it]{'loss': 1.519, 'grad_norm': 0.0, 'learning_rate': 6.948608137044969e-06, 'epoch': 0.65}
? Step 1221  Epoch: 0.65  Loss: 1.5403
                                                           65%|██████▌   | 1221/1868 [63:26:21<33:45:19, 187.82s/it] 65%|██████▌   | 1222/1868 [63:29:29<33:42:27, 187.84s/it]{'loss': 1.5403, 'grad_norm': 0.0, 'learning_rate': 6.937901498929337e-06, 'epoch': 0.65}
? Step 1222  Epoch: 0.65  Loss: 1.5307
                                                           65%|██████▌   | 1222/1868 [63:29:29<33:42:27, 187.84s/it] 65%|██████▌   | 1223/1868 [63:32:36<33:37:34, 187.68s/it]{'loss': 1.5307, 'grad_norm': 0.0, 'learning_rate': 6.9271948608137055e-06, 'epoch': 0.65}
? Step 1223  Epoch: 0.65  Loss: 1.7919
                                                           65%|██████▌   | 1223/1868 [63:32:36<33:37:34, 187.68s/it] 66%|██████▌   | 1224/1868 [63:35:44<33:34:21, 187.67s/it]{'loss': 1.7919, 'grad_norm': 0.0, 'learning_rate': 6.916488222698073e-06, 'epoch': 0.65}
? Step 1224  Epoch: 0.66  Loss: 1.5606
                                                           66%|██████▌   | 1224/1868 [63:35:44<33:34:21, 187.67s/it] 66%|██████▌   | 1225/1868 [63:38:52<33:31:48, 187.73s/it]{'loss': 1.5606, 'grad_norm': 0.0, 'learning_rate': 6.905781584582442e-06, 'epoch': 0.66}
? Step 1225  Epoch: 0.66  Loss: 1.6288
                                                           66%|██████▌   | 1225/1868 [63:38:52<33:31:48, 187.73s/it] 66%|██████▌   | 1226/1868 [63:42:00<33:29:06, 187.77s/it]{'loss': 1.6288, 'grad_norm': 0.0, 'learning_rate': 6.89507494646681e-06, 'epoch': 0.66}
? Step 1226  Epoch: 0.66  Loss: 1.5444
                                                           66%|██████▌   | 1226/1868 [63:42:00<33:29:06, 187.77s/it] 66%|██████▌   | 1227/1868 [63:45:07<33:26:20, 187.80s/it]{'loss': 1.5444, 'grad_norm': 0.0, 'learning_rate': 6.8843683083511785e-06, 'epoch': 0.66}
? Step 1227  Epoch: 0.66  Loss: 1.4329
                                                           66%|██████▌   | 1227/1868 [63:45:07<33:26:20, 187.80s/it] 66%|██████▌   | 1228/1868 [63:48:15<33:23:10, 187.80s/it]{'loss': 1.4329, 'grad_norm': 0.0, 'learning_rate': 6.873661670235546e-06, 'epoch': 0.66}
? Step 1228  Epoch: 0.66  Loss: 1.4130
                                                           66%|██████▌   | 1228/1868 [63:48:15<33:23:10, 187.80s/it] 66%|██████▌   | 1229/1868 [63:51:23<33:20:01, 187.80s/it]{'loss': 1.413, 'grad_norm': 0.0, 'learning_rate': 6.862955032119914e-06, 'epoch': 0.66}
? Step 1229  Epoch: 0.66  Loss: 1.4507
                                                           66%|██████▌   | 1229/1868 [63:51:23<33:20:01, 187.80s/it] 66%|██████▌   | 1230/1868 [63:54:31<33:16:16, 187.74s/it]{'loss': 1.4507, 'grad_norm': 0.0, 'learning_rate': 6.852248394004283e-06, 'epoch': 0.66}
? Step 1230  Epoch: 0.66  Loss: 1.6391
                                                           66%|██████▌   | 1230/1868 [63:54:31<33:16:16, 187.74s/it] 66%|██████▌   | 1231/1868 [63:57:38<33:13:12, 187.74s/it]{'loss': 1.6391, 'grad_norm': 0.0, 'learning_rate': 6.841541755888651e-06, 'epoch': 0.66}
? Step 1231  Epoch: 0.66  Loss: 1.3781
                                                           66%|██████▌   | 1231/1868 [63:57:38<33:13:12, 187.74s/it] 66%|██████▌   | 1232/1868 [64:00:46<33:09:12, 187.66s/it]{'loss': 1.3781, 'grad_norm': 0.0, 'learning_rate': 6.83083511777302e-06, 'epoch': 0.66}
? Step 1232  Epoch: 0.66  Loss: 1.4945
                                                           66%|██████▌   | 1232/1868 [64:00:46<33:09:12, 187.66s/it] 66%|██████▌   | 1233/1868 [64:03:54<33:06:12, 187.67s/it]{'loss': 1.4945, 'grad_norm': 0.0, 'learning_rate': 6.820128479657389e-06, 'epoch': 0.66}
? Step 1233  Epoch: 0.66  Loss: 1.7455
                                                           66%|██████▌   | 1233/1868 [64:03:54<33:06:12, 187.67s/it] 66%|██████▌   | 1234/1868 [64:07:01<33:02:37, 187.63s/it]{'loss': 1.7455, 'grad_norm': 0.0, 'learning_rate': 6.809421841541757e-06, 'epoch': 0.66}
? Step 1234  Epoch: 0.66  Loss: 1.8004
                                                           66%|██████▌   | 1234/1868 [64:07:01<33:02:37, 187.63s/it] 66%|██████▌   | 1235/1868 [64:10:09<32:59:01, 187.58s/it]{'loss': 1.8004, 'grad_norm': 0.0, 'learning_rate': 6.798715203426125e-06, 'epoch': 0.66}
? Step 1235  Epoch: 0.66  Loss: 1.7252
                                                           66%|██████▌   | 1235/1868 [64:10:09<32:59:01, 187.58s/it] 66%|██████▌   | 1236/1868 [64:13:16<32:56:19, 187.63s/it]{'loss': 1.7252, 'grad_norm': 0.0, 'learning_rate': 6.788008565310493e-06, 'epoch': 0.66}
? Step 1236  Epoch: 0.66  Loss: 1.7310
                                                           66%|██████▌   | 1236/1868 [64:13:16<32:56:19, 187.63s/it] 66%|██████▌   | 1237/1868 [64:16:24<32:52:29, 187.56s/it]{'loss': 1.731, 'grad_norm': 0.0, 'learning_rate': 6.777301927194862e-06, 'epoch': 0.66}
? Step 1237  Epoch: 0.66  Loss: 1.9447
                                                           66%|██████▌   | 1237/1868 [64:16:24<32:52:29, 187.56s/it] 66%|██████▋   | 1238/1868 [64:19:32<32:50:28, 187.66s/it]{'loss': 1.9447, 'grad_norm': 0.0, 'learning_rate': 6.76659528907923e-06, 'epoch': 0.66}
? Step 1238  Epoch: 0.66  Loss: 1.5344
                                                           66%|██████▋   | 1238/1868 [64:19:32<32:50:28, 187.66s/it] 66%|██████▋   | 1239/1868 [64:22:39<32:46:59, 187.63s/it]{'loss': 1.5344, 'grad_norm': 0.0, 'learning_rate': 6.755888650963598e-06, 'epoch': 0.66}
? Step 1239  Epoch: 0.66  Loss: 1.5798
                                                           66%|██████▋   | 1239/1868 [64:22:39<32:46:59, 187.63s/it] 66%|██████▋   | 1240/1868 [64:25:47<32:43:40, 187.61s/it]{'loss': 1.5798, 'grad_norm': 0.0, 'learning_rate': 6.745182012847966e-06, 'epoch': 0.66}
? Step 1240  Epoch: 0.66  Loss: 1.6130
                                                           66%|██████▋   | 1240/1868 [64:25:47<32:43:40, 187.61s/it] 66%|██████▋   | 1241/1868 [64:28:54<32:39:24, 187.50s/it]{'loss': 1.613, 'grad_norm': 0.0, 'learning_rate': 6.734475374732334e-06, 'epoch': 0.66}
? Step 1241  Epoch: 0.66  Loss: 1.7086
                                                           66%|██████▋   | 1241/1868 [64:28:54<32:39:24, 187.50s/it] 66%|██████▋   | 1242/1868 [64:32:01<32:36:05, 187.49s/it]{'loss': 1.7086, 'grad_norm': 0.0, 'learning_rate': 6.723768736616703e-06, 'epoch': 0.66}
? Step 1242  Epoch: 0.66  Loss: 1.4386
                                                           66%|██████▋   | 1242/1868 [64:32:01<32:36:05, 187.49s/it] 67%|██████▋   | 1243/1868 [64:35:09<32:32:41, 187.46s/it]{'loss': 1.4386, 'grad_norm': 0.0, 'learning_rate': 6.7130620985010705e-06, 'epoch': 0.66}
? Step 1243  Epoch: 0.67  Loss: 1.6573
                                                           67%|██████▋   | 1243/1868 [64:35:09<32:32:41, 187.46s/it] 67%|██████▋   | 1244/1868 [64:38:16<32:29:09, 187.42s/it]{'loss': 1.6573, 'grad_norm': 0.0, 'learning_rate': 6.702355460385439e-06, 'epoch': 0.67}
? Step 1244  Epoch: 0.67  Loss: 1.8877
                                                           67%|██████▋   | 1244/1868 [64:38:16<32:29:09, 187.42s/it] 67%|██████▋   | 1245/1868 [64:41:24<32:26:07, 187.43s/it]{'loss': 1.8877, 'grad_norm': 0.0, 'learning_rate': 6.691648822269807e-06, 'epoch': 0.67}
? Step 1245  Epoch: 0.67  Loss: 1.9188
                                                           67%|██████▋   | 1245/1868 [64:41:24<32:26:07, 187.43s/it] 67%|██████▋   | 1246/1868 [64:44:31<32:23:35, 187.48s/it]{'loss': 1.9188, 'grad_norm': 0.0, 'learning_rate': 6.6809421841541765e-06, 'epoch': 0.67}
? Step 1246  Epoch: 0.67  Loss: 1.6569
                                                           67%|██████▋   | 1246/1868 [64:44:31<32:23:35, 187.48s/it] 67%|██████▋   | 1247/1868 [64:47:39<32:20:42, 187.51s/it]{'loss': 1.6569, 'grad_norm': 0.0, 'learning_rate': 6.670235546038545e-06, 'epoch': 0.67}
? Step 1247  Epoch: 0.67  Loss: 1.7672
                                                           67%|██████▋   | 1247/1868 [64:47:39<32:20:42, 187.51s/it] 67%|██████▋   | 1248/1868 [64:50:46<32:17:59, 187.55s/it]{'loss': 1.7672, 'grad_norm': 0.0, 'learning_rate': 6.659528907922913e-06, 'epoch': 0.67}
? Step 1248  Epoch: 0.67  Loss: 1.5018
                                                           67%|██████▋   | 1248/1868 [64:50:46<32:17:59, 187.55s/it] 67%|██████▋   | 1249/1868 [64:53:54<32:15:33, 187.61s/it]{'loss': 1.5018, 'grad_norm': 0.0, 'learning_rate': 6.648822269807282e-06, 'epoch': 0.67}
? Step 1249  Epoch: 0.67  Loss: 1.5781
                                                           67%|██████▋   | 1249/1868 [64:53:54<32:15:33, 187.61s/it] 67%|██████▋   | 1250/1868 [64:57:02<32:12:32, 187.63s/it]{'loss': 1.5781, 'grad_norm': 0.0, 'learning_rate': 6.6381156316916495e-06, 'epoch': 0.67}
? Step 1250  Epoch: 0.67  Loss: 1.5622
                                                           67%|██████▋   | 1250/1868 [64:57:02<32:12:32, 187.63s/it] 67%|██████▋   | 1251/1868 [65:00:09<32:08:54, 187.58s/it]{'loss': 1.5622, 'grad_norm': 0.0, 'learning_rate': 6.627408993576018e-06, 'epoch': 0.67}
? Step 1251  Epoch: 0.67  Loss: 1.8936
                                                           67%|██████▋   | 1251/1868 [65:00:09<32:08:54, 187.58s/it] 67%|██████▋   | 1252/1868 [65:03:17<32:05:59, 187.60s/it]{'loss': 1.8936, 'grad_norm': 0.0, 'learning_rate': 6.616702355460386e-06, 'epoch': 0.67}
? Step 1252  Epoch: 0.67  Loss: 1.5019
                                                           67%|██████▋   | 1252/1868 [65:03:17<32:05:59, 187.60s/it] 67%|██████▋   | 1253/1868 [65:06:24<32:01:30, 187.46s/it]{'loss': 1.5019, 'grad_norm': 0.0, 'learning_rate': 6.605995717344754e-06, 'epoch': 0.67}
? Step 1253  Epoch: 0.67  Loss: 2.2618
                                                           67%|██████▋   | 1253/1868 [65:06:24<32:01:30, 187.46s/it] 67%|██████▋   | 1254/1868 [65:09:32<31:59:02, 187.53s/it]{'loss': 2.2618, 'grad_norm': 0.0, 'learning_rate': 6.5952890792291225e-06, 'epoch': 0.67}
? Step 1254  Epoch: 0.67  Loss: 1.2795
                                                           67%|██████▋   | 1254/1868 [65:09:32<31:59:02, 187.53s/it] 67%|██████▋   | 1255/1868 [65:12:39<31:56:23, 187.57s/it]{'loss': 1.2795, 'grad_norm': 0.0, 'learning_rate': 6.58458244111349e-06, 'epoch': 0.67}
? Step 1255  Epoch: 0.67  Loss: 1.4981
                                                           67%|██████▋   | 1255/1868 [65:12:39<31:56:23, 187.57s/it] 67%|██████▋   | 1256/1868 [65:15:47<31:53:06, 187.56s/it]{'loss': 1.4981, 'grad_norm': 0.0, 'learning_rate': 6.573875802997859e-06, 'epoch': 0.67}
? Step 1256  Epoch: 0.67  Loss: 1.6394
                                                           67%|██████▋   | 1256/1868 [65:15:47<31:53:06, 187.56s/it] 67%|██████▋   | 1257/1868 [65:18:54<31:49:33, 187.52s/it]{'loss': 1.6394, 'grad_norm': 0.0, 'learning_rate': 6.563169164882227e-06, 'epoch': 0.67}
? Step 1257  Epoch: 0.67  Loss: 1.2696
                                                           67%|██████▋   | 1257/1868 [65:18:54<31:49:33, 187.52s/it] 67%|██████▋   | 1258/1868 [65:22:02<31:47:13, 187.60s/it]{'loss': 1.2696, 'grad_norm': 0.0, 'learning_rate': 6.5524625267665955e-06, 'epoch': 0.67}
? Step 1258  Epoch: 0.67  Loss: 1.4127
                                                           67%|██████▋   | 1258/1868 [65:22:02<31:47:13, 187.60s/it] 67%|██████▋   | 1259/1868 [65:25:10<31:43:23, 187.53s/it]{'loss': 1.4127, 'grad_norm': 0.0, 'learning_rate': 6.541755888650965e-06, 'epoch': 0.67}
? Step 1259  Epoch: 0.67  Loss: 1.4057
                                                           67%|██████▋   | 1259/1868 [65:25:10<31:43:23, 187.53s/it] 67%|██████▋   | 1260/1868 [65:28:17<31:40:21, 187.54s/it]{'loss': 1.4057, 'grad_norm': 0.0, 'learning_rate': 6.531049250535333e-06, 'epoch': 0.67}
? Step 1260  Epoch: 0.67  Loss: 1.6625
                                                           67%|██████▋   | 1260/1868 [65:28:17<31:40:21, 187.54s/it] 68%|██████▊   | 1261/1868 [65:31:25<31:37:05, 187.52s/it]{'loss': 1.6625, 'grad_norm': 0.0, 'learning_rate': 6.5203426124197015e-06, 'epoch': 0.67}
? Step 1261  Epoch: 0.68  Loss: 1.4381
                                                           68%|██████▊   | 1261/1868 [65:31:25<31:37:05, 187.52s/it] 68%|██████▊   | 1262/1868 [65:34:32<31:34:31, 187.58s/it]{'loss': 1.4381, 'grad_norm': 0.0, 'learning_rate': 6.509635974304069e-06, 'epoch': 0.68}
? Step 1262  Epoch: 0.68  Loss: 1.3270
                                                           68%|██████▊   | 1262/1868 [65:34:32<31:34:31, 187.58s/it] 68%|██████▊   | 1263/1868 [65:37:39<31:29:56, 187.43s/it]{'loss': 1.327, 'grad_norm': 0.0, 'learning_rate': 6.498929336188438e-06, 'epoch': 0.68}
? Step 1263  Epoch: 0.68  Loss: 1.7191
                                                           68%|██████▊   | 1263/1868 [65:37:39<31:29:56, 187.43s/it] 68%|██████▊   | 1264/1868 [65:40:47<31:25:59, 187.35s/it]{'loss': 1.7191, 'grad_norm': 0.0, 'learning_rate': 6.488222698072806e-06, 'epoch': 0.68}
? Step 1264  Epoch: 0.68  Loss: 1.6897
                                                           68%|██████▊   | 1264/1868 [65:40:47<31:25:59, 187.35s/it] 68%|██████▊   | 1265/1868 [65:43:54<31:22:26, 187.31s/it]{'loss': 1.6897, 'grad_norm': 0.0, 'learning_rate': 6.477516059957174e-06, 'epoch': 0.68}
? Step 1265  Epoch: 0.68  Loss: 1.4543
                                                           68%|██████▊   | 1265/1868 [65:43:54<31:22:26, 187.31s/it] 68%|██████▊   | 1266/1868 [65:47:01<31:20:23, 187.41s/it]{'loss': 1.4543, 'grad_norm': 0.0, 'learning_rate': 6.466809421841542e-06, 'epoch': 0.68}
? Step 1266  Epoch: 0.68  Loss: 1.3968
                                                           68%|██████▊   | 1266/1868 [65:47:01<31:20:23, 187.41s/it] 68%|██████▊   | 1267/1868 [65:50:09<31:17:11, 187.41s/it]{'loss': 1.3968, 'grad_norm': 0.0, 'learning_rate': 6.45610278372591e-06, 'epoch': 0.68}
? Step 1267  Epoch: 0.68  Loss: 1.6226
                                                           68%|██████▊   | 1267/1868 [65:50:09<31:17:11, 187.41s/it] 68%|██████▊   | 1268/1868 [65:53:16<31:14:16, 187.43s/it]{'loss': 1.6226, 'grad_norm': 0.0, 'learning_rate': 6.445396145610279e-06, 'epoch': 0.68}
? Step 1268  Epoch: 0.68  Loss: 1.4893
                                                           68%|██████▊   | 1268/1868 [65:53:16<31:14:16, 187.43s/it] 68%|██████▊   | 1269/1868 [65:56:24<31:11:32, 187.47s/it]{'loss': 1.4893, 'grad_norm': 0.0, 'learning_rate': 6.434689507494647e-06, 'epoch': 0.68}
? Step 1269  Epoch: 0.68  Loss: 1.5230
                                                           68%|██████▊   | 1269/1868 [65:56:24<31:11:32, 187.47s/it] 68%|██████▊   | 1270/1868 [65:59:31<31:08:46, 187.50s/it]{'loss': 1.523, 'grad_norm': 0.0, 'learning_rate': 6.423982869379015e-06, 'epoch': 0.68}
? Step 1270  Epoch: 0.68  Loss: 1.5815
                                                           68%|██████▊   | 1270/1868 [65:59:31<31:08:46, 187.50s/it] 68%|██████▊   | 1271/1868 [66:02:39<31:05:33, 187.49s/it]{'loss': 1.5815, 'grad_norm': 0.0, 'learning_rate': 6.413276231263383e-06, 'epoch': 0.68}
? Step 1271  Epoch: 0.68  Loss: 1.4358
                                                           68%|██████▊   | 1271/1868 [66:02:39<31:05:33, 187.49s/it] 68%|██████▊   | 1272/1868 [66:05:46<31:02:10, 187.47s/it]{'loss': 1.4358, 'grad_norm': 0.0, 'learning_rate': 6.402569593147752e-06, 'epoch': 0.68}
? Step 1272  Epoch: 0.68  Loss: 1.5690
                                                           68%|██████▊   | 1272/1868 [66:05:46<31:02:10, 187.47s/it] 68%|██████▊   | 1273/1868 [66:08:54<30:58:39, 187.43s/it]{'loss': 1.569, 'grad_norm': 0.0, 'learning_rate': 6.391862955032121e-06, 'epoch': 0.68}
? Step 1273  Epoch: 0.68  Loss: 1.3410
                                                           68%|██████▊   | 1273/1868 [66:08:54<30:58:39, 187.43s/it] 68%|██████▊   | 1274/1868 [66:12:01<30:56:05, 187.48s/it]{'loss': 1.341, 'grad_norm': 0.0, 'learning_rate': 6.381156316916489e-06, 'epoch': 0.68}
? Step 1274  Epoch: 0.68  Loss: 1.6767
                                                           68%|██████▊   | 1274/1868 [66:12:01<30:56:05, 187.48s/it] 68%|██████▊   | 1275/1868 [66:15:09<30:52:52, 187.47s/it]{'loss': 1.6767, 'grad_norm': 0.0, 'learning_rate': 6.370449678800858e-06, 'epoch': 0.68}
? Step 1275  Epoch: 0.68  Loss: 1.4429
                                                           68%|██████▊   | 1275/1868 [66:15:09<30:52:52, 187.47s/it] 68%|██████▊   | 1276/1868 [66:18:16<30:49:06, 187.41s/it]{'loss': 1.4429, 'grad_norm': 0.0, 'learning_rate': 6.359743040685226e-06, 'epoch': 0.68}
? Step 1276  Epoch: 0.68  Loss: 1.5079
                                                           68%|██████▊   | 1276/1868 [66:18:16<30:49:06, 187.41s/it] 68%|██████▊   | 1277/1868 [66:21:23<30:45:39, 187.38s/it]{'loss': 1.5079, 'grad_norm': 0.0, 'learning_rate': 6.3490364025695935e-06, 'epoch': 0.68}
? Step 1277  Epoch: 0.68  Loss: 1.8366
                                                           68%|██████▊   | 1277/1868 [66:21:23<30:45:39, 187.38s/it] 68%|██████▊   | 1278/1868 [66:24:30<30:42:00, 187.32s/it]{'loss': 1.8366, 'grad_norm': 0.0, 'learning_rate': 6.338329764453962e-06, 'epoch': 0.68}
? Step 1278  Epoch: 0.68  Loss: 1.5944
                                                           68%|██████▊   | 1278/1868 [66:24:30<30:42:00, 187.32s/it] 68%|██████▊   | 1279/1868 [66:27:38<30:39:05, 187.34s/it]{'loss': 1.5944, 'grad_norm': 0.0, 'learning_rate': 6.32762312633833e-06, 'epoch': 0.68}
? Step 1279  Epoch: 0.68  Loss: 1.5738
                                                           68%|██████▊   | 1279/1868 [66:27:38<30:39:05, 187.34s/it] 69%|██████▊   | 1280/1868 [66:30:45<30:35:38, 187.31s/it]{'loss': 1.5738, 'grad_norm': 0.0, 'learning_rate': 6.316916488222699e-06, 'epoch': 0.68}
? Step 1280  Epoch: 0.69  Loss: 1.5884
                                                           69%|██████▊   | 1280/1868 [66:30:45<30:35:38, 187.31s/it] 69%|██████▊   | 1281/1868 [66:33:53<30:33:47, 187.44s/it]{'loss': 1.5884, 'grad_norm': 0.0, 'learning_rate': 6.3062098501070665e-06, 'epoch': 0.69}
? Step 1281  Epoch: 0.69  Loss: 1.5879
                                                           69%|██████▊   | 1281/1868 [66:33:53<30:33:47, 187.44s/it] 69%|██████▊   | 1282/1868 [66:37:01<30:31:22, 187.51s/it]{'loss': 1.5879, 'grad_norm': 0.0, 'learning_rate': 6.295503211991435e-06, 'epoch': 0.69}
? Step 1282  Epoch: 0.69  Loss: 1.5551
                                                           69%|██████▊   | 1282/1868 [66:37:01<30:31:22, 187.51s/it] 69%|██████▊   | 1283/1868 [66:40:08<30:27:52, 187.48s/it]{'loss': 1.5551, 'grad_norm': 0.0, 'learning_rate': 6.284796573875803e-06, 'epoch': 0.69}
? Step 1283  Epoch: 0.69  Loss: 1.4008
                                                           69%|██████▊   | 1283/1868 [66:40:08<30:27:52, 187.48s/it] 69%|██████▊   | 1284/1868 [66:43:15<30:24:40, 187.47s/it]{'loss': 1.4008, 'grad_norm': 0.0, 'learning_rate': 6.274089935760172e-06, 'epoch': 0.69}
? Step 1284  Epoch: 0.69  Loss: 1.3302
                                                           69%|██████▊   | 1284/1868 [66:43:15<30:24:40, 187.47s/it] 69%|██████▉   | 1285/1868 [66:46:23<30:21:02, 187.41s/it]{'loss': 1.3302, 'grad_norm': 0.0, 'learning_rate': 6.2633832976445395e-06, 'epoch': 0.69}
? Step 1285  Epoch: 0.69  Loss: 1.8770
                                                           69%|██████▉   | 1285/1868 [66:46:23<30:21:02, 187.41s/it] 69%|██████▉   | 1286/1868 [66:49:30<30:18:22, 187.46s/it]{'loss': 1.877, 'grad_norm': 0.0, 'learning_rate': 6.252676659528908e-06, 'epoch': 0.69}
? Step 1286  Epoch: 0.69  Loss: 1.4675
                                                           69%|██████▉   | 1286/1868 [66:49:30<30:18:22, 187.46s/it] 69%|██████▉   | 1287/1868 [66:52:37<30:14:15, 187.36s/it]{'loss': 1.4675, 'grad_norm': 0.0, 'learning_rate': 6.241970021413278e-06, 'epoch': 0.69}
? Step 1287  Epoch: 0.69  Loss: 1.6003
                                                           69%|██████▉   | 1287/1868 [66:52:37<30:14:15, 187.36s/it] 69%|██████▉   | 1288/1868 [66:55:45<30:11:53, 187.44s/it]{'loss': 1.6003, 'grad_norm': 0.0, 'learning_rate': 6.2312633832976455e-06, 'epoch': 0.69}
? Step 1288  Epoch: 0.69  Loss: 1.5215
                                                           69%|██████▉   | 1288/1868 [66:55:45<30:11:53, 187.44s/it] 69%|██████▉   | 1289/1868 [66:58:52<30:08:45, 187.44s/it]{'loss': 1.5215, 'grad_norm': 0.0, 'learning_rate': 6.220556745182013e-06, 'epoch': 0.69}
? Step 1289  Epoch: 0.69  Loss: 1.6533
                                                           69%|██████▉   | 1289/1868 [66:58:52<30:08:45, 187.44s/it] 69%|██████▉   | 1290/1868 [67:02:00<30:05:40, 187.44s/it]{'loss': 1.6533, 'grad_norm': 0.0, 'learning_rate': 6.209850107066382e-06, 'epoch': 0.69}
? Step 1290  Epoch: 0.69  Loss: 1.4464
                                                           69%|██████▉   | 1290/1868 [67:02:00<30:05:40, 187.44s/it] 69%|██████▉   | 1291/1868 [67:05:07<30:01:40, 187.35s/it]{'loss': 1.4464, 'grad_norm': 0.0, 'learning_rate': 6.19914346895075e-06, 'epoch': 0.69}
? Step 1291  Epoch: 0.69  Loss: 1.6322
                                                           69%|██████▉   | 1291/1868 [67:05:07<30:01:40, 187.35s/it] 69%|██████▉   | 1292/1868 [67:08:14<29:58:51, 187.38s/it]{'loss': 1.6322, 'grad_norm': 0.0, 'learning_rate': 6.1884368308351185e-06, 'epoch': 0.69}
? Step 1292  Epoch: 0.69  Loss: 1.4745
                                                           69%|██████▉   | 1292/1868 [67:08:14<29:58:51, 187.38s/it] 69%|██████▉   | 1293/1868 [67:11:22<29:55:55, 187.40s/it]{'loss': 1.4745, 'grad_norm': 0.0, 'learning_rate': 6.177730192719486e-06, 'epoch': 0.69}
? Step 1293  Epoch: 0.69  Loss: 1.5083
                                                           69%|██████▉   | 1293/1868 [67:11:22<29:55:55, 187.40s/it] 69%|██████▉   | 1294/1868 [67:14:29<29:53:08, 187.44s/it]{'loss': 1.5083, 'grad_norm': 0.0, 'learning_rate': 6.167023554603855e-06, 'epoch': 0.69}
? Step 1294  Epoch: 0.69  Loss: 1.5439
                                                           69%|██████▉   | 1294/1868 [67:14:29<29:53:08, 187.44s/it] 69%|██████▉   | 1295/1868 [67:17:37<29:49:09, 187.35s/it]{'loss': 1.5439, 'grad_norm': 0.0, 'learning_rate': 6.156316916488223e-06, 'epoch': 0.69}
? Step 1295  Epoch: 0.69  Loss: 1.7328
                                                           69%|██████▉   | 1295/1868 [67:17:37<29:49:09, 187.35s/it] 69%|██████▉   | 1296/1868 [67:20:44<29:46:21, 187.38s/it]{'loss': 1.7328, 'grad_norm': 0.0, 'learning_rate': 6.1456102783725915e-06, 'epoch': 0.69}
? Step 1296  Epoch: 0.69  Loss: 1.8547
                                                           69%|██████▉   | 1296/1868 [67:20:44<29:46:21, 187.38s/it] 69%|██████▉   | 1297/1868 [67:23:52<29:43:57, 187.46s/it]{'loss': 1.8547, 'grad_norm': 0.0, 'learning_rate': 6.134903640256959e-06, 'epoch': 0.69}
? Step 1297  Epoch: 0.69  Loss: 1.5382
                                                           69%|██████▉   | 1297/1868 [67:23:52<29:43:57, 187.46s/it] 69%|██████▉   | 1298/1868 [67:26:59<29:39:17, 187.29s/it]{'loss': 1.5382, 'grad_norm': 0.0, 'learning_rate': 6.124197002141328e-06, 'epoch': 0.69}
? Step 1298  Epoch: 0.69  Loss: 1.8632
                                                           69%|██████▉   | 1298/1868 [67:26:59<29:39:17, 187.29s/it] 70%|██████▉   | 1299/1868 [67:30:06<29:36:40, 187.35s/it]{'loss': 1.8632, 'grad_norm': 0.0, 'learning_rate': 6.113490364025696e-06, 'epoch': 0.69}
? Step 1299  Epoch: 0.70  Loss: 1.5139
                                                           70%|██████▉   | 1299/1868 [67:30:06<29:36:40, 187.35s/it] 70%|██████▉   | 1300/1868 [67:33:13<29:33:41, 187.36s/it]{'loss': 1.5139, 'grad_norm': 0.0, 'learning_rate': 6.102783725910065e-06, 'epoch': 0.7}
? Step 1300  Epoch: 0.70  Loss: 1.5916
                                                           70%|██████▉   | 1300/1868 [67:33:13<29:33:41, 187.36s/it]/home/dev25-01/mistral-env/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 70%|██████▉   | 1301/1868 [67:36:21<29:31:28, 187.46s/it]{'loss': 1.5916, 'grad_norm': 0.0, 'learning_rate': 6.092077087794433e-06, 'epoch': 0.7}
? Step 1301  Epoch: 0.70  Loss: 1.5749
                                                           70%|██████▉   | 1301/1868 [67:36:21<29:31:28, 187.46s/it] 70%|██████▉   | 1302/1868 [67:39:28<29:27:17, 187.35s/it]{'loss': 1.5749, 'grad_norm': 0.0, 'learning_rate': 6.081370449678802e-06, 'epoch': 0.7}
? Step 1302  Epoch: 0.70  Loss: 1.5520
                                                           70%|██████▉   | 1302/1868 [67:39:28<29:27:17, 187.35s/it] 70%|██████▉   | 1303/1868 [67:42:35<29:23:56, 187.32s/it]{'loss': 1.552, 'grad_norm': 0.0, 'learning_rate': 6.07066381156317e-06, 'epoch': 0.7}
? Step 1303  Epoch: 0.70  Loss: 1.5062
                                                           70%|██████▉   | 1303/1868 [67:42:35<29:23:56, 187.32s/it] 70%|██████▉   | 1304/1868 [67:45:42<29:19:56, 187.23s/it]{'loss': 1.5062, 'grad_norm': 0.0, 'learning_rate': 6.059957173447538e-06, 'epoch': 0.7}
? Step 1304  Epoch: 0.70  Loss: 1.7575
                                                           70%|██████▉   | 1304/1868 [67:45:42<29:19:56, 187.23s/it] 70%|██████▉   | 1305/1868 [67:48:50<29:17:12, 187.27s/it]{'loss': 1.7575, 'grad_norm': 0.0, 'learning_rate': 6.049250535331906e-06, 'epoch': 0.7}
? Step 1305  Epoch: 0.70  Loss: 1.5751
                                                           70%|██████▉   | 1305/1868 [67:48:50<29:17:12, 187.27s/it] 70%|██████▉   | 1306/1868 [67:51:57<29:14:04, 187.27s/it]{'loss': 1.5751, 'grad_norm': 0.0, 'learning_rate': 6.038543897216275e-06, 'epoch': 0.7}
? Step 1306  Epoch: 0.70  Loss: 1.4905
                                                           70%|██████▉   | 1306/1868 [67:51:57<29:14:04, 187.27s/it] 70%|██████▉   | 1307/1868 [67:55:04<29:10:18, 187.20s/it]{'loss': 1.4905, 'grad_norm': 0.0, 'learning_rate': 6.027837259100643e-06, 'epoch': 0.7}
? Step 1307  Epoch: 0.70  Loss: 1.6561
                                                           70%|██████▉   | 1307/1868 [67:55:04<29:10:18, 187.20s/it] 70%|███████   | 1308/1868 [67:58:11<29:06:07, 187.08s/it]{'loss': 1.6561, 'grad_norm': 0.0, 'learning_rate': 6.017130620985011e-06, 'epoch': 0.7}
? Step 1308  Epoch: 0.70  Loss: 1.4798
                                                           70%|███████   | 1308/1868 [67:58:11<29:06:07, 187.08s/it] 70%|███████   | 1309/1868 [68:01:18<29:03:53, 187.18s/it]{'loss': 1.4798, 'grad_norm': 0.0, 'learning_rate': 6.006423982869379e-06, 'epoch': 0.7}
? Step 1309  Epoch: 0.70  Loss: 1.6022
                                                           70%|███████   | 1309/1868 [68:01:18<29:03:53, 187.18s/it] 70%|███████   | 1310/1868 [68:04:25<29:00:33, 187.16s/it]{'loss': 1.6022, 'grad_norm': 0.0, 'learning_rate': 5.995717344753748e-06, 'epoch': 0.7}
? Step 1310  Epoch: 0.70  Loss: 1.4262
                                                           70%|███████   | 1310/1868 [68:04:25<29:00:33, 187.16s/it] 70%|███████   | 1311/1868 [68:07:33<28:57:42, 187.19s/it]{'loss': 1.4262, 'grad_norm': 0.0, 'learning_rate': 5.985010706638116e-06, 'epoch': 0.7}
? Step 1311  Epoch: 0.70  Loss: 1.5894
                                                           70%|███████   | 1311/1868 [68:07:33<28:57:42, 187.19s/it] 70%|███████   | 1312/1868 [68:10:40<28:55:10, 187.25s/it]{'loss': 1.5894, 'grad_norm': 0.0, 'learning_rate': 5.974304068522484e-06, 'epoch': 0.7}
? Step 1312  Epoch: 0.70  Loss: 1.5660
                                                           70%|███████   | 1312/1868 [68:10:40<28:55:10, 187.25s/it] 70%|███████   | 1313/1868 [68:13:47<28:52:29, 187.30s/it]{'loss': 1.566, 'grad_norm': 0.0, 'learning_rate': 5.963597430406852e-06, 'epoch': 0.7}
? Step 1313  Epoch: 0.70  Loss: 1.5652
                                                           70%|███████   | 1313/1868 [68:13:47<28:52:29, 187.30s/it] 70%|███████   | 1314/1868 [68:16:55<28:49:46, 187.34s/it]{'loss': 1.5652, 'grad_norm': 0.0, 'learning_rate': 5.952890792291222e-06, 'epoch': 0.7}
? Step 1314  Epoch: 0.70  Loss: 1.5068
                                                           70%|███████   | 1314/1868 [68:16:55<28:49:46, 187.34s/it] 70%|███████   | 1315/1868 [68:20:02<28:46:30, 187.33s/it]{'loss': 1.5068, 'grad_norm': 0.0, 'learning_rate': 5.9421841541755895e-06, 'epoch': 0.7}
? Step 1315  Epoch: 0.70  Loss: 1.6266
                                                           70%|███████   | 1315/1868 [68:20:02<28:46:30, 187.33s/it] 70%|███████   | 1316/1868 [68:23:10<28:43:18, 187.32s/it]{'loss': 1.6266, 'grad_norm': 0.0, 'learning_rate': 5.931477516059958e-06, 'epoch': 0.7}
? Step 1316  Epoch: 0.70  Loss: 1.4906
                                                           70%|███████   | 1316/1868 [68:23:10<28:43:18, 187.32s/it] 71%|███████   | 1317/1868 [68:26:17<28:40:28, 187.35s/it]{'loss': 1.4906, 'grad_norm': 0.0, 'learning_rate': 5.920770877944326e-06, 'epoch': 0.7}
? Step 1317  Epoch: 0.71  Loss: 1.6570
                                                           71%|███████   | 1317/1868 [68:26:17<28:40:28, 187.35s/it] 71%|███████   | 1318/1868 [68:29:24<28:37:11, 187.33s/it]{'loss': 1.657, 'grad_norm': 0.0, 'learning_rate': 5.910064239828695e-06, 'epoch': 0.71}
? Step 1318  Epoch: 0.71  Loss: 1.5433
                                                           71%|███████   | 1318/1868 [68:29:24<28:37:11, 187.33s/it] 71%|███████   | 1319/1868 [68:32:31<28:32:44, 187.18s/it]{'loss': 1.5433, 'grad_norm': 0.0, 'learning_rate': 5.8993576017130625e-06, 'epoch': 0.71}
? Step 1319  Epoch: 0.71  Loss: 1.8118
                                                           71%|███████   | 1319/1868 [68:32:31<28:32:44, 187.18s/it] 71%|███████   | 1320/1868 [68:35:38<28:29:00, 187.12s/it]{'loss': 1.8118, 'grad_norm': 0.0, 'learning_rate': 5.888650963597431e-06, 'epoch': 0.71}
? Step 1320  Epoch: 0.71  Loss: 1.8221
                                                           71%|███████   | 1320/1868 [68:35:38<28:29:00, 187.12s/it] 71%|███████   | 1321/1868 [68:38:45<28:25:56, 187.12s/it]{'loss': 1.8221, 'grad_norm': 0.0, 'learning_rate': 5.877944325481799e-06, 'epoch': 0.71}
? Step 1321  Epoch: 0.71  Loss: 1.6629
                                                           71%|███████   | 1321/1868 [68:38:45<28:25:56, 187.12s/it] 71%|███████   | 1322/1868 [68:41:52<28:22:53, 187.13s/it]{'loss': 1.6629, 'grad_norm': 0.0, 'learning_rate': 5.867237687366168e-06, 'epoch': 0.71}
? Step 1322  Epoch: 0.71  Loss: 1.5735
                                                           71%|███████   | 1322/1868 [68:41:52<28:22:53, 187.13s/it] 71%|███████   | 1323/1868 [68:45:00<28:20:30, 187.21s/it]{'loss': 1.5735, 'grad_norm': 0.0, 'learning_rate': 5.8565310492505354e-06, 'epoch': 0.71}
? Step 1323  Epoch: 0.71  Loss: 1.4930
                                                           71%|███████   | 1323/1868 [68:45:00<28:20:30, 187.21s/it] 71%|███████   | 1324/1868 [68:48:07<28:16:15, 187.09s/it]{'loss': 1.493, 'grad_norm': 0.0, 'learning_rate': 5.845824411134904e-06, 'epoch': 0.71}
? Step 1324  Epoch: 0.71  Loss: 1.9054
                                                           71%|███████   | 1324/1868 [68:48:07<28:16:15, 187.09s/it] 71%|███████   | 1325/1868 [68:51:13<28:12:18, 187.00s/it]{'loss': 1.9054, 'grad_norm': 0.0, 'learning_rate': 5.835117773019272e-06, 'epoch': 0.71}
? Step 1325  Epoch: 0.71  Loss: 1.5618
                                                           71%|███████   | 1325/1868 [68:51:13<28:12:18, 187.00s/it] 71%|███████   | 1326/1868 [68:54:20<28:09:35, 187.04s/it]{'loss': 1.5618, 'grad_norm': 0.0, 'learning_rate': 5.824411134903641e-06, 'epoch': 0.71}
? Step 1326  Epoch: 0.71  Loss: 1.4862
                                                           71%|███████   | 1326/1868 [68:54:20<28:09:35, 187.04s/it] 71%|███████   | 1327/1868 [68:57:27<28:06:13, 187.01s/it]{'loss': 1.4862, 'grad_norm': 0.0, 'learning_rate': 5.8137044967880084e-06, 'epoch': 0.71}
? Step 1327  Epoch: 0.71  Loss: 1.6732
                                                           71%|███████   | 1327/1868 [68:57:27<28:06:13, 187.01s/it] 71%|███████   | 1328/1868 [69:00:34<28:03:04, 187.01s/it]{'loss': 1.6732, 'grad_norm': 0.0, 'learning_rate': 5.802997858672378e-06, 'epoch': 0.71}
? Step 1328  Epoch: 0.71  Loss: 1.5760
                                                           71%|███████   | 1328/1868 [69:00:34<28:03:04, 187.01s/it] 71%|███████   | 1329/1868 [69:03:41<27:59:33, 186.96s/it]{'loss': 1.576, 'grad_norm': 0.0, 'learning_rate': 5.792291220556746e-06, 'epoch': 0.71}
? Step 1329  Epoch: 0.71  Loss: 1.7592
                                                           71%|███████   | 1329/1868 [69:03:41<27:59:33, 186.96s/it] 71%|███████   | 1330/1868 [69:06:49<27:57:21, 187.07s/it]{'loss': 1.7592, 'grad_norm': 0.0, 'learning_rate': 5.7815845824411145e-06, 'epoch': 0.71}
? Step 1330  Epoch: 0.71  Loss: 1.4864
                                                           71%|███████   | 1330/1868 [69:06:49<27:57:21, 187.07s/it] 71%|███████▏  | 1331/1868 [69:09:56<27:53:57, 187.04s/it]{'loss': 1.4864, 'grad_norm': 0.0, 'learning_rate': 5.770877944325482e-06, 'epoch': 0.71}
? Step 1331  Epoch: 0.71  Loss: 1.4030
                                                           71%|███████▏  | 1331/1868 [69:09:56<27:53:57, 187.04s/it] 71%|███████▏  | 1332/1868 [69:13:03<27:50:55, 187.04s/it]{'loss': 1.403, 'grad_norm': 0.0, 'learning_rate': 5.760171306209851e-06, 'epoch': 0.71}
? Step 1332  Epoch: 0.71  Loss: 1.4198
                                                           71%|███████▏  | 1332/1868 [69:13:03<27:50:55, 187.04s/it] 71%|███████▏  | 1333/1868 [69:16:10<27:48:03, 187.07s/it]{'loss': 1.4198, 'grad_norm': 0.0, 'learning_rate': 5.749464668094219e-06, 'epoch': 0.71}
? Step 1333  Epoch: 0.71  Loss: 1.4436
                                                           71%|███████▏  | 1333/1868 [69:16:10<27:48:03, 187.07s/it] 71%|███████▏  | 1334/1868 [69:19:17<27:44:50, 187.06s/it]{'loss': 1.4436, 'grad_norm': 0.0, 'learning_rate': 5.7387580299785874e-06, 'epoch': 0.71}
? Step 1334  Epoch: 0.71  Loss: 1.5300
                                                           71%|███████▏  | 1334/1868 [69:19:17<27:44:50, 187.06s/it] 71%|███████▏  | 1335/1868 [69:22:24<27:41:02, 186.98s/it]{'loss': 1.53, 'grad_norm': 0.0, 'learning_rate': 5.728051391862955e-06, 'epoch': 0.71}
? Step 1335  Epoch: 0.71  Loss: 1.5567
                                                           71%|███████▏  | 1335/1868 [69:22:24<27:41:02, 186.98s/it] 72%|███████▏  | 1336/1868 [69:25:30<27:37:28, 186.93s/it]{'loss': 1.5567, 'grad_norm': 0.0, 'learning_rate': 5.717344753747324e-06, 'epoch': 0.71}
? Step 1336  Epoch: 0.72  Loss: 1.8102
                                                           72%|███████▏  | 1336/1868 [69:25:30<27:37:28, 186.93s/it] 72%|███████▏  | 1337/1868 [69:28:38<27:35:10, 187.02s/it]{'loss': 1.8102, 'grad_norm': 0.0, 'learning_rate': 5.706638115631692e-06, 'epoch': 0.72}
? Step 1337  Epoch: 0.72  Loss: 1.5072
                                                           72%|███████▏  | 1337/1868 [69:28:38<27:35:10, 187.02s/it] 72%|███████▏  | 1338/1868 [69:31:45<27:32:08, 187.04s/it]{'loss': 1.5072, 'grad_norm': 0.0, 'learning_rate': 5.6959314775160604e-06, 'epoch': 0.72}
? Step 1338  Epoch: 0.72  Loss: 1.6399
                                                           72%|███████▏  | 1338/1868 [69:31:45<27:32:08, 187.04s/it] 72%|███████▏  | 1339/1868 [69:34:52<27:28:54, 187.02s/it]{'loss': 1.6399, 'grad_norm': 0.0, 'learning_rate': 5.685224839400428e-06, 'epoch': 0.72}
? Step 1339  Epoch: 0.72  Loss: 1.6410
                                                           72%|███████▏  | 1339/1868 [69:34:52<27:28:54, 187.02s/it] 72%|███████▏  | 1340/1868 [69:37:59<27:25:41, 187.01s/it]{'loss': 1.641, 'grad_norm': 0.0, 'learning_rate': 5.674518201284797e-06, 'epoch': 0.72}
? Step 1340  Epoch: 0.72  Loss: 1.6350
                                                           72%|███████▏  | 1340/1868 [69:37:59<27:25:41, 187.01s/it] 72%|███████▏  | 1341/1868 [69:41:06<27:23:28, 187.11s/it]{'loss': 1.635, 'grad_norm': 0.0, 'learning_rate': 5.663811563169165e-06, 'epoch': 0.72}
? Step 1341  Epoch: 0.72  Loss: 1.6439
                                                           72%|███████▏  | 1341/1868 [69:41:06<27:23:28, 187.11s/it] 72%|███████▏  | 1342/1868 [69:44:13<27:20:07, 187.09s/it]{'loss': 1.6439, 'grad_norm': 0.0, 'learning_rate': 5.653104925053534e-06, 'epoch': 0.72}
? Step 1342  Epoch: 0.72  Loss: 1.6882
                                                           72%|███████▏  | 1342/1868 [69:44:13<27:20:07, 187.09s/it] 72%|███████▏  | 1343/1868 [69:47:20<27:16:59, 187.08s/it]{'loss': 1.6882, 'grad_norm': 0.0, 'learning_rate': 5.642398286937902e-06, 'epoch': 0.72}
? Step 1343  Epoch: 0.72  Loss: 1.3428
                                                           72%|███████▏  | 1343/1868 [69:47:20<27:16:59, 187.08s/it] 72%|███████▏  | 1344/1868 [69:50:27<27:13:16, 187.02s/it]{'loss': 1.3428, 'grad_norm': 0.0, 'learning_rate': 5.631691648822271e-06, 'epoch': 0.72}
? Step 1344  Epoch: 0.72  Loss: 1.5744
                                                           72%|███████▏  | 1344/1868 [69:50:27<27:13:16, 187.02s/it] 72%|███████▏  | 1345/1868 [69:53:34<27:09:55, 186.99s/it]{'loss': 1.5744, 'grad_norm': 0.0, 'learning_rate': 5.620985010706639e-06, 'epoch': 0.72}
? Step 1345  Epoch: 0.72  Loss: 1.6107
                                                           72%|███████▏  | 1345/1868 [69:53:34<27:09:55, 186.99s/it] 72%|███████▏  | 1346/1868 [69:56:41<27:06:22, 186.94s/it]{'loss': 1.6107, 'grad_norm': 0.0, 'learning_rate': 5.610278372591007e-06, 'epoch': 0.72}
? Step 1346  Epoch: 0.72  Loss: 1.5425
                                                           72%|███████▏  | 1346/1868 [69:56:41<27:06:22, 186.94s/it] 72%|███████▏  | 1347/1868 [69:59:48<27:03:24, 186.96s/it]{'loss': 1.5425, 'grad_norm': 0.0, 'learning_rate': 5.599571734475375e-06, 'epoch': 0.72}
? Step 1347  Epoch: 0.72  Loss: 1.2892
                                                           72%|███████▏  | 1347/1868 [69:59:48<27:03:24, 186.96s/it] 72%|███████▏  | 1348/1868 [70:02:55<26:59:56, 186.92s/it]{'loss': 1.2892, 'grad_norm': 0.0, 'learning_rate': 5.588865096359744e-06, 'epoch': 0.72}
? Step 1348  Epoch: 0.72  Loss: 1.5950
                                                           72%|███████▏  | 1348/1868 [70:02:55<26:59:56, 186.92s/it] 72%|███████▏  | 1349/1868 [70:06:01<26:56:11, 186.84s/it]{'loss': 1.595, 'grad_norm': 0.0, 'learning_rate': 5.578158458244112e-06, 'epoch': 0.72}
? Step 1349  Epoch: 0.72  Loss: 1.7869
                                                           72%|███████▏  | 1349/1868 [70:06:01<26:56:11, 186.84s/it] 72%|███████▏  | 1350/1868 [70:09:08<26:53:00, 186.84s/it]{'loss': 1.7869, 'grad_norm': 0.0, 'learning_rate': 5.56745182012848e-06, 'epoch': 0.72}
? Step 1350  Epoch: 0.72  Loss: 1.8183
                                                           72%|███████▏  | 1350/1868 [70:09:08<26:53:00, 186.84s/it] 72%|███████▏  | 1351/1868 [70:12:15<26:50:17, 186.88s/it]{'loss': 1.8183, 'grad_norm': 0.0, 'learning_rate': 5.556745182012848e-06, 'epoch': 0.72}
? Step 1351  Epoch: 0.72  Loss: 1.5969
                                                           72%|███████▏  | 1351/1868 [70:12:15<26:50:17, 186.88s/it] 72%|███████▏  | 1352/1868 [70:15:22<26:47:16, 186.89s/it]{'loss': 1.5969, 'grad_norm': 0.0, 'learning_rate': 5.546038543897217e-06, 'epoch': 0.72}
? Step 1352  Epoch: 0.72  Loss: 1.7971
                                                           72%|███████▏  | 1352/1868 [70:15:22<26:47:16, 186.89s/it] 72%|███████▏  | 1353/1868 [70:18:29<26:43:41, 186.84s/it]{'loss': 1.7971, 'grad_norm': 0.0, 'learning_rate': 5.535331905781585e-06, 'epoch': 0.72}
? Step 1353  Epoch: 0.72  Loss: 1.4276
                                                           72%|███████▏  | 1353/1868 [70:18:29<26:43:41, 186.84s/it] 72%|███████▏  | 1354/1868 [70:21:35<26:39:39, 186.73s/it]{'loss': 1.4276, 'grad_norm': 0.0, 'learning_rate': 5.524625267665953e-06, 'epoch': 0.72}
? Step 1354  Epoch: 0.72  Loss: 1.8487
                                                           72%|███████▏  | 1354/1868 [70:21:35<26:39:39, 186.73s/it] 73%|███████▎  | 1355/1868 [70:24:42<26:36:44, 186.75s/it]{'loss': 1.8487, 'grad_norm': 0.0, 'learning_rate': 5.513918629550322e-06, 'epoch': 0.72}
? Step 1355  Epoch: 0.73  Loss: 1.6235
                                                           73%|███████▎  | 1355/1868 [70:24:42<26:36:44, 186.75s/it] 73%|███████▎  | 1356/1868 [70:27:49<26:34:35, 186.87s/it]{'loss': 1.6235, 'grad_norm': 0.0, 'learning_rate': 5.503211991434691e-06, 'epoch': 0.73}
? Step 1356  Epoch: 0.73  Loss: 1.4698
                                                           73%|███████▎  | 1356/1868 [70:27:49<26:34:35, 186.87s/it] 73%|███████▎  | 1357/1868 [70:30:56<26:31:32, 186.87s/it]{'loss': 1.4698, 'grad_norm': 0.0, 'learning_rate': 5.4925053533190584e-06, 'epoch': 0.73}
? Step 1357  Epoch: 0.73  Loss: 1.6894
                                                           73%|███████▎  | 1357/1868 [70:30:56<26:31:32, 186.87s/it] 73%|███████▎  | 1358/1868 [70:34:03<26:28:42, 186.91s/it]{'loss': 1.6894, 'grad_norm': 0.0, 'learning_rate': 5.481798715203427e-06, 'epoch': 0.73}
? Step 1358  Epoch: 0.73  Loss: 1.4858
                                                           73%|███████▎  | 1358/1868 [70:34:03<26:28:42, 186.91s/it] 73%|███████▎  | 1359/1868 [70:37:10<26:25:44, 186.92s/it]{'loss': 1.4858, 'grad_norm': 0.0, 'learning_rate': 5.471092077087795e-06, 'epoch': 0.73}
? Step 1359  Epoch: 0.73  Loss: 1.5988
                                                           73%|███████▎  | 1359/1868 [70:37:10<26:25:44, 186.92s/it] 73%|███████▎  | 1360/1868 [70:40:17<26:23:05, 186.98s/it]{'loss': 1.5988, 'grad_norm': 0.0, 'learning_rate': 5.460385438972164e-06, 'epoch': 0.73}
? Step 1360  Epoch: 0.73  Loss: 1.8028
                                                           73%|███████▎  | 1360/1868 [70:40:17<26:23:05, 186.98s/it] 73%|███████▎  | 1361/1868 [70:43:24<26:19:54, 186.97s/it]{'loss': 1.8028, 'grad_norm': 0.0, 'learning_rate': 5.4496788008565314e-06, 'epoch': 0.73}
? Step 1361  Epoch: 0.73  Loss: 1.9529
                                                           73%|███████▎  | 1361/1868 [70:43:24<26:19:54, 186.97s/it] 73%|███████▎  | 1362/1868 [70:46:31<26:17:24, 187.04s/it]{'loss': 1.9529, 'grad_norm': 0.0, 'learning_rate': 5.4389721627409e-06, 'epoch': 0.73}
? Step 1362  Epoch: 0.73  Loss: 1.5494
                                                           73%|███████▎  | 1362/1868 [70:46:31<26:17:24, 187.04s/it] 73%|███████▎  | 1363/1868 [70:49:38<26:14:26, 187.06s/it]{'loss': 1.5494, 'grad_norm': 0.0, 'learning_rate': 5.428265524625268e-06, 'epoch': 0.73}
? Step 1363  Epoch: 0.73  Loss: 1.6151
                                                           73%|███████▎  | 1363/1868 [70:49:38<26:14:26, 187.06s/it] 73%|███████▎  | 1364/1868 [70:52:45<26:10:51, 187.01s/it]{'loss': 1.6151, 'grad_norm': 0.0, 'learning_rate': 5.417558886509637e-06, 'epoch': 0.73}
? Step 1364  Epoch: 0.73  Loss: 1.5918
                                                           73%|███████▎  | 1364/1868 [70:52:45<26:10:51, 187.01s/it] 73%|███████▎  | 1365/1868 [70:55:52<26:07:03, 186.92s/it]{'loss': 1.5918, 'grad_norm': 0.0, 'learning_rate': 5.4068522483940044e-06, 'epoch': 0.73}
? Step 1365  Epoch: 0.73  Loss: 1.6900
                                                           73%|███████▎  | 1365/1868 [70:55:52<26:07:03, 186.92s/it] 73%|███████▎  | 1366/1868 [70:58:59<26:03:57, 186.93s/it]{'loss': 1.69, 'grad_norm': 0.0, 'learning_rate': 5.396145610278373e-06, 'epoch': 0.73}
? Step 1366  Epoch: 0.73  Loss: 1.8480
                                                           73%|███████▎  | 1366/1868 [70:58:59<26:03:57, 186.93s/it] 73%|███████▎  | 1367/1868 [71:02:06<26:00:44, 186.92s/it]{'loss': 1.848, 'grad_norm': 0.0, 'learning_rate': 5.385438972162741e-06, 'epoch': 0.73}
? Step 1367  Epoch: 0.73  Loss: 1.6873
                                                           73%|███████▎  | 1367/1868 [71:02:06<26:00:44, 186.92s/it] 73%|███████▎  | 1368/1868 [71:05:12<25:57:01, 186.84s/it]{'loss': 1.6873, 'grad_norm': 0.0, 'learning_rate': 5.374732334047109e-06, 'epoch': 0.73}
? Step 1368  Epoch: 0.73  Loss: 1.5937
                                                           73%|███████▎  | 1368/1868 [71:05:12<25:57:01, 186.84s/it] 73%|███████▎  | 1369/1868 [71:08:19<25:53:40, 186.82s/it]{'loss': 1.5937, 'grad_norm': 0.0, 'learning_rate': 5.364025695931478e-06, 'epoch': 0.73}
? Step 1369  Epoch: 0.73  Loss: 1.6428
                                                           73%|███████▎  | 1369/1868 [71:08:19<25:53:40, 186.82s/it] 73%|███████▎  | 1370/1868 [71:11:26<25:51:14, 186.90s/it]{'loss': 1.6428, 'grad_norm': 0.0, 'learning_rate': 5.353319057815847e-06, 'epoch': 0.73}
? Step 1370  Epoch: 0.73  Loss: 1.4438
                                                           73%|███████▎  | 1370/1868 [71:11:26<25:51:14, 186.90s/it] 73%|███████▎  | 1371/1868 [71:14:33<25:48:31, 186.94s/it]{'loss': 1.4438, 'grad_norm': 0.0, 'learning_rate': 5.342612419700215e-06, 'epoch': 0.73}
? Step 1371  Epoch: 0.73  Loss: 1.7782
                                                           73%|███████▎  | 1371/1868 [71:14:33<25:48:31, 186.94s/it] 73%|███████▎  | 1372/1868 [71:17:40<25:45:37, 186.97s/it]{'loss': 1.7782, 'grad_norm': 0.0, 'learning_rate': 5.3319057815845834e-06, 'epoch': 0.73}
? Step 1372  Epoch: 0.73  Loss: 1.6828
                                                           73%|███████▎  | 1372/1868 [71:17:40<25:45:37, 186.97s/it] 74%|███████▎  | 1373/1868 [71:20:47<25:41:19, 186.83s/it]{'loss': 1.6828, 'grad_norm': 0.0, 'learning_rate': 5.321199143468951e-06, 'epoch': 0.73}
? Step 1373  Epoch: 0.74  Loss: 2.0319
                                                           74%|███████▎  | 1373/1868 [71:20:47<25:41:19, 186.83s/it] 74%|███████▎  | 1374/1868 [71:23:54<25:38:03, 186.81s/it]{'loss': 2.0319, 'grad_norm': 0.0, 'learning_rate': 5.31049250535332e-06, 'epoch': 0.74}
? Step 1374  Epoch: 0.74  Loss: 1.7582
                                                           74%|███████▎  | 1374/1868 [71:23:54<25:38:03, 186.81s/it] 74%|███████▎  | 1375/1868 [71:27:01<25:35:29, 186.88s/it]{'loss': 1.7582, 'grad_norm': 0.0, 'learning_rate': 5.299785867237688e-06, 'epoch': 0.74}
? Step 1375  Epoch: 0.74  Loss: 1.4073
                                                           74%|███████▎  | 1375/1868 [71:27:01<25:35:29, 186.88s/it] 74%|███████▎  | 1376/1868 [71:30:07<25:31:48, 186.81s/it]{'loss': 1.4073, 'grad_norm': 0.0, 'learning_rate': 5.2890792291220564e-06, 'epoch': 0.74}
? Step 1376  Epoch: 0.74  Loss: 1.5173
                                                           74%|███████▎  | 1376/1868 [71:30:07<25:31:48, 186.81s/it] 74%|███████▎  | 1377/1868 [71:33:14<25:29:01, 186.85s/it]{'loss': 1.5173, 'grad_norm': 0.0, 'learning_rate': 5.278372591006424e-06, 'epoch': 0.74}
? Step 1377  Epoch: 0.74  Loss: 1.4912
                                                           74%|███████▎  | 1377/1868 [71:33:14<25:29:01, 186.85s/it] 74%|███████▍  | 1378/1868 [71:36:21<25:25:30, 186.80s/it]{'loss': 1.4912, 'grad_norm': 0.0, 'learning_rate': 5.267665952890793e-06, 'epoch': 0.74}
? Step 1378  Epoch: 0.74  Loss: 1.5997
                                                           74%|███████▍  | 1378/1868 [71:36:21<25:25:30, 186.80s/it] 74%|███████▍  | 1379/1868 [71:39:27<25:21:57, 186.74s/it]{'loss': 1.5997, 'grad_norm': 0.0, 'learning_rate': 5.256959314775161e-06, 'epoch': 0.74}
? Step 1379  Epoch: 0.74  Loss: 1.5827
                                                           74%|███████▍  | 1379/1868 [71:39:27<25:21:57, 186.74s/it] 74%|███████▍  | 1380/1868 [71:42:34<25:19:00, 186.76s/it]{'loss': 1.5827, 'grad_norm': 0.0, 'learning_rate': 5.2462526766595286e-06, 'epoch': 0.74}
? Step 1380  Epoch: 0.74  Loss: 1.5414
                                                           74%|███████▍  | 1380/1868 [71:42:34<25:19:00, 186.76s/it] 74%|███████▍  | 1381/1868 [71:45:41<25:16:07, 186.79s/it]{'loss': 1.5414, 'grad_norm': 0.0, 'learning_rate': 5.235546038543897e-06, 'epoch': 0.74}
? Step 1381  Epoch: 0.74  Loss: 1.4695
                                                           74%|███████▍  | 1381/1868 [71:45:41<25:16:07, 186.79s/it] 74%|███████▍  | 1382/1868 [71:48:48<25:12:19, 186.71s/it]{'loss': 1.4695, 'grad_norm': 0.0, 'learning_rate': 5.224839400428265e-06, 'epoch': 0.74}
? Step 1382  Epoch: 0.74  Loss: 1.6968
                                                           74%|███████▍  | 1382/1868 [71:48:48<25:12:19, 186.71s/it] 74%|███████▍  | 1383/1868 [71:51:55<25:09:57, 186.80s/it]{'loss': 1.6968, 'grad_norm': 0.0, 'learning_rate': 5.214132762312635e-06, 'epoch': 0.74}
? Step 1383  Epoch: 0.74  Loss: 1.3760
                                                           74%|███████▍  | 1383/1868 [71:51:55<25:09:57, 186.80s/it] 74%|███████▍  | 1384/1868 [71:55:01<25:06:29, 186.75s/it]{'loss': 1.376, 'grad_norm': 0.0, 'learning_rate': 5.203426124197003e-06, 'epoch': 0.74}
? Step 1384  Epoch: 0.74  Loss: 1.3844
                                                           74%|███████▍  | 1384/1868 [71:55:01<25:06:29, 186.75s/it] 74%|███████▍  | 1385/1868 [71:58:08<25:03:32, 186.77s/it]{'loss': 1.3844, 'grad_norm': 0.0, 'learning_rate': 5.192719486081371e-06, 'epoch': 0.74}
? Step 1385  Epoch: 0.74  Loss: 1.5116
                                                           74%|███████▍  | 1385/1868 [71:58:08<25:03:32, 186.77s/it] 74%|███████▍  | 1386/1868 [72:01:15<25:01:02, 186.85s/it]{'loss': 1.5116, 'grad_norm': 0.0, 'learning_rate': 5.18201284796574e-06, 'epoch': 0.74}
? Step 1386  Epoch: 0.74  Loss: 1.6784
                                                           74%|███████▍  | 1386/1868 [72:01:15<25:01:02, 186.85s/it] 74%|███████▍  | 1387/1868 [72:04:22<24:57:51, 186.84s/it]{'loss': 1.6784, 'grad_norm': 0.0, 'learning_rate': 5.171306209850108e-06, 'epoch': 0.74}
? Step 1387  Epoch: 0.74  Loss: 1.6096
                                                           74%|███████▍  | 1387/1868 [72:04:22<24:57:51, 186.84s/it] 74%|███████▍  | 1388/1868 [72:07:29<24:54:33, 186.82s/it]{'loss': 1.6096, 'grad_norm': 0.0, 'learning_rate': 5.160599571734476e-06, 'epoch': 0.74}
? Step 1388  Epoch: 0.74  Loss: 1.7164
                                                           74%|███████▍  | 1388/1868 [72:07:29<24:54:33, 186.82s/it] 74%|███████▍  | 1389/1868 [72:10:35<24:50:28, 186.70s/it]{'loss': 1.7164, 'grad_norm': 0.0, 'learning_rate': 5.149892933618844e-06, 'epoch': 0.74}
? Step 1389  Epoch: 0.74  Loss: 1.6866
                                                           74%|███████▍  | 1389/1868 [72:10:35<24:50:28, 186.70s/it] 74%|███████▍  | 1390/1868 [72:13:42<24:47:40, 186.74s/it]{'loss': 1.6866, 'grad_norm': 0.0, 'learning_rate': 5.139186295503213e-06, 'epoch': 0.74}
? Step 1390  Epoch: 0.74  Loss: 1.5358
                                                           74%|███████▍  | 1390/1868 [72:13:42<24:47:40, 186.74s/it] 74%|███████▍  | 1391/1868 [72:16:49<24:44:39, 186.75s/it]{'loss': 1.5358, 'grad_norm': 0.0, 'learning_rate': 5.128479657387581e-06, 'epoch': 0.74}
? Step 1391  Epoch: 0.74  Loss: 1.8353
                                                           74%|███████▍  | 1391/1868 [72:16:49<24:44:39, 186.75s/it] 75%|███████▍  | 1392/1868 [72:19:55<24:41:06, 186.69s/it]{'loss': 1.8353, 'grad_norm': 0.0, 'learning_rate': 5.117773019271948e-06, 'epoch': 0.74}
? Step 1392  Epoch: 0.75  Loss: 2.0320
                                                           75%|███████▍  | 1392/1868 [72:19:55<24:41:06, 186.69s/it] 75%|███████▍  | 1393/1868 [72:23:02<24:38:27, 186.75s/it]{'loss': 2.032, 'grad_norm': 0.0, 'learning_rate': 5.107066381156317e-06, 'epoch': 0.75}
? Step 1393  Epoch: 0.75  Loss: 1.4567
                                                           75%|███████▍  | 1393/1868 [72:23:02<24:38:27, 186.75s/it] 75%|███████▍  | 1394/1868 [72:26:09<24:35:47, 186.81s/it]{'loss': 1.4567, 'grad_norm': 0.0, 'learning_rate': 5.096359743040685e-06, 'epoch': 0.75}
? Step 1394  Epoch: 0.75  Loss: 1.5649
                                                           75%|███████▍  | 1394/1868 [72:26:09<24:35:47, 186.81s/it] 75%|███████▍  | 1395/1868 [72:29:16<24:32:14, 186.75s/it]{'loss': 1.5649, 'grad_norm': 0.0, 'learning_rate': 5.0856531049250536e-06, 'epoch': 0.75}
? Step 1395  Epoch: 0.75  Loss: 1.6326
                                                           75%|███████▍  | 1395/1868 [72:29:16<24:32:14, 186.75s/it] 75%|███████▍  | 1396/1868 [72:32:23<24:29:09, 186.76s/it]{'loss': 1.6326, 'grad_norm': 0.0, 'learning_rate': 5.074946466809421e-06, 'epoch': 0.75}
? Step 1396  Epoch: 0.75  Loss: 1.4965
                                                           75%|███████▍  | 1396/1868 [72:32:23<24:29:09, 186.76s/it] 75%|███████▍  | 1397/1868 [72:35:29<24:26:22, 186.80s/it]{'loss': 1.4965, 'grad_norm': 0.0, 'learning_rate': 5.064239828693791e-06, 'epoch': 0.75}
? Step 1397  Epoch: 0.75  Loss: 1.6945
                                                           75%|███████▍  | 1397/1868 [72:35:29<24:26:22, 186.80s/it] 75%|███████▍  | 1398/1868 [72:38:36<24:22:31, 186.71s/it]{'loss': 1.6945, 'grad_norm': 0.0, 'learning_rate': 5.05353319057816e-06, 'epoch': 0.75}
? Step 1398  Epoch: 0.75  Loss: 1.6072
                                                           75%|███████▍  | 1398/1868 [72:38:36<24:22:31, 186.71s/it] 75%|███████▍  | 1399/1868 [72:41:43<24:19:12, 186.68s/it]{'loss': 1.6072, 'grad_norm': 0.0, 'learning_rate': 5.042826552462527e-06, 'epoch': 0.75}
? Step 1399  Epoch: 0.75  Loss: 1.6429
                                                           75%|███████▍  | 1399/1868 [72:41:43<24:19:12, 186.68s/it] 75%|███████▍  | 1400/1868 [72:44:49<24:16:29, 186.73s/it]{'loss': 1.6429, 'grad_norm': 0.0, 'learning_rate': 5.032119914346896e-06, 'epoch': 0.75}
? Step 1400  Epoch: 0.75  Loss: 1.6100
                                                           75%|███████▍  | 1400/1868 [72:44:49<24:16:29, 186.73s/it]/home/dev25-01/mistral-env/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 75%|███████▌  | 1401/1868 [72:47:56<24:14:09, 186.83s/it]{'loss': 1.61, 'grad_norm': 0.0, 'learning_rate': 5.021413276231264e-06, 'epoch': 0.75}
? Step 1401  Epoch: 0.75  Loss: 1.3556
                                                           75%|███████▌  | 1401/1868 [72:47:56<24:14:09, 186.83s/it] 75%|███████▌  | 1402/1868 [72:51:03<24:10:59, 186.82s/it]{'loss': 1.3556, 'grad_norm': 0.0, 'learning_rate': 5.010706638115633e-06, 'epoch': 0.75}
? Step 1402  Epoch: 0.75  Loss: 1.3868
                                                           75%|███████▌  | 1402/1868 [72:51:03<24:10:59, 186.82s/it] 75%|███████▌  | 1403/1868 [72:54:10<24:07:50, 186.82s/it]{'loss': 1.3868, 'grad_norm': 0.0, 'learning_rate': 5e-06, 'epoch': 0.75}
? Step 1403  Epoch: 0.75  Loss: 1.5047
                                                           75%|███████▌  | 1403/1868 [72:54:10<24:07:50, 186.82s/it] 75%|███████▌  | 1404/1868 [72:57:17<24:04:58, 186.85s/it]{'loss': 1.5047, 'grad_norm': 0.0, 'learning_rate': 4.989293361884368e-06, 'epoch': 0.75}
? Step 1404  Epoch: 0.75  Loss: 1.4633
                                                           75%|███████▌  | 1404/1868 [72:57:17<24:04:58, 186.85s/it] 75%|███████▌  | 1405/1868 [73:00:24<24:01:24, 186.79s/it]{'loss': 1.4633, 'grad_norm': 0.0, 'learning_rate': 4.978586723768737e-06, 'epoch': 0.75}
? Step 1405  Epoch: 0.75  Loss: 1.6143
                                                           75%|███████▌  | 1405/1868 [73:00:24<24:01:24, 186.79s/it] 75%|███████▌  | 1406/1868 [73:03:31<23:58:39, 186.84s/it]{'loss': 1.6143, 'grad_norm': 0.0, 'learning_rate': 4.967880085653105e-06, 'epoch': 0.75}
? Step 1406  Epoch: 0.75  Loss: 1.5254
                                                           75%|███████▌  | 1406/1868 [73:03:31<23:58:39, 186.84s/it] 75%|███████▌  | 1407/1868 [73:06:37<23:54:37, 186.72s/it]{'loss': 1.5254, 'grad_norm': 0.0, 'learning_rate': 4.957173447537474e-06, 'epoch': 0.75}
? Step 1407  Epoch: 0.75  Loss: 1.5216
                                                           75%|███████▌  | 1407/1868 [73:06:37<23:54:37, 186.72s/it] 75%|███████▌  | 1408/1868 [73:09:43<23:50:20, 186.57s/it]{'loss': 1.5216, 'grad_norm': 0.0, 'learning_rate': 4.946466809421842e-06, 'epoch': 0.75}
? Step 1408  Epoch: 0.75  Loss: 1.7244
                                                           75%|███████▌  | 1408/1868 [73:09:43<23:50:20, 186.57s/it] 75%|███████▌  | 1409/1868 [73:12:50<23:46:43, 186.50s/it]{'loss': 1.7244, 'grad_norm': 0.0, 'learning_rate': 4.935760171306211e-06, 'epoch': 0.75}
? Step 1409  Epoch: 0.75  Loss: 2.0616
                                                           75%|███████▌  | 1409/1868 [73:12:50<23:46:43, 186.50s/it] 75%|███████▌  | 1410/1868 [73:15:56<23:43:27, 186.48s/it]{'loss': 2.0616, 'grad_norm': 0.0, 'learning_rate': 4.9250535331905786e-06, 'epoch': 0.75}
? Step 1410  Epoch: 0.75  Loss: 1.3568
                                                           75%|███████▌  | 1410/1868 [73:15:56<23:43:27, 186.48s/it] 76%|███████▌  | 1411/1868 [73:19:03<23:40:41, 186.52s/it]{'loss': 1.3568, 'grad_norm': 0.0, 'learning_rate': 4.914346895074946e-06, 'epoch': 0.75}
? Step 1411  Epoch: 0.76  Loss: 1.6476
                                                           76%|███████▌  | 1411/1868 [73:19:03<23:40:41, 186.52s/it] 76%|███████▌  | 1412/1868 [73:22:10<23:38:33, 186.65s/it]{'loss': 1.6476, 'grad_norm': 0.0, 'learning_rate': 4.903640256959315e-06, 'epoch': 0.76}
? Step 1412  Epoch: 0.76  Loss: 1.3416
                                                           76%|███████▌  | 1412/1868 [73:22:10<23:38:33, 186.65s/it] 76%|███████▌  | 1413/1868 [73:25:16<23:34:45, 186.56s/it]{'loss': 1.3416, 'grad_norm': 0.0, 'learning_rate': 4.892933618843683e-06, 'epoch': 0.76}
? Step 1413  Epoch: 0.76  Loss: 1.6092
                                                           76%|███████▌  | 1413/1868 [73:25:16<23:34:45, 186.56s/it] 76%|███████▌  | 1414/1868 [73:28:22<23:31:01, 186.48s/it]{'loss': 1.6092, 'grad_norm': 0.0, 'learning_rate': 4.882226980728052e-06, 'epoch': 0.76}
? Step 1414  Epoch: 0.76  Loss: 1.4922
                                                           76%|███████▌  | 1414/1868 [73:28:22<23:31:01, 186.48s/it] 76%|███████▌  | 1415/1868 [73:31:29<23:28:17, 186.53s/it]{'loss': 1.4922, 'grad_norm': 0.0, 'learning_rate': 4.87152034261242e-06, 'epoch': 0.76}
? Step 1415  Epoch: 0.76  Loss: 1.5209
                                                           76%|███████▌  | 1415/1868 [73:31:29<23:28:17, 186.53s/it] 76%|███████▌  | 1416/1868 [73:34:36<23:25:22, 186.55s/it]{'loss': 1.5209, 'grad_norm': 0.0, 'learning_rate': 4.860813704496788e-06, 'epoch': 0.76}
? Step 1416  Epoch: 0.76  Loss: 1.5168
                                                           76%|███████▌  | 1416/1868 [73:34:36<23:25:22, 186.55s/it] 76%|███████▌  | 1417/1868 [73:37:42<23:22:33, 186.59s/it]{'loss': 1.5168, 'grad_norm': 0.0, 'learning_rate': 4.850107066381157e-06, 'epoch': 0.76}
? Step 1417  Epoch: 0.76  Loss: 1.6340
                                                           76%|███████▌  | 1417/1868 [73:37:42<23:22:33, 186.59s/it] 76%|███████▌  | 1418/1868 [73:40:49<23:19:21, 186.58s/it]{'loss': 1.634, 'grad_norm': 0.0, 'learning_rate': 4.8394004282655246e-06, 'epoch': 0.76}
? Step 1418  Epoch: 0.76  Loss: 1.4775
                                                           76%|███████▌  | 1418/1868 [73:40:49<23:19:21, 186.58s/it] 76%|███████▌  | 1419/1868 [73:43:55<23:16:34, 186.62s/it]{'loss': 1.4775, 'grad_norm': 0.0, 'learning_rate': 4.828693790149893e-06, 'epoch': 0.76}
? Step 1419  Epoch: 0.76  Loss: 1.6296
                                                           76%|███████▌  | 1419/1868 [73:43:55<23:16:34, 186.62s/it] 76%|███████▌  | 1420/1868 [73:47:02<23:13:28, 186.63s/it]{'loss': 1.6296, 'grad_norm': 0.0, 'learning_rate': 4.817987152034261e-06, 'epoch': 0.76}
? Step 1420  Epoch: 0.76  Loss: 1.5848
                                                           76%|███████▌  | 1420/1868 [73:47:02<23:13:28, 186.63s/it] 76%|███████▌  | 1421/1868 [73:50:09<23:11:04, 186.72s/it]{'loss': 1.5848, 'grad_norm': 0.0, 'learning_rate': 4.807280513918631e-06, 'epoch': 0.76}
? Step 1421  Epoch: 0.76  Loss: 1.4366
                                                           76%|███████▌  | 1421/1868 [73:50:09<23:11:04, 186.72s/it] 76%|███████▌  | 1422/1868 [73:53:16<23:07:51, 186.71s/it]{'loss': 1.4366, 'grad_norm': 0.0, 'learning_rate': 4.796573875802998e-06, 'epoch': 0.76}
? Step 1422  Epoch: 0.76  Loss: 1.4763
                                                           76%|███████▌  | 1422/1868 [73:53:16<23:07:51, 186.71s/it] 76%|███████▌  | 1423/1868 [73:56:22<23:04:44, 186.71s/it]{'loss': 1.4763, 'grad_norm': 0.0, 'learning_rate': 4.785867237687366e-06, 'epoch': 0.76}
? Step 1423  Epoch: 0.76  Loss: 1.8161
                                                           76%|███████▌  | 1423/1868 [73:56:22<23:04:44, 186.71s/it] 76%|███████▌  | 1424/1868 [73:59:29<23:01:28, 186.69s/it]{'loss': 1.8161, 'grad_norm': 0.0, 'learning_rate': 4.775160599571735e-06, 'epoch': 0.76}
? Step 1424  Epoch: 0.76  Loss: 1.5182
                                                           76%|███████▌  | 1424/1868 [73:59:29<23:01:28, 186.69s/it] 76%|███████▋  | 1425/1868 [74:02:36<22:57:51, 186.62s/it]{'loss': 1.5182, 'grad_norm': 0.0, 'learning_rate': 4.764453961456103e-06, 'epoch': 0.76}
? Step 1425  Epoch: 0.76  Loss: 1.6298
                                                           76%|███████▋  | 1425/1868 [74:02:36<22:57:51, 186.62s/it] 76%|███████▋  | 1426/1868 [74:05:42<22:54:56, 186.64s/it]{'loss': 1.6298, 'grad_norm': 0.0, 'learning_rate': 4.753747323340471e-06, 'epoch': 0.76}
? Step 1426  Epoch: 0.76  Loss: 1.5145
                                                           76%|███████▋  | 1426/1868 [74:05:42<22:54:56, 186.64s/it] 76%|███████▋  | 1427/1868 [74:08:49<22:52:13, 186.70s/it]{'loss': 1.5145, 'grad_norm': 0.0, 'learning_rate': 4.743040685224839e-06, 'epoch': 0.76}
? Step 1427  Epoch: 0.76  Loss: 1.5879
                                                           76%|███████▋  | 1427/1868 [74:08:49<22:52:13, 186.70s/it] 76%|███████▋  | 1428/1868 [74:11:56<22:49:01, 186.69s/it]{'loss': 1.5879, 'grad_norm': 0.0, 'learning_rate': 4.732334047109208e-06, 'epoch': 0.76}
? Step 1428  Epoch: 0.76  Loss: 1.5981
                                                           76%|███████▋  | 1428/1868 [74:11:56<22:49:01, 186.69s/it] 76%|███████▋  | 1429/1868 [74:15:02<22:45:34, 186.64s/it]{'loss': 1.5981, 'grad_norm': 0.0, 'learning_rate': 4.7216274089935766e-06, 'epoch': 0.76}
? Step 1429  Epoch: 0.76  Loss: 1.6120
                                                           76%|███████▋  | 1429/1868 [74:15:02<22:45:34, 186.64s/it] 77%|███████▋  | 1430/1868 [74:18:09<22:42:02, 186.58s/it]{'loss': 1.612, 'grad_norm': 0.0, 'learning_rate': 4.710920770877944e-06, 'epoch': 0.76}
? Step 1430  Epoch: 0.77  Loss: 1.5229
                                                           77%|███████▋  | 1430/1868 [74:18:09<22:42:02, 186.58s/it] 77%|███████▋  | 1431/1868 [74:21:15<22:39:05, 186.60s/it]{'loss': 1.5229, 'grad_norm': 0.0, 'learning_rate': 4.700214132762313e-06, 'epoch': 0.77}
? Step 1431  Epoch: 0.77  Loss: 1.7626
                                                           77%|███████▋  | 1431/1868 [74:21:15<22:39:05, 186.60s/it] 77%|███████▋  | 1432/1868 [74:24:22<22:35:16, 186.51s/it]{'loss': 1.7626, 'grad_norm': 0.0, 'learning_rate': 4.689507494646681e-06, 'epoch': 0.77}
? Step 1432  Epoch: 0.77  Loss: 1.9157
                                                           77%|███████▋  | 1432/1868 [74:24:22<22:35:16, 186.51s/it] 77%|███████▋  | 1433/1868 [74:27:28<22:31:53, 186.47s/it]{'loss': 1.9157, 'grad_norm': 0.0, 'learning_rate': 4.6788008565310496e-06, 'epoch': 0.77}
? Step 1433  Epoch: 0.77  Loss: 1.5502
                                                           77%|███████▋  | 1433/1868 [74:27:28<22:31:53, 186.47s/it] 77%|███████▋  | 1434/1868 [74:30:34<22:28:36, 186.44s/it]{'loss': 1.5502, 'grad_norm': 0.0, 'learning_rate': 4.668094218415418e-06, 'epoch': 0.77}
? Step 1434  Epoch: 0.77  Loss: 1.7043
                                                           77%|███████▋  | 1434/1868 [74:30:34<22:28:36, 186.44s/it] 77%|███████▋  | 1435/1868 [74:33:41<22:26:33, 186.59s/it]{'loss': 1.7043, 'grad_norm': 0.0, 'learning_rate': 4.657387580299786e-06, 'epoch': 0.77}
? Step 1435  Epoch: 0.77  Loss: 1.5430
                                                           77%|███████▋  | 1435/1868 [74:33:41<22:26:33, 186.59s/it] 77%|███████▋  | 1436/1868 [74:36:48<22:23:11, 186.55s/it]{'loss': 1.543, 'grad_norm': 0.0, 'learning_rate': 4.646680942184155e-06, 'epoch': 0.77}
? Step 1436  Epoch: 0.77  Loss: 1.7346
                                                           77%|███████▋  | 1436/1868 [74:36:48<22:23:11, 186.55s/it] 77%|███████▋  | 1437/1868 [74:39:54<22:20:16, 186.58s/it]{'loss': 1.7346, 'grad_norm': 0.0, 'learning_rate': 4.6359743040685226e-06, 'epoch': 0.77}
? Step 1437  Epoch: 0.77  Loss: 1.6047
                                                           77%|███████▋  | 1437/1868 [74:39:54<22:20:16, 186.58s/it] 77%|███████▋  | 1438/1868 [74:43:01<22:17:20, 186.61s/it]{'loss': 1.6047, 'grad_norm': 0.0, 'learning_rate': 4.625267665952891e-06, 'epoch': 0.77}
? Step 1438  Epoch: 0.77  Loss: 1.5122
                                                           77%|███████▋  | 1438/1868 [74:43:01<22:17:20, 186.61s/it] 77%|███████▋  | 1439/1868 [74:46:08<22:14:05, 186.59s/it]{'loss': 1.5122, 'grad_norm': 0.0, 'learning_rate': 4.614561027837259e-06, 'epoch': 0.77}
? Step 1439  Epoch: 0.77  Loss: 1.5060
                                                           77%|███████▋  | 1439/1868 [74:46:08<22:14:05, 186.59s/it] 77%|███████▋  | 1440/1868 [74:49:14<22:10:51, 186.57s/it]{'loss': 1.506, 'grad_norm': 0.0, 'learning_rate': 4.603854389721628e-06, 'epoch': 0.77}
? Step 1440  Epoch: 0.77  Loss: 1.5232
                                                           77%|███████▋  | 1440/1868 [74:49:14<22:10:51, 186.57s/it] 77%|███████▋  | 1441/1868 [74:52:21<22:07:49, 186.58s/it]{'loss': 1.5232, 'grad_norm': 0.0, 'learning_rate': 4.593147751605996e-06, 'epoch': 0.77}
? Step 1441  Epoch: 0.77  Loss: 1.5976
                                                           77%|███████▋  | 1441/1868 [74:52:21<22:07:49, 186.58s/it] 77%|███████▋  | 1442/1868 [74:55:27<22:04:50, 186.60s/it]{'loss': 1.5976, 'grad_norm': 0.0, 'learning_rate': 4.582441113490364e-06, 'epoch': 0.77}
? Step 1442  Epoch: 0.77  Loss: 1.5065
                                                           77%|███████▋  | 1442/1868 [74:55:27<22:04:50, 186.60s/it] 77%|███████▋  | 1443/1868 [74:58:34<22:01:57, 186.63s/it]{'loss': 1.5065, 'grad_norm': 0.0, 'learning_rate': 4.571734475374733e-06, 'epoch': 0.77}
? Step 1443  Epoch: 0.77  Loss: 1.6765
                                                           77%|███████▋  | 1443/1868 [74:58:34<22:01:57, 186.63s/it] 77%|███████▋  | 1444/1868 [75:01:40<21:58:19, 186.56s/it]{'loss': 1.6765, 'grad_norm': 0.0, 'learning_rate': 4.561027837259101e-06, 'epoch': 0.77}
? Step 1444  Epoch: 0.77  Loss: 1.7390
                                                           77%|███████▋  | 1444/1868 [75:01:40<21:58:19, 186.56s/it] 77%|███████▋  | 1445/1868 [75:04:47<21:55:01, 186.53s/it]{'loss': 1.739, 'grad_norm': 0.0, 'learning_rate': 4.550321199143469e-06, 'epoch': 0.77}
? Step 1445  Epoch: 0.77  Loss: 1.6950
                                                           77%|███████▋  | 1445/1868 [75:04:47<21:55:01, 186.53s/it] 77%|███████▋  | 1446/1868 [75:07:54<21:52:13, 186.57s/it]{'loss': 1.695, 'grad_norm': 0.0, 'learning_rate': 4.539614561027837e-06, 'epoch': 0.77}
? Step 1446  Epoch: 0.77  Loss: 1.3632
                                                           77%|███████▋  | 1446/1868 [75:07:54<21:52:13, 186.57s/it] 77%|███████▋  | 1447/1868 [75:11:00<21:49:37, 186.65s/it]{'loss': 1.3632, 'grad_norm': 0.0, 'learning_rate': 4.528907922912206e-06, 'epoch': 0.77}
? Step 1447  Epoch: 0.77  Loss: 1.6117
                                                           77%|███████▋  | 1447/1868 [75:11:00<21:49:37, 186.65s/it] 78%|███████▊  | 1448/1868 [75:14:07<21:47:00, 186.72s/it]{'loss': 1.6117, 'grad_norm': 0.0, 'learning_rate': 4.5182012847965746e-06, 'epoch': 0.77}
? Step 1448  Epoch: 0.78  Loss: 1.5098
                                                           78%|███████▊  | 1448/1868 [75:14:07<21:47:00, 186.72s/it] 78%|███████▊  | 1449/1868 [75:17:14<21:44:00, 186.73s/it]{'loss': 1.5098, 'grad_norm': 0.0, 'learning_rate': 4.507494646680942e-06, 'epoch': 0.78}
? Step 1449  Epoch: 0.78  Loss: 1.3804
                                                           78%|███████▊  | 1449/1868 [75:17:14<21:44:00, 186.73s/it] 78%|███████▊  | 1450/1868 [75:20:21<21:40:16, 186.64s/it]{'loss': 1.3804, 'grad_norm': 0.0, 'learning_rate': 4.496788008565311e-06, 'epoch': 0.78}
? Step 1450  Epoch: 0.78  Loss: 1.4385
                                                           78%|███████▊  | 1450/1868 [75:20:21<21:40:16, 186.64s/it] 78%|███████▊  | 1451/1868 [75:23:27<21:36:49, 186.59s/it]{'loss': 1.4385, 'grad_norm': 0.0, 'learning_rate': 4.486081370449679e-06, 'epoch': 0.78}
? Step 1451  Epoch: 0.78  Loss: 1.6685
                                                           78%|███████▊  | 1451/1868 [75:23:27<21:36:49, 186.59s/it] 78%|███████▊  | 1452/1868 [75:26:34<21:33:51, 186.61s/it]{'loss': 1.6685, 'grad_norm': 0.0, 'learning_rate': 4.4753747323340476e-06, 'epoch': 0.78}
? Step 1452  Epoch: 0.78  Loss: 1.4830
                                                           78%|███████▊  | 1452/1868 [75:26:34<21:33:51, 186.61s/it] 78%|███████▊  | 1453/1868 [75:29:40<21:30:30, 186.58s/it]{'loss': 1.483, 'grad_norm': 0.0, 'learning_rate': 4.464668094218415e-06, 'epoch': 0.78}
? Step 1453  Epoch: 0.78  Loss: 1.8480
                                                           78%|███████▊  | 1453/1868 [75:29:40<21:30:30, 186.58s/it] 78%|███████▊  | 1454/1868 [75:32:47<21:27:45, 186.63s/it]{'loss': 1.848, 'grad_norm': 0.0, 'learning_rate': 4.453961456102784e-06, 'epoch': 0.78}
? Step 1454  Epoch: 0.78  Loss: 1.6594
                                                           78%|███████▊  | 1454/1868 [75:32:47<21:27:45, 186.63s/it] 78%|███████▊  | 1455/1868 [75:35:54<21:24:47, 186.65s/it]{'loss': 1.6594, 'grad_norm': 0.0, 'learning_rate': 4.443254817987153e-06, 'epoch': 0.78}
? Step 1455  Epoch: 0.78  Loss: 1.6877
                                                           78%|███████▊  | 1455/1868 [75:35:54<21:24:47, 186.65s/it] 78%|███████▊  | 1456/1868 [75:39:00<21:21:46, 186.67s/it]{'loss': 1.6877, 'grad_norm': 0.0, 'learning_rate': 4.4325481798715205e-06, 'epoch': 0.78}
? Step 1456  Epoch: 0.78  Loss: 1.7459
                                                           78%|███████▊  | 1456/1868 [75:39:00<21:21:46, 186.67s/it] 78%|███████▊  | 1457/1868 [75:42:07<21:18:44, 186.68s/it]{'loss': 1.7459, 'grad_norm': 0.0, 'learning_rate': 4.421841541755889e-06, 'epoch': 0.78}
? Step 1457  Epoch: 0.78  Loss: 1.5795
                                                           78%|███████▊  | 1457/1868 [75:42:07<21:18:44, 186.68s/it] 78%|███████▊  | 1458/1868 [75:45:14<21:15:31, 186.66s/it]{'loss': 1.5795, 'grad_norm': 0.0, 'learning_rate': 4.411134903640257e-06, 'epoch': 0.78}
? Step 1458  Epoch: 0.78  Loss: 1.7532
                                                           78%|███████▊  | 1458/1868 [75:45:14<21:15:31, 186.66s/it] 78%|███████▊  | 1459/1868 [75:48:20<21:12:25, 186.66s/it]{'loss': 1.7532, 'grad_norm': 0.0, 'learning_rate': 4.400428265524626e-06, 'epoch': 0.78}
? Step 1459  Epoch: 0.78  Loss: 1.9105
                                                           78%|███████▊  | 1459/1868 [75:48:20<21:12:25, 186.66s/it] 78%|███████▊  | 1460/1868 [75:51:27<21:09:54, 186.75s/it]{'loss': 1.9105, 'grad_norm': 0.0, 'learning_rate': 4.3897216274089935e-06, 'epoch': 0.78}
? Step 1460  Epoch: 0.78  Loss: 1.5804
                                                           78%|███████▊  | 1460/1868 [75:51:27<21:09:54, 186.75s/it] 78%|███████▊  | 1461/1868 [75:54:34<21:06:21, 186.69s/it]{'loss': 1.5804, 'grad_norm': 0.0, 'learning_rate': 4.379014989293362e-06, 'epoch': 0.78}
? Step 1461  Epoch: 0.78  Loss: 1.4780
                                                           78%|███████▊  | 1461/1868 [75:54:34<21:06:21, 186.69s/it] 78%|███████▊  | 1462/1868 [75:57:41<21:03:19, 186.70s/it]{'loss': 1.478, 'grad_norm': 0.0, 'learning_rate': 4.368308351177731e-06, 'epoch': 0.78}
? Step 1462  Epoch: 0.78  Loss: 1.7875
                                                           78%|███████▊  | 1462/1868 [75:57:41<21:03:19, 186.70s/it] 78%|███████▊  | 1463/1868 [76:00:47<21:00:39, 186.76s/it]{'loss': 1.7875, 'grad_norm': 0.0, 'learning_rate': 4.357601713062099e-06, 'epoch': 0.78}
? Step 1463  Epoch: 0.78  Loss: 1.4965
                                                           78%|███████▊  | 1463/1868 [76:00:47<21:00:39, 186.76s/it] 78%|███████▊  | 1464/1868 [76:03:54<20:57:08, 186.70s/it]{'loss': 1.4965, 'grad_norm': 0.0, 'learning_rate': 4.346895074946467e-06, 'epoch': 0.78}
? Step 1464  Epoch: 0.78  Loss: 1.5642
                                                           78%|███████▊  | 1464/1868 [76:03:54<20:57:08, 186.70s/it] 78%|███████▊  | 1465/1868 [76:07:00<20:53:34, 186.64s/it]{'loss': 1.5642, 'grad_norm': 0.0, 'learning_rate': 4.336188436830835e-06, 'epoch': 0.78}
? Step 1465  Epoch: 0.78  Loss: 2.1520
                                                           78%|███████▊  | 1465/1868 [76:07:01<20:53:34, 186.64s/it] 78%|███████▊  | 1466/1868 [76:10:07<20:50:43, 186.68s/it]{'loss': 2.152, 'grad_norm': 0.0, 'learning_rate': 4.325481798715204e-06, 'epoch': 0.78}
? Step 1466  Epoch: 0.78  Loss: 1.6052
                                                           78%|███████▊  | 1466/1868 [76:10:07<20:50:43, 186.68s/it] 79%|███████▊  | 1467/1868 [76:13:14<20:47:50, 186.71s/it]{'loss': 1.6052, 'grad_norm': 0.0, 'learning_rate': 4.314775160599572e-06, 'epoch': 0.78}
? Step 1467  Epoch: 0.79  Loss: 1.5246
                                                           79%|███████▊  | 1467/1868 [76:13:14<20:47:50, 186.71s/it] 79%|███████▊  | 1468/1868 [76:16:21<20:44:33, 186.68s/it]{'loss': 1.5246, 'grad_norm': 0.0, 'learning_rate': 4.30406852248394e-06, 'epoch': 0.79}
? Step 1468  Epoch: 0.79  Loss: 1.8819
                                                           79%|███████▊  | 1468/1868 [76:16:21<20:44:33, 186.68s/it] 79%|███████▊  | 1469/1868 [76:19:27<20:41:22, 186.67s/it]{'loss': 1.8819, 'grad_norm': 0.0, 'learning_rate': 4.293361884368309e-06, 'epoch': 0.79}
? Step 1469  Epoch: 0.79  Loss: 1.8827
                                                           79%|███████▊  | 1469/1868 [76:19:27<20:41:22, 186.67s/it] 79%|███████▊  | 1470/1868 [76:22:34<20:38:48, 186.76s/it]{'loss': 1.8827, 'grad_norm': 0.0, 'learning_rate': 4.282655246252677e-06, 'epoch': 0.79}
? Step 1470  Epoch: 0.79  Loss: 1.4145
                                                           79%|███████▊  | 1470/1868 [76:22:34<20:38:48, 186.76s/it] 79%|███████▊  | 1471/1868 [76:25:41<20:35:55, 186.79s/it]{'loss': 1.4145, 'grad_norm': 0.0, 'learning_rate': 4.2719486081370455e-06, 'epoch': 0.79}
? Step 1471  Epoch: 0.79  Loss: 1.8010
                                                           79%|███████▊  | 1471/1868 [76:25:41<20:35:55, 186.79s/it] 79%|███████▉  | 1472/1868 [76:28:48<20:32:40, 186.77s/it]{'loss': 1.801, 'grad_norm': 0.0, 'learning_rate': 4.261241970021413e-06, 'epoch': 0.79}
? Step 1472  Epoch: 0.79  Loss: 1.4127
                                                           79%|███████▉  | 1472/1868 [76:28:48<20:32:40, 186.77s/it] 79%|███████▉  | 1473/1868 [76:31:55<20:29:53, 186.82s/it]{'loss': 1.4127, 'grad_norm': 0.0, 'learning_rate': 4.250535331905782e-06, 'epoch': 0.79}
? Step 1473  Epoch: 0.79  Loss: 1.4884
                                                           79%|███████▉  | 1473/1868 [76:31:55<20:29:53, 186.82s/it] 79%|███████▉  | 1474/1868 [76:35:01<20:26:01, 186.71s/it]{'loss': 1.4884, 'grad_norm': 0.0, 'learning_rate': 4.23982869379015e-06, 'epoch': 0.79}
? Step 1474  Epoch: 0.79  Loss: 1.5602
                                                           79%|███████▉  | 1474/1868 [76:35:01<20:26:01, 186.71s/it] 79%|███████▉  | 1475/1868 [76:38:08<20:22:38, 186.66s/it]{'loss': 1.5602, 'grad_norm': 0.0, 'learning_rate': 4.2291220556745185e-06, 'epoch': 0.79}
? Step 1475  Epoch: 0.79  Loss: 1.7105
                                                           79%|███████▉  | 1475/1868 [76:38:08<20:22:38, 186.66s/it] 79%|███████▉  | 1476/1868 [76:41:14<20:19:26, 186.65s/it]{'loss': 1.7105, 'grad_norm': 0.0, 'learning_rate': 4.218415417558887e-06, 'epoch': 0.79}
? Step 1476  Epoch: 0.79  Loss: 1.6124
                                                           79%|███████▉  | 1476/1868 [76:41:14<20:19:26, 186.65s/it] 79%|███████▉  | 1477/1868 [76:44:21<20:16:08, 186.62s/it]{'loss': 1.6124, 'grad_norm': 0.0, 'learning_rate': 4.207708779443255e-06, 'epoch': 0.79}
? Step 1477  Epoch: 0.79  Loss: 1.4686
                                                           79%|███████▉  | 1477/1868 [76:44:21<20:16:08, 186.62s/it] 79%|███████▉  | 1478/1868 [76:47:28<20:13:37, 186.71s/it]{'loss': 1.4686, 'grad_norm': 0.0, 'learning_rate': 4.197002141327624e-06, 'epoch': 0.79}
? Step 1478  Epoch: 0.79  Loss: 1.6170
                                                           79%|███████▉  | 1478/1868 [76:47:28<20:13:37, 186.71s/it] 79%|███████▉  | 1479/1868 [76:50:34<20:09:53, 186.62s/it]{'loss': 1.617, 'grad_norm': 0.0, 'learning_rate': 4.1862955032119915e-06, 'epoch': 0.79}
? Step 1479  Epoch: 0.79  Loss: 1.8431
                                                           79%|███████▉  | 1479/1868 [76:50:34<20:09:53, 186.62s/it] 79%|███████▉  | 1480/1868 [76:53:41<20:07:23, 186.71s/it]{'loss': 1.8431, 'grad_norm': 0.0, 'learning_rate': 4.17558886509636e-06, 'epoch': 0.79}
? Step 1480  Epoch: 0.79  Loss: 1.5107
                                                           79%|███████▉  | 1480/1868 [76:53:41<20:07:23, 186.71s/it] 79%|███████▉  | 1481/1868 [76:56:48<20:04:14, 186.70s/it]{'loss': 1.5107, 'grad_norm': 0.0, 'learning_rate': 4.164882226980728e-06, 'epoch': 0.79}
? Step 1481  Epoch: 0.79  Loss: 1.4504
                                                           79%|███████▉  | 1481/1868 [76:56:48<20:04:14, 186.70s/it] 79%|███████▉  | 1482/1868 [76:59:55<20:01:00, 186.69s/it]{'loss': 1.4504, 'grad_norm': 0.0, 'learning_rate': 4.154175588865097e-06, 'epoch': 0.79}
? Step 1482  Epoch: 0.79  Loss: 1.7063
                                                           79%|███████▉  | 1482/1868 [76:59:55<20:01:00, 186.69s/it] 79%|███████▉  | 1483/1868 [77:03:01<19:57:47, 186.67s/it]{'loss': 1.7063, 'grad_norm': 0.0, 'learning_rate': 4.143468950749465e-06, 'epoch': 0.79}
? Step 1483  Epoch: 0.79  Loss: 1.4424
                                                           79%|███████▉  | 1483/1868 [77:03:01<19:57:47, 186.67s/it] 79%|███████▉  | 1484/1868 [77:06:08<19:54:45, 186.68s/it]{'loss': 1.4424, 'grad_norm': 0.0, 'learning_rate': 4.132762312633833e-06, 'epoch': 0.79}
? Step 1484  Epoch: 0.79  Loss: 1.4308
                                                           79%|███████▉  | 1484/1868 [77:06:08<19:54:45, 186.68s/it] 79%|███████▉  | 1485/1868 [77:09:15<19:51:56, 186.73s/it]{'loss': 1.4308, 'grad_norm': 0.0, 'learning_rate': 4.122055674518202e-06, 'epoch': 0.79}
? Step 1485  Epoch: 0.79  Loss: 1.7592
                                                           79%|███████▉  | 1485/1868 [77:09:15<19:51:56, 186.73s/it] 80%|███████▉  | 1486/1868 [77:12:22<19:48:56, 186.75s/it]{'loss': 1.7592, 'grad_norm': 0.0, 'learning_rate': 4.11134903640257e-06, 'epoch': 0.79}
? Step 1486  Epoch: 0.80  Loss: 1.4995
                                                           80%|███████▉  | 1486/1868 [77:12:22<19:48:56, 186.75s/it] 80%|███████▉  | 1487/1868 [77:15:28<19:46:03, 186.78s/it]{'loss': 1.4995, 'grad_norm': 0.0, 'learning_rate': 4.100642398286938e-06, 'epoch': 0.8}
? Step 1487  Epoch: 0.80  Loss: 1.5764
                                                           80%|███████▉  | 1487/1868 [77:15:28<19:46:03, 186.78s/it] 80%|███████▉  | 1488/1868 [77:18:35<19:42:45, 186.75s/it]{'loss': 1.5764, 'grad_norm': 0.0, 'learning_rate': 4.089935760171306e-06, 'epoch': 0.8}
? Step 1488  Epoch: 0.80  Loss: 1.8148
                                                           80%|███████▉  | 1488/1868 [77:18:35<19:42:45, 186.75s/it] 80%|███████▉  | 1489/1868 [77:21:42<19:40:14, 186.85s/it]{'loss': 1.8148, 'grad_norm': 0.0, 'learning_rate': 4.079229122055675e-06, 'epoch': 0.8}
? Step 1489  Epoch: 0.80  Loss: 1.4323
                                                           80%|███████▉  | 1489/1868 [77:21:42<19:40:14, 186.85s/it] 80%|███████▉  | 1490/1868 [77:24:49<19:37:05, 186.84s/it]{'loss': 1.4323, 'grad_norm': 0.0, 'learning_rate': 4.0685224839400435e-06, 'epoch': 0.8}
? Step 1490  Epoch: 0.80  Loss: 1.5523
                                                           80%|███████▉  | 1490/1868 [77:24:49<19:37:05, 186.84s/it] 80%|███████▉  | 1491/1868 [77:27:56<19:34:32, 186.93s/it]{'loss': 1.5523, 'grad_norm': 0.0, 'learning_rate': 4.057815845824411e-06, 'epoch': 0.8}
? Step 1491  Epoch: 0.80  Loss: 1.4168
                                                           80%|███████▉  | 1491/1868 [77:27:56<19:34:32, 186.93s/it] 80%|███████▉  | 1492/1868 [77:31:03<19:31:09, 186.89s/it]{'loss': 1.4168, 'grad_norm': 0.0, 'learning_rate': 4.04710920770878e-06, 'epoch': 0.8}
? Step 1492  Epoch: 0.80  Loss: 1.5967
                                                           80%|███████▉  | 1492/1868 [77:31:03<19:31:09, 186.89s/it] 80%|███████▉  | 1493/1868 [77:34:10<19:27:46, 186.84s/it]{'loss': 1.5967, 'grad_norm': 0.0, 'learning_rate': 4.036402569593148e-06, 'epoch': 0.8}
? Step 1493  Epoch: 0.80  Loss: 1.4657
                                                           80%|███████▉  | 1493/1868 [77:34:10<19:27:46, 186.84s/it] 80%|███████▉  | 1494/1868 [77:37:17<19:25:13, 186.93s/it]{'loss': 1.4657, 'grad_norm': 0.0, 'learning_rate': 4.0256959314775165e-06, 'epoch': 0.8}
? Step 1494  Epoch: 0.80  Loss: 1.4064
                                                           80%|███████▉  | 1494/1868 [77:37:17<19:25:13, 186.93s/it] 80%|████████  | 1495/1868 [77:40:24<19:22:15, 186.96s/it]{'loss': 1.4064, 'grad_norm': 0.0, 'learning_rate': 4.014989293361884e-06, 'epoch': 0.8}
? Step 1495  Epoch: 0.80  Loss: 1.3596
                                                           80%|████████  | 1495/1868 [77:40:24<19:22:15, 186.96s/it] 80%|████████  | 1496/1868 [77:43:31<19:19:06, 186.95s/it]{'loss': 1.3596, 'grad_norm': 0.0, 'learning_rate': 4.004282655246253e-06, 'epoch': 0.8}
? Step 1496  Epoch: 0.80  Loss: 1.6120
                                                           80%|████████  | 1496/1868 [77:43:31<19:19:06, 186.95s/it] 80%|████████  | 1497/1868 [77:46:37<19:15:38, 186.90s/it]{'loss': 1.612, 'grad_norm': 0.0, 'learning_rate': 3.993576017130622e-06, 'epoch': 0.8}
? Step 1497  Epoch: 0.80  Loss: 1.6082
                                                           80%|████████  | 1497/1868 [77:46:37<19:15:38, 186.90s/it] 80%|████████  | 1498/1868 [77:49:44<19:12:43, 186.93s/it]{'loss': 1.6082, 'grad_norm': 0.0, 'learning_rate': 3.9828693790149895e-06, 'epoch': 0.8}
? Step 1498  Epoch: 0.80  Loss: 1.5702
                                                           80%|████████  | 1498/1868 [77:49:44<19:12:43, 186.93s/it] 80%|████████  | 1499/1868 [77:52:51<19:09:21, 186.89s/it]{'loss': 1.5702, 'grad_norm': 0.0, 'learning_rate': 3.972162740899358e-06, 'epoch': 0.8}
? Step 1499  Epoch: 0.80  Loss: 1.7267
                                                           80%|████████  | 1499/1868 [77:52:51<19:09:21, 186.89s/it] 80%|████████  | 1500/1868 [77:55:58<19:06:23, 186.91s/it]{'loss': 1.7267, 'grad_norm': 0.0, 'learning_rate': 3.961456102783726e-06, 'epoch': 0.8}
? Step 1500  Epoch: 0.80  Loss: 1.6084
                                                           80%|████████  | 1500/1868 [77:55:58<19:06:23, 186.91s/it]/home/dev25-01/mistral-env/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████  | 1501/1868 [77:59:06<19:03:56, 187.02s/it]{'loss': 1.6084, 'grad_norm': 0.0, 'learning_rate': 3.950749464668095e-06, 'epoch': 0.8}
? Step 1501  Epoch: 0.80  Loss: 1.5704
                                                           80%|████████  | 1501/1868 [77:59:06<19:03:56, 187.02s/it] 80%|████████  | 1502/1868 [78:02:12<19:00:36, 186.98s/it]{'loss': 1.5704, 'grad_norm': 0.0, 'learning_rate': 3.9400428265524625e-06, 'epoch': 0.8}
? Step 1502  Epoch: 0.80  Loss: 1.5258
                                                           80%|████████  | 1502/1868 [78:02:12<19:00:36, 186.98s/it] 80%|████████  | 1503/1868 [78:05:19<18:56:36, 186.84s/it]{'loss': 1.5258, 'grad_norm': 0.0, 'learning_rate': 3.929336188436831e-06, 'epoch': 0.8}
? Step 1503  Epoch: 0.80  Loss: 2.3489
                                                           80%|████████  | 1503/1868 [78:05:19<18:56:36, 186.84s/it] 81%|████████  | 1504/1868 [78:08:26<18:54:18, 186.97s/it]{'loss': 2.3489, 'grad_norm': 0.0, 'learning_rate': 3.9186295503212e-06, 'epoch': 0.8}
? Step 1504  Epoch: 0.81  Loss: 1.3257
                                                           81%|████████  | 1504/1868 [78:08:26<18:54:18, 186.97s/it] 81%|████████  | 1505/1868 [78:11:33<18:50:26, 186.85s/it]{'loss': 1.3257, 'grad_norm': 0.0, 'learning_rate': 3.907922912205568e-06, 'epoch': 0.81}
? Step 1505  Epoch: 0.81  Loss: 1.8999
                                                           81%|████████  | 1505/1868 [78:11:33<18:50:26, 186.85s/it] 81%|████████  | 1506/1868 [78:14:40<18:47:07, 186.82s/it]{'loss': 1.8999, 'grad_norm': 0.0, 'learning_rate': 3.897216274089936e-06, 'epoch': 0.81}
? Step 1506  Epoch: 0.81  Loss: 1.9602
                                                           81%|████████  | 1506/1868 [78:14:40<18:47:07, 186.82s/it] 81%|████████  | 1507/1868 [78:17:46<18:44:13, 186.85s/it]{'loss': 1.9602, 'grad_norm': 0.0, 'learning_rate': 3.886509635974304e-06, 'epoch': 0.81}
? Step 1507  Epoch: 0.81  Loss: 1.8423
                                                           81%|████████  | 1507/1868 [78:17:46<18:44:13, 186.85s/it] 81%|████████  | 1508/1868 [78:20:53<18:41:17, 186.88s/it]{'loss': 1.8423, 'grad_norm': 0.0, 'learning_rate': 3.875802997858673e-06, 'epoch': 0.81}
? Step 1508  Epoch: 0.81  Loss: 1.5738
                                                           81%|████████  | 1508/1868 [78:20:53<18:41:17, 186.88s/it] 81%|████████  | 1509/1868 [78:24:01<18:39:02, 187.03s/it]{'loss': 1.5738, 'grad_norm': 0.0, 'learning_rate': 3.865096359743041e-06, 'epoch': 0.81}
? Step 1509  Epoch: 0.81  Loss: 1.4696
                                                           81%|████████  | 1509/1868 [78:24:01<18:39:02, 187.03s/it] 81%|████████  | 1510/1868 [78:27:08<18:35:51, 187.02s/it]{'loss': 1.4696, 'grad_norm': 0.0, 'learning_rate': 3.854389721627409e-06, 'epoch': 0.81}
? Step 1510  Epoch: 0.81  Loss: 1.6042
                                                           81%|████████  | 1510/1868 [78:27:08<18:35:51, 187.02s/it] 81%|████████  | 1511/1868 [78:30:15<18:32:47, 187.03s/it]{'loss': 1.6042, 'grad_norm': 0.0, 'learning_rate': 3.843683083511778e-06, 'epoch': 0.81}
? Step 1511  Epoch: 0.81  Loss: 1.3590
                                                           81%|████████  | 1511/1868 [78:30:15<18:32:47, 187.03s/it] 81%|████████  | 1512/1868 [78:33:22<18:29:48, 187.05s/it]{'loss': 1.359, 'grad_norm': 0.0, 'learning_rate': 3.832976445396146e-06, 'epoch': 0.81}
? Step 1512  Epoch: 0.81  Loss: 1.5137
                                                           81%|████████  | 1512/1868 [78:33:22<18:29:48, 187.05s/it] 81%|████████  | 1513/1868 [78:36:29<18:26:15, 186.97s/it]{'loss': 1.5137, 'grad_norm': 0.0, 'learning_rate': 3.8222698072805145e-06, 'epoch': 0.81}
? Step 1513  Epoch: 0.81  Loss: 1.8995
                                                           81%|████████  | 1513/1868 [78:36:29<18:26:15, 186.97s/it] 81%|████████  | 1514/1868 [78:39:36<18:23:51, 187.09s/it]{'loss': 1.8995, 'grad_norm': 0.0, 'learning_rate': 3.8115631691648823e-06, 'epoch': 0.81}
? Step 1514  Epoch: 0.81  Loss: 1.5465
                                                           81%|████████  | 1514/1868 [78:39:36<18:23:51, 187.09s/it] 81%|████████  | 1515/1868 [78:42:43<18:20:51, 187.12s/it]{'loss': 1.5465, 'grad_norm': 0.0, 'learning_rate': 3.8008565310492506e-06, 'epoch': 0.81}
? Step 1515  Epoch: 0.81  Loss: 1.7807
                                                           81%|████████  | 1515/1868 [78:42:43<18:20:51, 187.12s/it] 81%|████████  | 1516/1868 [78:45:50<18:17:32, 187.08s/it]{'loss': 1.7807, 'grad_norm': 0.0, 'learning_rate': 3.790149892933619e-06, 'epoch': 0.81}
? Step 1516  Epoch: 0.81  Loss: 1.8175
                                                           81%|████████  | 1516/1868 [78:45:50<18:17:32, 187.08s/it] 81%|████████  | 1517/1868 [78:48:57<18:14:21, 187.07s/it]{'loss': 1.8175, 'grad_norm': 0.0, 'learning_rate': 3.7794432548179875e-06, 'epoch': 0.81}
? Step 1517  Epoch: 0.81  Loss: 1.5064
                                                           81%|████████  | 1517/1868 [78:48:57<18:14:21, 187.07s/it] 81%|████████▏ | 1518/1868 [78:52:04<18:10:59, 187.03s/it]{'loss': 1.5064, 'grad_norm': 0.0, 'learning_rate': 3.7687366167023558e-06, 'epoch': 0.81}
? Step 1518  Epoch: 0.81  Loss: 1.5714
                                                           81%|████████▏ | 1518/1868 [78:52:04<18:10:59, 187.03s/it] 81%|████████▏ | 1519/1868 [78:55:11<18:08:09, 187.07s/it]{'loss': 1.5714, 'grad_norm': 0.0, 'learning_rate': 3.758029978586724e-06, 'epoch': 0.81}
? Step 1519  Epoch: 0.81  Loss: 1.4928
                                                           81%|████████▏ | 1519/1868 [78:55:11<18:08:09, 187.07s/it] 81%|████████▏ | 1520/1868 [78:58:19<18:05:06, 187.09s/it]{'loss': 1.4928, 'grad_norm': 0.0, 'learning_rate': 3.7473233404710923e-06, 'epoch': 0.81}
? Step 1520  Epoch: 0.81  Loss: 1.8425
                                                           81%|████████▏ | 1520/1868 [78:58:19<18:05:06, 187.09s/it] 81%|████████▏ | 1521/1868 [79:01:26<18:02:24, 187.16s/it]{'loss': 1.8425, 'grad_norm': 0.0, 'learning_rate': 3.7366167023554605e-06, 'epoch': 0.81}
? Step 1521  Epoch: 0.81  Loss: 1.4131
                                                           81%|████████▏ | 1521/1868 [79:01:26<18:02:24, 187.16s/it] 81%|████████▏ | 1522/1868 [79:04:33<17:59:35, 187.21s/it]{'loss': 1.4131, 'grad_norm': 0.0, 'learning_rate': 3.7259100642398288e-06, 'epoch': 0.81}
? Step 1522  Epoch: 0.81  Loss: 1.8545
                                                           81%|████████▏ | 1522/1868 [79:04:33<17:59:35, 187.21s/it] 82%|████████▏ | 1523/1868 [79:07:40<17:55:47, 187.09s/it]{'loss': 1.8545, 'grad_norm': 0.0, 'learning_rate': 3.715203426124197e-06, 'epoch': 0.81}
? Step 1523  Epoch: 0.82  Loss: 1.8575
                                                           82%|████████▏ | 1523/1868 [79:07:40<17:55:47, 187.09s/it] 82%|████████▏ | 1524/1868 [79:10:47<17:52:39, 187.09s/it]{'loss': 1.8575, 'grad_norm': 0.0, 'learning_rate': 3.7044967880085657e-06, 'epoch': 0.82}
? Step 1524  Epoch: 0.82  Loss: 1.3004
                                                           82%|████████▏ | 1524/1868 [79:10:47<17:52:39, 187.09s/it] 82%|████████▏ | 1525/1868 [79:13:55<17:51:02, 187.35s/it]{'loss': 1.3004, 'grad_norm': 0.0, 'learning_rate': 3.693790149892934e-06, 'epoch': 0.82}
? Step 1525  Epoch: 0.82  Loss: 1.4208
                                                           82%|████████▏ | 1525/1868 [79:13:55<17:51:02, 187.35s/it] 82%|████████▏ | 1526/1868 [79:17:02<17:47:57, 187.36s/it]{'loss': 1.4208, 'grad_norm': 0.0, 'learning_rate': 3.683083511777302e-06, 'epoch': 0.82}
? Step 1526  Epoch: 0.82  Loss: 1.4811
                                                           82%|████████▏ | 1526/1868 [79:17:02<17:47:57, 187.36s/it] 82%|████████▏ | 1527/1868 [79:20:10<17:44:54, 187.37s/it]{'loss': 1.4811, 'grad_norm': 0.0, 'learning_rate': 3.6723768736616704e-06, 'epoch': 0.82}
? Step 1527  Epoch: 0.82  Loss: 1.4761
                                                           82%|████████▏ | 1527/1868 [79:20:10<17:44:54, 187.37s/it] 82%|████████▏ | 1528/1868 [79:23:17<17:41:49, 187.38s/it]{'loss': 1.4761, 'grad_norm': 0.0, 'learning_rate': 3.6616702355460387e-06, 'epoch': 0.82}
? Step 1528  Epoch: 0.82  Loss: 1.6804
                                                           82%|████████▏ | 1528/1868 [79:23:17<17:41:49, 187.38s/it] 82%|████████▏ | 1529/1868 [79:26:24<17:38:15, 187.30s/it]{'loss': 1.6804, 'grad_norm': 0.0, 'learning_rate': 3.650963597430407e-06, 'epoch': 0.82}
? Step 1529  Epoch: 0.82  Loss: 1.4884
                                                           82%|████████▏ | 1529/1868 [79:26:24<17:38:15, 187.30s/it] 82%|████████▏ | 1530/1868 [79:29:32<17:35:34, 187.38s/it]{'loss': 1.4884, 'grad_norm': 0.0, 'learning_rate': 3.640256959314775e-06, 'epoch': 0.82}
? Step 1530  Epoch: 0.82  Loss: 1.8302
                                                           82%|████████▏ | 1530/1868 [79:29:32<17:35:34, 187.38s/it] 82%|████████▏ | 1531/1868 [79:32:39<17:32:36, 187.41s/it]{'loss': 1.8302, 'grad_norm': 0.0, 'learning_rate': 3.629550321199144e-06, 'epoch': 0.82}
? Step 1531  Epoch: 0.82  Loss: 1.5578
                                                           82%|████████▏ | 1531/1868 [79:32:39<17:32:36, 187.41s/it] 82%|████████▏ | 1532/1868 [79:35:47<17:29:07, 187.34s/it]{'loss': 1.5578, 'grad_norm': 0.0, 'learning_rate': 3.618843683083512e-06, 'epoch': 0.82}
? Step 1532  Epoch: 0.82  Loss: 1.3976
                                                           82%|████████▏ | 1532/1868 [79:35:47<17:29:07, 187.34s/it] 82%|████████▏ | 1533/1868 [79:38:54<17:26:28, 187.43s/it]{'loss': 1.3976, 'grad_norm': 0.0, 'learning_rate': 3.6081370449678803e-06, 'epoch': 0.82}
? Step 1533  Epoch: 0.82  Loss: 1.3842
                                                           82%|████████▏ | 1533/1868 [79:38:54<17:26:28, 187.43s/it] 82%|████████▏ | 1534/1868 [79:42:02<17:23:18, 187.42s/it]{'loss': 1.3842, 'grad_norm': 0.0, 'learning_rate': 3.5974304068522486e-06, 'epoch': 0.82}
? Step 1534  Epoch: 0.82  Loss: 1.7818
                                                           82%|████████▏ | 1534/1868 [79:42:02<17:23:18, 187.42s/it] 82%|████████▏ | 1535/1868 [79:45:09<17:20:14, 187.43s/it]{'loss': 1.7818, 'grad_norm': 0.0, 'learning_rate': 3.586723768736617e-06, 'epoch': 0.82}
? Step 1535  Epoch: 0.82  Loss: 1.5633
                                                           82%|████████▏ | 1535/1868 [79:45:09<17:20:14, 187.43s/it] 82%|████████▏ | 1536/1868 [79:48:17<17:17:08, 187.44s/it]{'loss': 1.5633, 'grad_norm': 0.0, 'learning_rate': 3.576017130620985e-06, 'epoch': 0.82}
? Step 1536  Epoch: 0.82  Loss: 1.8734
                                                           82%|████████▏ | 1536/1868 [79:48:17<17:17:08, 187.44s/it] 82%|████████▏ | 1537/1868 [79:51:24<17:13:25, 187.33s/it]{'loss': 1.8734, 'grad_norm': 0.0, 'learning_rate': 3.5653104925053538e-06, 'epoch': 0.82}
? Step 1537  Epoch: 0.82  Loss: 2.0943
                                                           82%|████████▏ | 1537/1868 [79:51:24<17:13:25, 187.33s/it] 82%|████████▏ | 1538/1868 [79:54:31<17:11:02, 187.46s/it]{'loss': 2.0943, 'grad_norm': 0.0, 'learning_rate': 3.554603854389722e-06, 'epoch': 0.82}
? Step 1538  Epoch: 0.82  Loss: 1.3852
                                                           82%|████████▏ | 1538/1868 [79:54:31<17:11:02, 187.46s/it] 82%|████████▏ | 1539/1868 [79:57:39<17:08:01, 187.48s/it]{'loss': 1.3852, 'grad_norm': 0.0, 'learning_rate': 3.5438972162740903e-06, 'epoch': 0.82}
? Step 1539  Epoch: 0.82  Loss: 1.7067
                                                           82%|████████▏ | 1539/1868 [79:57:39<17:08:01, 187.48s/it] 82%|████████▏ | 1540/1868 [80:00:47<17:05:26, 187.58s/it]{'loss': 1.7067, 'grad_norm': 0.0, 'learning_rate': 3.5331905781584585e-06, 'epoch': 0.82}
? Step 1540  Epoch: 0.82  Loss: 1.3857
                                                           82%|████████▏ | 1540/1868 [80:00:47<17:05:26, 187.58s/it] 82%|████████▏ | 1541/1868 [80:03:54<17:02:20, 187.59s/it]{'loss': 1.3857, 'grad_norm': 0.0, 'learning_rate': 3.5224839400428268e-06, 'epoch': 0.82}
? Step 1541  Epoch: 0.82  Loss: 1.3379
                                                           82%|████████▏ | 1541/1868 [80:03:54<17:02:20, 187.59s/it] 83%|████████▎ | 1542/1868 [80:07:02<16:59:12, 187.58s/it]{'loss': 1.3379, 'grad_norm': 0.0, 'learning_rate': 3.511777301927195e-06, 'epoch': 0.82}
? Step 1542  Epoch: 0.83  Loss: 1.5837
                                                           83%|████████▎ | 1542/1868 [80:07:02<16:59:12, 187.58s/it] 83%|████████▎ | 1543/1868 [80:10:09<16:56:05, 187.59s/it]{'loss': 1.5837, 'grad_norm': 0.0, 'learning_rate': 3.5010706638115632e-06, 'epoch': 0.83}
? Step 1543  Epoch: 0.83  Loss: 1.6782
                                                           83%|████████▎ | 1543/1868 [80:10:09<16:56:05, 187.59s/it] 83%|████████▎ | 1544/1868 [80:13:17<16:52:53, 187.57s/it]{'loss': 1.6782, 'grad_norm': 0.0, 'learning_rate': 3.490364025695932e-06, 'epoch': 0.83}
? Step 1544  Epoch: 0.83  Loss: 1.6875
                                                           83%|████████▎ | 1544/1868 [80:13:17<16:52:53, 187.57s/it] 83%|████████▎ | 1545/1868 [80:16:25<16:50:17, 187.67s/it]{'loss': 1.6875, 'grad_norm': 0.0, 'learning_rate': 3.4796573875803e-06, 'epoch': 0.83}
? Step 1545  Epoch: 0.83  Loss: 1.5155
                                                           83%|████████▎ | 1545/1868 [80:16:25<16:50:17, 187.67s/it] 83%|████████▎ | 1546/1868 [80:19:32<16:47:00, 187.64s/it]{'loss': 1.5155, 'grad_norm': 0.0, 'learning_rate': 3.4689507494646684e-06, 'epoch': 0.83}
? Step 1546  Epoch: 0.83  Loss: 1.4987
                                                           83%|████████▎ | 1546/1868 [80:19:32<16:47:00, 187.64s/it] 83%|████████▎ | 1547/1868 [80:22:40<16:43:51, 187.64s/it]{'loss': 1.4987, 'grad_norm': 0.0, 'learning_rate': 3.4582441113490367e-06, 'epoch': 0.83}
? Step 1547  Epoch: 0.83  Loss: 1.7655
                                                           83%|████████▎ | 1547/1868 [80:22:40<16:43:51, 187.64s/it] 83%|████████▎ | 1548/1868 [80:25:48<16:40:51, 187.66s/it]{'loss': 1.7655, 'grad_norm': 0.0, 'learning_rate': 3.447537473233405e-06, 'epoch': 0.83}
? Step 1548  Epoch: 0.83  Loss: 1.7295
                                                           83%|████████▎ | 1548/1868 [80:25:48<16:40:51, 187.66s/it] 83%|████████▎ | 1549/1868 [80:28:55<16:37:24, 187.60s/it]{'loss': 1.7295, 'grad_norm': 0.0, 'learning_rate': 3.436830835117773e-06, 'epoch': 0.83}
? Step 1549  Epoch: 0.83  Loss: 1.4783
                                                           83%|████████▎ | 1549/1868 [80:28:55<16:37:24, 187.60s/it] 83%|████████▎ | 1550/1868 [80:32:03<16:33:50, 187.52s/it]{'loss': 1.4783, 'grad_norm': 0.0, 'learning_rate': 3.4261241970021414e-06, 'epoch': 0.83}
? Step 1550  Epoch: 0.83  Loss: 1.9620
                                                           83%|████████▎ | 1550/1868 [80:32:03<16:33:50, 187.52s/it] 83%|████████▎ | 1551/1868 [80:35:10<16:30:42, 187.52s/it]{'loss': 1.962, 'grad_norm': 0.0, 'learning_rate': 3.41541755888651e-06, 'epoch': 0.83}
? Step 1551  Epoch: 0.83  Loss: 1.9642
                                                           83%|████████▎ | 1551/1868 [80:35:10<16:30:42, 187.52s/it] 83%|████████▎ | 1552/1868 [80:38:18<16:27:55, 187.58s/it]{'loss': 1.9642, 'grad_norm': 0.0, 'learning_rate': 3.4047109207708783e-06, 'epoch': 0.83}
? Step 1552  Epoch: 0.83  Loss: 1.5349
                                                           83%|████████▎ | 1552/1868 [80:38:18<16:27:55, 187.58s/it] 83%|████████▎ | 1553/1868 [80:41:25<16:24:39, 187.56s/it]{'loss': 1.5349, 'grad_norm': 0.0, 'learning_rate': 3.3940042826552466e-06, 'epoch': 0.83}
? Step 1553  Epoch: 0.83  Loss: 1.5329
                                                           83%|████████▎ | 1553/1868 [80:41:25<16:24:39, 187.56s/it] 83%|████████▎ | 1554/1868 [80:44:33<16:21:52, 187.62s/it]{'loss': 1.5329, 'grad_norm': 0.0, 'learning_rate': 3.383297644539615e-06, 'epoch': 0.83}
? Step 1554  Epoch: 0.83  Loss: 1.3361
                                                           83%|████████▎ | 1554/1868 [80:44:33<16:21:52, 187.62s/it] 83%|████████▎ | 1555/1868 [80:47:41<16:18:57, 187.66s/it]{'loss': 1.3361, 'grad_norm': 0.0, 'learning_rate': 3.372591006423983e-06, 'epoch': 0.83}
? Step 1555  Epoch: 0.83  Loss: 1.6266
                                                           83%|████████▎ | 1555/1868 [80:47:41<16:18:57, 187.66s/it] 83%|████████▎ | 1556/1868 [80:50:49<16:16:13, 187.73s/it]{'loss': 1.6266, 'grad_norm': 0.0, 'learning_rate': 3.3618843683083513e-06, 'epoch': 0.83}
? Step 1556  Epoch: 0.83  Loss: 1.6215
                                                           83%|████████▎ | 1556/1868 [80:50:49<16:16:13, 187.73s/it] 83%|████████▎ | 1557/1868 [80:53:57<16:13:15, 187.77s/it]{'loss': 1.6215, 'grad_norm': 0.0, 'learning_rate': 3.3511777301927196e-06, 'epoch': 0.83}
? Step 1557  Epoch: 0.83  Loss: 1.7911
                                                           83%|████████▎ | 1557/1868 [80:53:57<16:13:15, 187.77s/it] 83%|████████▎ | 1558/1868 [80:57:04<16:10:09, 187.77s/it]{'loss': 1.7911, 'grad_norm': 0.0, 'learning_rate': 3.3404710920770882e-06, 'epoch': 0.83}
? Step 1558  Epoch: 0.83  Loss: 1.4962
                                                           83%|████████▎ | 1558/1868 [80:57:04<16:10:09, 187.77s/it] 83%|████████▎ | 1559/1868 [81:00:12<16:06:49, 187.73s/it]{'loss': 1.4962, 'grad_norm': 0.0, 'learning_rate': 3.3297644539614565e-06, 'epoch': 0.83}
? Step 1559  Epoch: 0.83  Loss: 1.5481
                                                           83%|████████▎ | 1559/1868 [81:00:12<16:06:49, 187.73s/it] 84%|████████▎ | 1560/1868 [81:03:20<16:03:56, 187.78s/it]{'loss': 1.5481, 'grad_norm': 0.0, 'learning_rate': 3.3190578158458247e-06, 'epoch': 0.83}
? Step 1560  Epoch: 0.84  Loss: 1.4580
                                                           84%|████████▎ | 1560/1868 [81:03:20<16:03:56, 187.78s/it] 84%|████████▎ | 1561/1868 [81:06:27<16:00:24, 187.70s/it]{'loss': 1.458, 'grad_norm': 0.0, 'learning_rate': 3.308351177730193e-06, 'epoch': 0.84}
? Step 1561  Epoch: 0.84  Loss: 2.0314
                                                           84%|████████▎ | 1561/1868 [81:06:27<16:00:24, 187.70s/it] 84%|████████▎ | 1562/1868 [81:09:35<15:57:22, 187.72s/it]{'loss': 2.0314, 'grad_norm': 0.0, 'learning_rate': 3.2976445396145612e-06, 'epoch': 0.84}
? Step 1562  Epoch: 0.84  Loss: 1.4436
                                                           84%|████████▎ | 1562/1868 [81:09:35<15:57:22, 187.72s/it] 84%|████████▎ | 1563/1868 [81:12:43<15:54:08, 187.70s/it]{'loss': 1.4436, 'grad_norm': 0.0, 'learning_rate': 3.2869379014989295e-06, 'epoch': 0.84}
? Step 1563  Epoch: 0.84  Loss: 1.6360
                                                           84%|████████▎ | 1563/1868 [81:12:43<15:54:08, 187.70s/it] 84%|████████▎ | 1564/1868 [81:15:51<15:51:03, 187.71s/it]{'loss': 1.636, 'grad_norm': 0.0, 'learning_rate': 3.2762312633832977e-06, 'epoch': 0.84}
? Step 1564  Epoch: 0.84  Loss: 1.7773
                                                           84%|████████▎ | 1564/1868 [81:15:51<15:51:03, 187.71s/it] 84%|████████▍ | 1565/1868 [81:18:58<15:48:06, 187.74s/it]{'loss': 1.7773, 'grad_norm': 0.0, 'learning_rate': 3.2655246252676664e-06, 'epoch': 0.84}
? Step 1565  Epoch: 0.84  Loss: 1.3563
                                                           84%|████████▍ | 1565/1868 [81:18:58<15:48:06, 187.74s/it] 84%|████████▍ | 1566/1868 [81:22:06<15:45:18, 187.81s/it]{'loss': 1.3563, 'grad_norm': 0.0, 'learning_rate': 3.2548179871520347e-06, 'epoch': 0.84}
? Step 1566  Epoch: 0.84  Loss: 1.4938
                                                           84%|████████▍ | 1566/1868 [81:22:06<15:45:18, 187.81s/it] 84%|████████▍ | 1567/1868 [81:25:14<15:42:09, 187.81s/it]{'loss': 1.4938, 'grad_norm': 0.0, 'learning_rate': 3.244111349036403e-06, 'epoch': 0.84}
? Step 1567  Epoch: 0.84  Loss: 1.4886
                                                           84%|████████▍ | 1567/1868 [81:25:14<15:42:09, 187.81s/it] 84%|████████▍ | 1568/1868 [81:28:22<15:38:43, 187.75s/it]{'loss': 1.4886, 'grad_norm': 0.0, 'learning_rate': 3.233404710920771e-06, 'epoch': 0.84}
? Step 1568  Epoch: 0.84  Loss: 1.7759
                                                           84%|████████▍ | 1568/1868 [81:28:22<15:38:43, 187.75s/it] 84%|████████▍ | 1569/1868 [81:31:29<15:35:29, 187.73s/it]{'loss': 1.7759, 'grad_norm': 0.0, 'learning_rate': 3.2226980728051394e-06, 'epoch': 0.84}
? Step 1569  Epoch: 0.84  Loss: 1.6282
                                                           84%|████████▍ | 1569/1868 [81:31:29<15:35:29, 187.73s/it] 84%|████████▍ | 1570/1868 [81:34:37<15:32:32, 187.76s/it]{'loss': 1.6282, 'grad_norm': 0.0, 'learning_rate': 3.2119914346895077e-06, 'epoch': 0.84}
? Step 1570  Epoch: 0.84  Loss: 1.4098
                                                           84%|████████▍ | 1570/1868 [81:34:37<15:32:32, 187.76s/it] 84%|████████▍ | 1571/1868 [81:37:45<15:29:20, 187.75s/it]{'loss': 1.4098, 'grad_norm': 0.0, 'learning_rate': 3.201284796573876e-06, 'epoch': 0.84}
? Step 1571  Epoch: 0.84  Loss: 1.5011
                                                           84%|████████▍ | 1571/1868 [81:37:45<15:29:20, 187.75s/it] 84%|████████▍ | 1572/1868 [81:40:53<15:26:10, 187.74s/it]{'loss': 1.5011, 'grad_norm': 0.0, 'learning_rate': 3.1905781584582446e-06, 'epoch': 0.84}
? Step 1572  Epoch: 0.84  Loss: 1.8498
                                                           84%|████████▍ | 1572/1868 [81:40:53<15:26:10, 187.74s/it] 84%|████████▍ | 1573/1868 [81:44:01<15:23:35, 187.85s/it]{'loss': 1.8498, 'grad_norm': 0.0, 'learning_rate': 3.179871520342613e-06, 'epoch': 0.84}
? Step 1573  Epoch: 0.84  Loss: 1.4183
                                                           84%|████████▍ | 1573/1868 [81:44:01<15:23:35, 187.85s/it] 84%|████████▍ | 1574/1868 [81:47:09<15:20:33, 187.87s/it]{'loss': 1.4183, 'grad_norm': 0.0, 'learning_rate': 3.169164882226981e-06, 'epoch': 0.84}
? Step 1574  Epoch: 0.84  Loss: 1.3477
                                                           84%|████████▍ | 1574/1868 [81:47:09<15:20:33, 187.87s/it] 84%|████████▍ | 1575/1868 [81:50:17<15:17:37, 187.91s/it]{'loss': 1.3477, 'grad_norm': 0.0, 'learning_rate': 3.1584582441113493e-06, 'epoch': 0.84}
? Step 1575  Epoch: 0.84  Loss: 1.5262
                                                           84%|████████▍ | 1575/1868 [81:50:17<15:17:37, 187.91s/it] 84%|████████▍ | 1576/1868 [81:53:25<15:14:54, 187.99s/it]{'loss': 1.5262, 'grad_norm': 0.0, 'learning_rate': 3.1477516059957176e-06, 'epoch': 0.84}
? Step 1576  Epoch: 0.84  Loss: 1.5993
                                                           84%|████████▍ | 1576/1868 [81:53:25<15:14:54, 187.99s/it] 84%|████████▍ | 1577/1868 [81:56:33<15:11:43, 187.98s/it]{'loss': 1.5993, 'grad_norm': 0.0, 'learning_rate': 3.137044967880086e-06, 'epoch': 0.84}
? Step 1577  Epoch: 0.84  Loss: 1.3232
                                                           84%|████████▍ | 1577/1868 [81:56:33<15:11:43, 187.98s/it] 84%|████████▍ | 1578/1868 [81:59:41<15:08:40, 188.00s/it]{'loss': 1.3232, 'grad_norm': 0.0, 'learning_rate': 3.126338329764454e-06, 'epoch': 0.84}
? Step 1578  Epoch: 0.84  Loss: 1.5281
                                                           84%|████████▍ | 1578/1868 [81:59:41<15:08:40, 188.00s/it] 85%|████████▍ | 1579/1868 [82:02:49<15:05:40, 188.03s/it]{'loss': 1.5281, 'grad_norm': 0.0, 'learning_rate': 3.1156316916488227e-06, 'epoch': 0.84}
? Step 1579  Epoch: 0.85  Loss: 1.5035
                                                           85%|████████▍ | 1579/1868 [82:02:49<15:05:40, 188.03s/it] 85%|████████▍ | 1580/1868 [82:05:57<15:02:43, 188.07s/it]{'loss': 1.5035, 'grad_norm': 0.0, 'learning_rate': 3.104925053533191e-06, 'epoch': 0.85}
? Step 1580  Epoch: 0.85  Loss: 1.6527
                                                           85%|████████▍ | 1580/1868 [82:05:57<15:02:43, 188.07s/it] 85%|████████▍ | 1581/1868 [82:09:05<14:59:46, 188.11s/it]{'loss': 1.6527, 'grad_norm': 0.0, 'learning_rate': 3.0942184154175592e-06, 'epoch': 0.85}
? Step 1581  Epoch: 0.85  Loss: 1.5529
                                                           85%|████████▍ | 1581/1868 [82:09:05<14:59:46, 188.11s/it] 85%|████████▍ | 1582/1868 [82:12:14<14:56:50, 188.15s/it]{'loss': 1.5529, 'grad_norm': 0.0, 'learning_rate': 3.0835117773019275e-06, 'epoch': 0.85}
? Step 1582  Epoch: 0.85  Loss: 1.8345
                                                           85%|████████▍ | 1582/1868 [82:12:14<14:56:50, 188.15s/it] 85%|████████▍ | 1583/1868 [82:15:22<14:53:33, 188.12s/it]{'loss': 1.8345, 'grad_norm': 0.0, 'learning_rate': 3.0728051391862957e-06, 'epoch': 0.85}
? Step 1583  Epoch: 0.85  Loss: 1.3898
                                                           85%|████████▍ | 1583/1868 [82:15:22<14:53:33, 188.12s/it] 85%|████████▍ | 1584/1868 [82:18:30<14:50:25, 188.12s/it]{'loss': 1.3898, 'grad_norm': 0.0, 'learning_rate': 3.062098501070664e-06, 'epoch': 0.85}
? Step 1584  Epoch: 0.85  Loss: 1.5719
                                                           85%|████████▍ | 1584/1868 [82:18:30<14:50:25, 188.12s/it] 85%|████████▍ | 1585/1868 [82:21:38<14:47:23, 188.14s/it]{'loss': 1.5719, 'grad_norm': 0.0, 'learning_rate': 3.0513918629550327e-06, 'epoch': 0.85}
? Step 1585  Epoch: 0.85  Loss: 1.3604
                                                           85%|████████▍ | 1585/1868 [82:21:38<14:47:23, 188.14s/it] 85%|████████▍ | 1586/1868 [82:24:46<14:44:01, 188.09s/it]{'loss': 1.3604, 'grad_norm': 0.0, 'learning_rate': 3.040685224839401e-06, 'epoch': 0.85}
? Step 1586  Epoch: 0.85  Loss: 1.6187
                                                           85%|████████▍ | 1586/1868 [82:24:46<14:44:01, 188.09s/it] 85%|████████▍ | 1587/1868 [82:27:54<14:41:23, 188.20s/it]{'loss': 1.6187, 'grad_norm': 0.0, 'learning_rate': 3.029978586723769e-06, 'epoch': 0.85}
? Step 1587  Epoch: 0.85  Loss: 1.5228
                                                           85%|████████▍ | 1587/1868 [82:27:54<14:41:23, 188.20s/it] 85%|████████▌ | 1588/1868 [82:31:03<14:38:20, 188.22s/it]{'loss': 1.5228, 'grad_norm': 0.0, 'learning_rate': 3.0192719486081374e-06, 'epoch': 0.85}
? Step 1588  Epoch: 0.85  Loss: 1.8286
                                                           85%|████████▌ | 1588/1868 [82:31:03<14:38:20, 188.22s/it] 85%|████████▌ | 1589/1868 [82:34:10<14:34:32, 188.07s/it]{'loss': 1.8286, 'grad_norm': 0.0, 'learning_rate': 3.0085653104925056e-06, 'epoch': 0.85}
? Step 1589  Epoch: 0.85  Loss: 2.2033
                                                           85%|████████▌ | 1589/1868 [82:34:10<14:34:32, 188.07s/it] 85%|████████▌ | 1590/1868 [82:37:18<14:31:20, 188.06s/it]{'loss': 2.2033, 'grad_norm': 0.0, 'learning_rate': 2.997858672376874e-06, 'epoch': 0.85}
? Step 1590  Epoch: 0.85  Loss: 1.7353
                                                           85%|████████▌ | 1590/1868 [82:37:18<14:31:20, 188.06s/it] 85%|████████▌ | 1591/1868 [82:40:26<14:28:08, 188.04s/it]{'loss': 1.7353, 'grad_norm': 0.0, 'learning_rate': 2.987152034261242e-06, 'epoch': 0.85}
? Step 1591  Epoch: 0.85  Loss: 1.4803
                                                           85%|████████▌ | 1591/1868 [82:40:26<14:28:08, 188.04s/it] 85%|████████▌ | 1592/1868 [82:43:35<14:25:05, 188.06s/it]{'loss': 1.4803, 'grad_norm': 0.0, 'learning_rate': 2.976445396145611e-06, 'epoch': 0.85}
? Step 1592  Epoch: 0.85  Loss: 1.5328
                                                           85%|████████▌ | 1592/1868 [82:43:35<14:25:05, 188.06s/it] 85%|████████▌ | 1593/1868 [82:46:43<14:21:58, 188.07s/it]{'loss': 1.5328, 'grad_norm': 0.0, 'learning_rate': 2.965738758029979e-06, 'epoch': 0.85}
? Step 1593  Epoch: 0.85  Loss: 1.5477
                                                           85%|████████▌ | 1593/1868 [82:46:43<14:21:58, 188.07s/it] 85%|████████▌ | 1594/1868 [82:49:51<14:19:18, 188.17s/it]{'loss': 1.5477, 'grad_norm': 0.0, 'learning_rate': 2.9550321199143473e-06, 'epoch': 0.85}
? Step 1594  Epoch: 0.85  Loss: 1.5484
                                                           85%|████████▌ | 1594/1868 [82:49:51<14:19:18, 188.17s/it] 85%|████████▌ | 1595/1868 [82:52:59<14:16:21, 188.21s/it]{'loss': 1.5484, 'grad_norm': 0.0, 'learning_rate': 2.9443254817987156e-06, 'epoch': 0.85}
? Step 1595  Epoch: 0.85  Loss: 1.6809
                                                           85%|████████▌ | 1595/1868 [82:52:59<14:16:21, 188.21s/it] 85%|████████▌ | 1596/1868 [82:56:08<14:13:16, 188.22s/it]{'loss': 1.6809, 'grad_norm': 0.0, 'learning_rate': 2.933618843683084e-06, 'epoch': 0.85}
? Step 1596  Epoch: 0.85  Loss: 1.4586
                                                           85%|████████▌ | 1596/1868 [82:56:08<14:13:16, 188.22s/it] 85%|████████▌ | 1597/1868 [82:59:16<14:10:19, 188.26s/it]{'loss': 1.4586, 'grad_norm': 0.0, 'learning_rate': 2.922912205567452e-06, 'epoch': 0.85}
? Step 1597  Epoch: 0.85  Loss: 1.4092
                                                           85%|████████▌ | 1597/1868 [82:59:16<14:10:19, 188.26s/it] 86%|████████▌ | 1598/1868 [83:02:24<14:07:14, 188.28s/it]{'loss': 1.4092, 'grad_norm': 0.0, 'learning_rate': 2.9122055674518203e-06, 'epoch': 0.85}
? Step 1598  Epoch: 0.86  Loss: 1.5753
                                                           86%|████████▌ | 1598/1868 [83:02:24<14:07:14, 188.28s/it] 86%|████████▌ | 1599/1868 [83:05:33<14:04:15, 188.31s/it]{'loss': 1.5753, 'grad_norm': 0.0, 'learning_rate': 2.901498929336189e-06, 'epoch': 0.86}
? Step 1599  Epoch: 0.86  Loss: 1.5468
                                                           86%|████████▌ | 1599/1868 [83:05:33<14:04:15, 188.31s/it] 86%|████████▌ | 1600/1868 [83:08:41<14:00:53, 188.26s/it]{'loss': 1.5468, 'grad_norm': 0.0, 'learning_rate': 2.8907922912205572e-06, 'epoch': 0.86}
? Step 1600  Epoch: 0.86  Loss: 1.4234
                                                           86%|████████▌ | 1600/1868 [83:08:41<14:00:53, 188.26s/it]/home/dev25-01/mistral-env/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 86%|████████▌ | 1601/1868 [83:11:49<13:58:11, 188.36s/it]{'loss': 1.4234, 'grad_norm': 0.0, 'learning_rate': 2.8800856531049255e-06, 'epoch': 0.86}
? Step 1601  Epoch: 0.86  Loss: 1.5061
                                                           86%|████████▌ | 1601/1868 [83:11:49<13:58:11, 188.36s/it] 86%|████████▌ | 1602/1868 [83:14:58<13:54:56, 188.33s/it]{'loss': 1.5061, 'grad_norm': 0.0, 'learning_rate': 2.8693790149892937e-06, 'epoch': 0.86}
? Step 1602  Epoch: 0.86  Loss: 1.6262
                                                           86%|████████▌ | 1602/1868 [83:14:58<13:54:56, 188.33s/it] 86%|████████▌ | 1603/1868 [83:18:06<13:51:30, 188.27s/it]{'loss': 1.6262, 'grad_norm': 0.0, 'learning_rate': 2.858672376873662e-06, 'epoch': 0.86}
? Step 1603  Epoch: 0.86  Loss: 1.6781
                                                           86%|████████▌ | 1603/1868 [83:18:06<13:51:30, 188.27s/it] 86%|████████▌ | 1604/1868 [83:21:14<13:48:41, 188.34s/it]{'loss': 1.6781, 'grad_norm': 0.0, 'learning_rate': 2.8479657387580302e-06, 'epoch': 0.86}
? Step 1604  Epoch: 0.86  Loss: 1.6116
                                                           86%|████████▌ | 1604/1868 [83:21:14<13:48:41, 188.34s/it] 86%|████████▌ | 1605/1868 [83:24:22<13:45:13, 188.27s/it]{'loss': 1.6116, 'grad_norm': 0.0, 'learning_rate': 2.8372591006423985e-06, 'epoch': 0.86}
? Step 1605  Epoch: 0.86  Loss: 1.6933
                                                           86%|████████▌ | 1605/1868 [83:24:22<13:45:13, 188.27s/it] 86%|████████▌ | 1606/1868 [83:27:31<13:42:49, 188.43s/it]{'loss': 1.6933, 'grad_norm': 0.0, 'learning_rate': 2.826552462526767e-06, 'epoch': 0.86}
? Step 1606  Epoch: 0.86  Loss: 1.6029
                                                           86%|████████▌ | 1606/1868 [83:27:31<13:42:49, 188.43s/it] 86%|████████▌ | 1607/1868 [83:30:39<13:39:24, 188.37s/it]{'loss': 1.6029, 'grad_norm': 0.0, 'learning_rate': 2.8158458244111354e-06, 'epoch': 0.86}
? Step 1607  Epoch: 0.86  Loss: 1.8650
                                                           86%|████████▌ | 1607/1868 [83:30:39<13:39:24, 188.37s/it] 86%|████████▌ | 1608/1868 [83:33:48<13:36:07, 188.34s/it]{'loss': 1.865, 'grad_norm': 0.0, 'learning_rate': 2.8051391862955036e-06, 'epoch': 0.86}
? Step 1608  Epoch: 0.86  Loss: 2.1752
                                                           86%|████████▌ | 1608/1868 [83:33:48<13:36:07, 188.34s/it] 86%|████████▌ | 1609/1868 [83:36:56<13:33:24, 188.44s/it]{'loss': 2.1752, 'grad_norm': 0.0, 'learning_rate': 2.794432548179872e-06, 'epoch': 0.86}
? Step 1609  Epoch: 0.86  Loss: 1.4190
                                                           86%|████████▌ | 1609/1868 [83:36:56<13:33:24, 188.44s/it] 86%|████████▌ | 1610/1868 [83:40:05<13:30:44, 188.55s/it]{'loss': 1.419, 'grad_norm': 0.0, 'learning_rate': 2.78372591006424e-06, 'epoch': 0.86}
? Step 1610  Epoch: 0.86  Loss: 1.6495
                                                           86%|████████▌ | 1610/1868 [83:40:05<13:30:44, 188.55s/it] 86%|████████▌ | 1611/1868 [83:43:14<13:27:39, 188.56s/it]{'loss': 1.6495, 'grad_norm': 0.0, 'learning_rate': 2.7730192719486084e-06, 'epoch': 0.86}
? Step 1611  Epoch: 0.86  Loss: 1.6678
                                                           86%|████████▌ | 1611/1868 [83:43:14<13:27:39, 188.56s/it] 86%|████████▋ | 1612/1868 [83:46:22<13:24:39, 188.59s/it]{'loss': 1.6678, 'grad_norm': 0.0, 'learning_rate': 2.7623126338329766e-06, 'epoch': 0.86}
? Step 1612  Epoch: 0.86  Loss: 1.4179
                                                           86%|████████▋ | 1612/1868 [83:46:22<13:24:39, 188.59s/it] 86%|████████▋ | 1613/1868 [83:49:31<13:21:35, 188.61s/it]{'loss': 1.4179, 'grad_norm': 0.0, 'learning_rate': 2.7516059957173453e-06, 'epoch': 0.86}
? Step 1613  Epoch: 0.86  Loss: 1.7560
                                                           86%|████████▋ | 1613/1868 [83:49:31<13:21:35, 188.61s/it] 86%|████████▋ | 1614/1868 [83:52:40<13:18:15, 188.56s/it]{'loss': 1.756, 'grad_norm': 0.0, 'learning_rate': 2.7408993576017136e-06, 'epoch': 0.86}
? Step 1614  Epoch: 0.86  Loss: 1.7710
                                                           86%|████████▋ | 1614/1868 [83:52:40<13:18:15, 188.56s/it] 86%|████████▋ | 1615/1868 [83:55:48<13:15:05, 188.56s/it]{'loss': 1.771, 'grad_norm': 0.0, 'learning_rate': 2.730192719486082e-06, 'epoch': 0.86}
? Step 1615  Epoch: 0.86  Loss: 1.5924
                                                           86%|████████▋ | 1615/1868 [83:55:48<13:15:05, 188.56s/it] 87%|████████▋ | 1616/1868 [83:58:57<13:12:26, 188.68s/it]{'loss': 1.5924, 'grad_norm': 0.0, 'learning_rate': 2.71948608137045e-06, 'epoch': 0.86}
? Step 1616  Epoch: 0.87  Loss: 1.6004
                                                           87%|████████▋ | 1616/1868 [83:58:57<13:12:26, 188.68s/it] 87%|████████▋ | 1617/1868 [84:02:06<13:09:14, 188.66s/it]{'loss': 1.6004, 'grad_norm': 0.0, 'learning_rate': 2.7087794432548183e-06, 'epoch': 0.87}
? Step 1617  Epoch: 0.87  Loss: 1.7310
                                                           87%|████████▋ | 1617/1868 [84:02:06<13:09:14, 188.66s/it] 87%|████████▋ | 1618/1868 [84:05:15<13:06:22, 188.73s/it]{'loss': 1.731, 'grad_norm': 0.0, 'learning_rate': 2.6980728051391865e-06, 'epoch': 0.87}
? Step 1618  Epoch: 0.87  Loss: 1.4865
                                                           87%|████████▋ | 1618/1868 [84:05:15<13:06:22, 188.73s/it] 87%|████████▋ | 1619/1868 [84:08:23<13:03:14, 188.73s/it]{'loss': 1.4865, 'grad_norm': 0.0, 'learning_rate': 2.6873661670235544e-06, 'epoch': 0.87}
? Step 1619  Epoch: 0.87  Loss: 1.6087
                                                           87%|████████▋ | 1619/1868 [84:08:23<13:03:14, 188.73s/it] 87%|████████▋ | 1620/1868 [84:11:32<13:00:02, 188.72s/it]{'loss': 1.6087, 'grad_norm': 0.0, 'learning_rate': 2.6766595289079235e-06, 'epoch': 0.87}
? Step 1620  Epoch: 0.87  Loss: 1.5548
                                                           87%|████████▋ | 1620/1868 [84:11:32<13:00:02, 188.72s/it] 87%|████████▋ | 1621/1868 [84:14:41<12:56:59, 188.74s/it]{'loss': 1.5548, 'grad_norm': 0.0, 'learning_rate': 2.6659528907922917e-06, 'epoch': 0.87}
? Step 1621  Epoch: 0.87  Loss: 1.5574
                                                           87%|████████▋ | 1621/1868 [84:14:41<12:56:59, 188.74s/it] 87%|████████▋ | 1622/1868 [84:17:50<12:54:03, 188.80s/it]{'loss': 1.5574, 'grad_norm': 0.0, 'learning_rate': 2.65524625267666e-06, 'epoch': 0.87}
? Step 1622  Epoch: 0.87  Loss: 1.4739
                                                           87%|████████▋ | 1622/1868 [84:17:50<12:54:03, 188.80s/it] 87%|████████▋ | 1623/1868 [84:20:58<12:50:49, 188.78s/it]{'loss': 1.4739, 'grad_norm': 0.0, 'learning_rate': 2.6445396145610282e-06, 'epoch': 0.87}
? Step 1623  Epoch: 0.87  Loss: 1.5139
                                                           87%|████████▋ | 1623/1868 [84:20:58<12:50:49, 188.78s/it] 87%|████████▋ | 1624/1868 [84:24:07<12:47:54, 188.83s/it]{'loss': 1.5139, 'grad_norm': 0.0, 'learning_rate': 2.6338329764453965e-06, 'epoch': 0.87}
? Step 1624  Epoch: 0.87  Loss: 1.6095
                                                           87%|████████▋ | 1624/1868 [84:24:07<12:47:54, 188.83s/it] 87%|████████▋ | 1625/1868 [84:27:16<12:44:52, 188.86s/it]{'loss': 1.6095, 'grad_norm': 0.0, 'learning_rate': 2.6231263383297643e-06, 'epoch': 0.87}
? Step 1625  Epoch: 0.87  Loss: 1.4980
                                                           87%|████████▋ | 1625/1868 [84:27:16<12:44:52, 188.86s/it] 87%|████████▋ | 1626/1868 [84:30:26<12:42:09, 188.96s/it]{'loss': 1.498, 'grad_norm': 0.0, 'learning_rate': 2.6124197002141325e-06, 'epoch': 0.87}
? Step 1626  Epoch: 0.87  Loss: 1.5664
                                                           87%|████████▋ | 1626/1868 [84:30:26<12:42:09, 188.96s/it] 87%|████████▋ | 1627/1868 [84:33:35<12:39:14, 189.02s/it]{'loss': 1.5664, 'grad_norm': 0.0, 'learning_rate': 2.6017130620985016e-06, 'epoch': 0.87}
? Step 1627  Epoch: 0.87  Loss: 1.4168
                                                           87%|████████▋ | 1627/1868 [84:33:35<12:39:14, 189.02s/it] 87%|████████▋ | 1628/1868 [84:36:44<12:36:24, 189.10s/it]{'loss': 1.4168, 'grad_norm': 0.0, 'learning_rate': 2.59100642398287e-06, 'epoch': 0.87}
? Step 1628  Epoch: 0.87  Loss: 1.4999
                                                           87%|████████▋ | 1628/1868 [84:36:44<12:36:24, 189.10s/it] 87%|████████▋ | 1629/1868 [84:39:53<12:33:19, 189.12s/it]{'loss': 1.4999, 'grad_norm': 0.0, 'learning_rate': 2.580299785867238e-06, 'epoch': 0.87}
? Step 1629  Epoch: 0.87  Loss: 1.5941
                                                           87%|████████▋ | 1629/1868 [84:39:53<12:33:19, 189.12s/it] 87%|████████▋ | 1630/1868 [84:43:02<12:30:10, 189.12s/it]{'loss': 1.5941, 'grad_norm': 0.0, 'learning_rate': 2.5695931477516064e-06, 'epoch': 0.87}
? Step 1630  Epoch: 0.87  Loss: 1.4841
                                                           87%|████████▋ | 1630/1868 [84:43:02<12:30:10, 189.12s/it] 87%|████████▋ | 1631/1868 [84:46:11<12:26:45, 189.05s/it]{'loss': 1.4841, 'grad_norm': 0.0, 'learning_rate': 2.558886509635974e-06, 'epoch': 0.87}
? Step 1631  Epoch: 0.87  Loss: 1.4787
                                                           87%|████████▋ | 1631/1868 [84:46:11<12:26:45, 189.05s/it] 87%|████████▋ | 1632/1868 [84:49:20<12:23:35, 189.05s/it]{'loss': 1.4787, 'grad_norm': 0.0, 'learning_rate': 2.5481798715203425e-06, 'epoch': 0.87}
? Step 1632  Epoch: 0.87  Loss: 1.4198
                                                           87%|████████▋ | 1632/1868 [84:49:20<12:23:35, 189.05s/it] 87%|████████▋ | 1633/1868 [84:52:29<12:20:38, 189.10s/it]{'loss': 1.4198, 'grad_norm': 0.0, 'learning_rate': 2.5374732334047107e-06, 'epoch': 0.87}
? Step 1633  Epoch: 0.87  Loss: 1.6857
                                                           87%|████████▋ | 1633/1868 [84:52:29<12:20:38, 189.10s/it] 87%|████████▋ | 1634/1868 [84:55:38<12:17:15, 189.04s/it]{'loss': 1.6857, 'grad_norm': 0.0, 'learning_rate': 2.52676659528908e-06, 'epoch': 0.87}
? Step 1634  Epoch: 0.87  Loss: 1.5745
                                                           87%|████████▋ | 1634/1868 [84:55:38<12:17:15, 189.04s/it] 88%|████████▊ | 1635/1868 [84:58:48<12:14:25, 189.12s/it]{'loss': 1.5745, 'grad_norm': 0.0, 'learning_rate': 2.516059957173448e-06, 'epoch': 0.87}
? Step 1635  Epoch: 0.88  Loss: 1.4838
                                                           88%|████████▊ | 1635/1868 [84:58:48<12:14:25, 189.12s/it] 88%|████████▊ | 1636/1868 [85:01:57<12:11:17, 189.13s/it]{'loss': 1.4838, 'grad_norm': 0.0, 'learning_rate': 2.5053533190578163e-06, 'epoch': 0.88}
? Step 1636  Epoch: 0.88  Loss: 1.5569
                                                           88%|████████▊ | 1636/1868 [85:01:57<12:11:17, 189.13s/it] 88%|████████▊ | 1637/1868 [85:05:06<12:08:04, 189.11s/it]{'loss': 1.5569, 'grad_norm': 0.0, 'learning_rate': 2.494646680942184e-06, 'epoch': 0.88}
? Step 1637  Epoch: 0.88  Loss: 1.5463
                                                           88%|████████▊ | 1637/1868 [85:05:06<12:08:04, 189.11s/it] 88%|████████▊ | 1638/1868 [85:08:15<12:04:37, 189.03s/it]{'loss': 1.5463, 'grad_norm': 0.0, 'learning_rate': 2.4839400428265524e-06, 'epoch': 0.88}
? Step 1638  Epoch: 0.88  Loss: 1.6041
                                                           88%|████████▊ | 1638/1868 [85:08:15<12:04:37, 189.03s/it] 88%|████████▊ | 1639/1868 [85:11:24<12:01:25, 189.02s/it]{'loss': 1.6041, 'grad_norm': 0.0, 'learning_rate': 2.473233404710921e-06, 'epoch': 0.88}
? Step 1639  Epoch: 0.88  Loss: 1.5561
                                                           88%|████████▊ | 1639/1868 [85:11:24<12:01:25, 189.02s/it] 88%|████████▊ | 1640/1868 [85:14:33<11:58:22, 189.04s/it]{'loss': 1.5561, 'grad_norm': 0.0, 'learning_rate': 2.4625267665952893e-06, 'epoch': 0.88}
? Step 1640  Epoch: 0.88  Loss: 1.4438
                                                           88%|████████▊ | 1640/1868 [85:14:33<11:58:22, 189.04s/it] 88%|████████▊ | 1641/1868 [85:17:42<11:55:12, 189.04s/it]{'loss': 1.4438, 'grad_norm': 0.0, 'learning_rate': 2.4518201284796575e-06, 'epoch': 0.88}
? Step 1641  Epoch: 0.88  Loss: 1.4799
                                                           88%|████████▊ | 1641/1868 [85:17:42<11:55:12, 189.04s/it] 88%|████████▊ | 1642/1868 [85:20:51<11:51:52, 188.99s/it]{'loss': 1.4799, 'grad_norm': 0.0, 'learning_rate': 2.441113490364026e-06, 'epoch': 0.88}
? Step 1642  Epoch: 0.88  Loss: 1.4537
                                                           88%|████████▊ | 1642/1868 [85:20:51<11:51:52, 188.99s/it] 88%|████████▊ | 1643/1868 [85:24:00<11:49:02, 189.08s/it]{'loss': 1.4537, 'grad_norm': 0.0, 'learning_rate': 2.430406852248394e-06, 'epoch': 0.88}
? Step 1643  Epoch: 0.88  Loss: 1.7298
                                                           88%|████████▊ | 1643/1868 [85:24:00<11:49:02, 189.08s/it] 88%|████████▊ | 1644/1868 [85:27:09<11:45:48, 189.06s/it]{'loss': 1.7298, 'grad_norm': 0.0, 'learning_rate': 2.4197002141327623e-06, 'epoch': 0.88}
? Step 1644  Epoch: 0.88  Loss: 1.5742
                                                           88%|████████▊ | 1644/1868 [85:27:09<11:45:48, 189.06s/it] 88%|████████▊ | 1645/1868 [85:30:18<11:42:28, 189.01s/it]{'loss': 1.5742, 'grad_norm': 0.0, 'learning_rate': 2.4089935760171305e-06, 'epoch': 0.88}
? Step 1645  Epoch: 0.88  Loss: 1.5017
                                                           88%|████████▊ | 1645/1868 [85:30:18<11:42:28, 189.01s/it] 88%|████████▊ | 1646/1868 [85:33:27<11:39:18, 189.00s/it]{'loss': 1.5017, 'grad_norm': 0.0, 'learning_rate': 2.398286937901499e-06, 'epoch': 0.88}
? Step 1646  Epoch: 0.88  Loss: 1.6837
                                                           88%|████████▊ | 1646/1868 [85:33:27<11:39:18, 189.00s/it] 88%|████████▊ | 1647/1868 [85:36:36<11:36:25, 189.07s/it]{'loss': 1.6837, 'grad_norm': 0.0, 'learning_rate': 2.3875802997858674e-06, 'epoch': 0.88}
? Step 1647  Epoch: 0.88  Loss: 1.5949
                                                           88%|████████▊ | 1647/1868 [85:36:36<11:36:25, 189.07s/it] 88%|████████▊ | 1648/1868 [85:39:45<11:33:06, 189.03s/it]{'loss': 1.5949, 'grad_norm': 0.0, 'learning_rate': 2.3768736616702357e-06, 'epoch': 0.88}
? Step 1648  Epoch: 0.88  Loss: 1.5031
                                                           88%|████████▊ | 1648/1868 [85:39:45<11:33:06, 189.03s/it] 88%|████████▊ | 1649/1868 [85:42:54<11:29:44, 188.97s/it]{'loss': 1.5031, 'grad_norm': 0.0, 'learning_rate': 2.366167023554604e-06, 'epoch': 0.88}
? Step 1649  Epoch: 0.88  Loss: 1.9670
                                                           88%|████████▊ | 1649/1868 [85:42:54<11:29:44, 188.97s/it] 88%|████████▊ | 1650/1868 [85:46:03<11:26:38, 188.98s/it]{'loss': 1.967, 'grad_norm': 0.0, 'learning_rate': 2.355460385438972e-06, 'epoch': 0.88}
? Step 1650  Epoch: 0.88  Loss: 1.6445
                                                           88%|████████▊ | 1650/1868 [85:46:03<11:26:38, 188.98s/it] 88%|████████▊ | 1651/1868 [85:49:12<11:23:20, 188.94s/it]{'loss': 1.6445, 'grad_norm': 0.0, 'learning_rate': 2.3447537473233404e-06, 'epoch': 0.88}
? Step 1651  Epoch: 0.88  Loss: 1.5979
                                                           88%|████████▊ | 1651/1868 [85:49:12<11:23:20, 188.94s/it] 88%|████████▊ | 1652/1868 [85:52:20<11:19:58, 188.88s/it]{'loss': 1.5979, 'grad_norm': 0.0, 'learning_rate': 2.334047109207709e-06, 'epoch': 0.88}
? Step 1652  Epoch: 0.88  Loss: 2.0136
                                                           88%|████████▊ | 1652/1868 [85:52:20<11:19:58, 188.88s/it] 88%|████████▊ | 1653/1868 [85:55:29<11:16:39, 188.83s/it]{'loss': 2.0136, 'grad_norm': 0.0, 'learning_rate': 2.3233404710920774e-06, 'epoch': 0.88}
? Step 1653  Epoch: 0.88  Loss: 1.6637
                                                           88%|████████▊ | 1653/1868 [85:55:29<11:16:39, 188.83s/it] 89%|████████▊ | 1654/1868 [85:58:38<11:13:31, 188.84s/it]{'loss': 1.6637, 'grad_norm': 0.0, 'learning_rate': 2.3126338329764456e-06, 'epoch': 0.88}
? Step 1654  Epoch: 0.89  Loss: 1.5320
                                                           89%|████████▊ | 1654/1868 [85:58:38<11:13:31, 188.84s/it] 89%|████████▊ | 1655/1868 [86:01:47<11:10:29, 188.87s/it]{'loss': 1.532, 'grad_norm': 0.0, 'learning_rate': 2.301927194860814e-06, 'epoch': 0.89}
? Step 1655  Epoch: 0.89  Loss: 1.4678
                                                           89%|████████▊ | 1655/1868 [86:01:47<11:10:29, 188.87s/it] 89%|████████▊ | 1656/1868 [86:04:56<11:07:37, 188.95s/it]{'loss': 1.4678, 'grad_norm': 0.0, 'learning_rate': 2.291220556745182e-06, 'epoch': 0.89}
? Step 1656  Epoch: 0.89  Loss: 1.4185
                                                           89%|████████▊ | 1656/1868 [86:04:56<11:07:37, 188.95s/it] 89%|████████▊ | 1657/1868 [86:08:05<11:04:42, 189.01s/it]{'loss': 1.4185, 'grad_norm': 0.0, 'learning_rate': 2.2805139186295504e-06, 'epoch': 0.89}
? Step 1657  Epoch: 0.89  Loss: 1.4639
                                                           89%|████████▊ | 1657/1868 [86:08:05<11:04:42, 189.01s/it] 89%|████████▉ | 1658/1868 [86:11:14<11:01:27, 188.99s/it]{'loss': 1.4639, 'grad_norm': 0.0, 'learning_rate': 2.2698072805139186e-06, 'epoch': 0.89}
? Step 1658  Epoch: 0.89  Loss: 1.7974
                                                           89%|████████▉ | 1658/1868 [86:11:14<11:01:27, 188.99s/it] 89%|████████▉ | 1659/1868 [86:14:23<10:58:19, 188.99s/it]{'loss': 1.7974, 'grad_norm': 0.0, 'learning_rate': 2.2591006423982873e-06, 'epoch': 0.89}
? Step 1659  Epoch: 0.89  Loss: 1.6051
                                                           89%|████████▉ | 1659/1868 [86:14:23<10:58:19, 188.99s/it] 89%|████████▉ | 1660/1868 [86:17:32<10:55:06, 188.97s/it]{'loss': 1.6051, 'grad_norm': 0.0, 'learning_rate': 2.2483940042826555e-06, 'epoch': 0.89}
? Step 1660  Epoch: 0.89  Loss: 1.7761
                                                           89%|████████▉ | 1660/1868 [86:17:32<10:55:06, 188.97s/it] 89%|████████▉ | 1661/1868 [86:20:41<10:51:58, 188.98s/it]{'loss': 1.7761, 'grad_norm': 0.0, 'learning_rate': 2.2376873661670238e-06, 'epoch': 0.89}
? Step 1661  Epoch: 0.89  Loss: 1.5817
                                                           89%|████████▉ | 1661/1868 [86:20:41<10:51:58, 188.98s/it] 89%|████████▉ | 1662/1868 [86:23:50<10:49:01, 189.04s/it]{'loss': 1.5817, 'grad_norm': 0.0, 'learning_rate': 2.226980728051392e-06, 'epoch': 0.89}
? Step 1662  Epoch: 0.89  Loss: 1.6562
                                                           89%|████████▉ | 1662/1868 [86:23:50<10:49:01, 189.04s/it] 89%|████████▉ | 1663/1868 [86:26:59<10:45:47, 189.01s/it]{'loss': 1.6562, 'grad_norm': 0.0, 'learning_rate': 2.2162740899357603e-06, 'epoch': 0.89}
? Step 1663  Epoch: 0.89  Loss: 1.5197
                                                           89%|████████▉ | 1663/1868 [86:26:59<10:45:47, 189.01s/it] 89%|████████▉ | 1664/1868 [86:30:08<10:42:24, 188.94s/it]{'loss': 1.5197, 'grad_norm': 0.0, 'learning_rate': 2.2055674518201285e-06, 'epoch': 0.89}
? Step 1664  Epoch: 0.89  Loss: 1.9148
                                                           89%|████████▉ | 1664/1868 [86:30:08<10:42:24, 188.94s/it] 89%|████████▉ | 1665/1868 [86:33:17<10:39:13, 188.93s/it]{'loss': 1.9148, 'grad_norm': 0.0, 'learning_rate': 2.1948608137044968e-06, 'epoch': 0.89}
? Step 1665  Epoch: 0.89  Loss: 1.5995
                                                           89%|████████▉ | 1665/1868 [86:33:17<10:39:13, 188.93s/it] 89%|████████▉ | 1666/1868 [86:36:26<10:35:45, 188.84s/it]{'loss': 1.5995, 'grad_norm': 0.0, 'learning_rate': 2.1841541755888654e-06, 'epoch': 0.89}
? Step 1666  Epoch: 0.89  Loss: 1.6018
                                                           89%|████████▉ | 1666/1868 [86:36:26<10:35:45, 188.84s/it] 89%|████████▉ | 1667/1868 [86:39:35<10:32:45, 188.88s/it]{'loss': 1.6018, 'grad_norm': 0.0, 'learning_rate': 2.1734475374732337e-06, 'epoch': 0.89}
? Step 1667  Epoch: 0.89  Loss: 1.6438
                                                           89%|████████▉ | 1667/1868 [86:39:35<10:32:45, 188.88s/it] 89%|████████▉ | 1668/1868 [86:42:43<10:29:26, 188.83s/it]{'loss': 1.6438, 'grad_norm': 0.0, 'learning_rate': 2.162740899357602e-06, 'epoch': 0.89}
? Step 1668  Epoch: 0.89  Loss: 1.5618
                                                           89%|████████▉ | 1668/1868 [86:42:43<10:29:26, 188.83s/it] 89%|████████▉ | 1669/1868 [86:45:52<10:26:39, 188.94s/it]{'loss': 1.5618, 'grad_norm': 0.0, 'learning_rate': 2.15203426124197e-06, 'epoch': 0.89}
? Step 1669  Epoch: 0.89  Loss: 1.3670
                                                           89%|████████▉ | 1669/1868 [86:45:52<10:26:39, 188.94s/it] 89%|████████▉ | 1670/1868 [86:49:01<10:23:10, 188.84s/it]{'loss': 1.367, 'grad_norm': 0.0, 'learning_rate': 2.1413276231263384e-06, 'epoch': 0.89}
? Step 1670  Epoch: 0.89  Loss: 1.9338
                                                           89%|████████▉ | 1670/1868 [86:49:01<10:23:10, 188.84s/it] 89%|████████▉ | 1671/1868 [86:52:10<10:20:03, 188.85s/it]{'loss': 1.9338, 'grad_norm': 0.0, 'learning_rate': 2.1306209850107067e-06, 'epoch': 0.89}
? Step 1671  Epoch: 0.89  Loss: 1.6201
                                                           89%|████████▉ | 1671/1868 [86:52:10<10:20:03, 188.85s/it] 90%|████████▉ | 1672/1868 [86:55:19<10:16:57, 188.87s/it]{'loss': 1.6201, 'grad_norm': 0.0, 'learning_rate': 2.119914346895075e-06, 'epoch': 0.89}
? Step 1672  Epoch: 0.90  Loss: 1.4599
                                                           90%|████████▉ | 1672/1868 [86:55:19<10:16:57, 188.87s/it] 90%|████████▉ | 1673/1868 [86:58:28<10:13:40, 188.82s/it]{'loss': 1.4599, 'grad_norm': 0.0, 'learning_rate': 2.1092077087794436e-06, 'epoch': 0.9}
? Step 1673  Epoch: 0.90  Loss: 1.7688
                                                           90%|████████▉ | 1673/1868 [86:58:28<10:13:40, 188.82s/it] 90%|████████▉ | 1674/1868 [87:01:37<10:10:43, 188.89s/it]{'loss': 1.7688, 'grad_norm': 0.0, 'learning_rate': 2.098501070663812e-06, 'epoch': 0.9}
? Step 1674  Epoch: 0.90  Loss: 1.5552
                                                           90%|████████▉ | 1674/1868 [87:01:37<10:10:43, 188.89s/it] 90%|████████▉ | 1675/1868 [87:04:45<10:07:34, 188.88s/it]{'loss': 1.5552, 'grad_norm': 0.0, 'learning_rate': 2.08779443254818e-06, 'epoch': 0.9}
? Step 1675  Epoch: 0.90  Loss: 1.5365
                                                           90%|████████▉ | 1675/1868 [87:04:45<10:07:34, 188.88s/it] 90%|████████▉ | 1676/1868 [87:07:55<10:04:38, 188.95s/it]{'loss': 1.5365, 'grad_norm': 0.0, 'learning_rate': 2.0770877944325484e-06, 'epoch': 0.9}
? Step 1676  Epoch: 0.90  Loss: 1.7277
                                                           90%|████████▉ | 1676/1868 [87:07:55<10:04:38, 188.95s/it] 90%|████████▉ | 1677/1868 [87:11:04<10:01:41, 189.01s/it]{'loss': 1.7277, 'grad_norm': 0.0, 'learning_rate': 2.0663811563169166e-06, 'epoch': 0.9}
? Step 1677  Epoch: 0.90  Loss: 1.5926
                                                           90%|████████▉ | 1677/1868 [87:11:04<10:01:41, 189.01s/it] 90%|████████▉ | 1678/1868 [87:14:13<9:58:35, 189.03s/it] {'loss': 1.5926, 'grad_norm': 0.0, 'learning_rate': 2.055674518201285e-06, 'epoch': 0.9}
? Step 1678  Epoch: 0.90  Loss: 1.6639
                                                          90%|████████▉ | 1678/1868 [87:14:13<9:58:35, 189.03s/it] 90%|████████▉ | 1679/1868 [87:17:22<9:55:18, 188.99s/it]{'loss': 1.6639, 'grad_norm': 0.0, 'learning_rate': 2.044967880085653e-06, 'epoch': 0.9}
? Step 1679  Epoch: 0.90  Loss: 1.5582
                                                          90%|████████▉ | 1679/1868 [87:17:22<9:55:18, 188.99s/it] 90%|████████▉ | 1680/1868 [87:20:30<9:51:53, 188.90s/it]{'loss': 1.5582, 'grad_norm': 0.0, 'learning_rate': 2.0342612419700218e-06, 'epoch': 0.9}
? Step 1680  Epoch: 0.90  Loss: 1.5053
                                                          90%|████████▉ | 1680/1868 [87:20:30<9:51:53, 188.90s/it] 90%|████████▉ | 1681/1868 [87:23:40<9:49:03, 189.00s/it]{'loss': 1.5053, 'grad_norm': 0.0, 'learning_rate': 2.02355460385439e-06, 'epoch': 0.9}
? Step 1681  Epoch: 0.90  Loss: 1.6185
                                                          90%|████████▉ | 1681/1868 [87:23:40<9:49:03, 189.00s/it] 90%|█████████ | 1682/1868 [87:26:49<9:46:11, 189.09s/it]{'loss': 1.6185, 'grad_norm': 0.0, 'learning_rate': 2.0128479657387583e-06, 'epoch': 0.9}
? Step 1682  Epoch: 0.90  Loss: 1.5084
                                                          90%|█████████ | 1682/1868 [87:26:49<9:46:11, 189.09s/it] 90%|█████████ | 1683/1868 [87:29:58<9:43:04, 189.10s/it]{'loss': 1.5084, 'grad_norm': 0.0, 'learning_rate': 2.0021413276231265e-06, 'epoch': 0.9}
? Step 1683  Epoch: 0.90  Loss: 1.7921
                                                          90%|█████████ | 1683/1868 [87:29:58<9:43:04, 189.10s/it] 90%|█████████ | 1684/1868 [87:33:07<9:39:48, 189.07s/it]{'loss': 1.7921, 'grad_norm': 0.0, 'learning_rate': 1.9914346895074948e-06, 'epoch': 0.9}
? Step 1684  Epoch: 0.90  Loss: 1.5716
                                                          90%|█████████ | 1684/1868 [87:33:07<9:39:48, 189.07s/it] 90%|█████████ | 1685/1868 [87:36:16<9:36:41, 189.08s/it]{'loss': 1.5716, 'grad_norm': 0.0, 'learning_rate': 1.980728051391863e-06, 'epoch': 0.9}
? Step 1685  Epoch: 0.90  Loss: 1.3680
                                                          90%|█████████ | 1685/1868 [87:36:16<9:36:41, 189.08s/it] 90%|█████████ | 1686/1868 [87:39:25<9:33:20, 189.01s/it]{'loss': 1.368, 'grad_norm': 0.0, 'learning_rate': 1.9700214132762313e-06, 'epoch': 0.9}
? Step 1686  Epoch: 0.90  Loss: 1.7330
                                                          90%|█████████ | 1686/1868 [87:39:25<9:33:20, 189.01s/it] 90%|█████████ | 1687/1868 [87:42:34<9:30:15, 189.03s/it]{'loss': 1.733, 'grad_norm': 0.0, 'learning_rate': 1.9593147751606e-06, 'epoch': 0.9}
? Step 1687  Epoch: 0.90  Loss: 1.4168
                                                          90%|█████████ | 1687/1868 [87:42:34<9:30:15, 189.03s/it] 90%|█████████ | 1688/1868 [87:45:43<9:27:15, 189.09s/it]{'loss': 1.4168, 'grad_norm': 0.0, 'learning_rate': 1.948608137044968e-06, 'epoch': 0.9}
? Step 1688  Epoch: 0.90  Loss: 1.4071
                                                          90%|█████████ | 1688/1868 [87:45:43<9:27:15, 189.09s/it] 90%|█████████ | 1689/1868 [87:48:52<9:24:06, 189.09s/it]{'loss': 1.4071, 'grad_norm': 0.0, 'learning_rate': 1.9379014989293364e-06, 'epoch': 0.9}
? Step 1689  Epoch: 0.90  Loss: 1.5680
                                                          90%|█████████ | 1689/1868 [87:48:52<9:24:06, 189.09s/it] 90%|█████████ | 1690/1868 [87:52:01<9:20:45, 189.02s/it]{'loss': 1.568, 'grad_norm': 0.0, 'learning_rate': 1.9271948608137047e-06, 'epoch': 0.9}
? Step 1690  Epoch: 0.90  Loss: 1.4141
                                                          90%|█████████ | 1690/1868 [87:52:01<9:20:45, 189.02s/it] 91%|█████████ | 1691/1868 [87:55:10<9:17:25, 188.96s/it]{'loss': 1.4141, 'grad_norm': 0.0, 'learning_rate': 1.916488222698073e-06, 'epoch': 0.9}
? Step 1691  Epoch: 0.91  Loss: 1.8665
                                                          91%|█████████ | 1691/1868 [87:55:10<9:17:25, 188.96s/it] 91%|█████████ | 1692/1868 [87:58:19<9:14:15, 188.95s/it]{'loss': 1.8665, 'grad_norm': 0.0, 'learning_rate': 1.9057815845824412e-06, 'epoch': 0.91}
? Step 1692  Epoch: 0.91  Loss: 1.7527
                                                          91%|█████████ | 1692/1868 [87:58:19<9:14:15, 188.95s/it] 91%|█████████ | 1693/1868 [88:01:28<9:11:11, 188.98s/it]{'loss': 1.7527, 'grad_norm': 0.0, 'learning_rate': 1.8950749464668094e-06, 'epoch': 0.91}
? Step 1693  Epoch: 0.91  Loss: 1.3469
                                                          91%|█████████ | 1693/1868 [88:01:28<9:11:11, 188.98s/it] 91%|█████████ | 1694/1868 [88:04:37<9:08:06, 189.01s/it]{'loss': 1.3469, 'grad_norm': 0.0, 'learning_rate': 1.8843683083511779e-06, 'epoch': 0.91}
? Step 1694  Epoch: 0.91  Loss: 1.4254
                                                          91%|█████████ | 1694/1868 [88:04:37<9:08:06, 189.01s/it] 91%|█████████ | 1695/1868 [88:07:46<9:04:51, 188.97s/it]{'loss': 1.4254, 'grad_norm': 0.0, 'learning_rate': 1.8736616702355461e-06, 'epoch': 0.91}
? Step 1695  Epoch: 0.91  Loss: 1.6451
                                                          91%|█████████ | 1695/1868 [88:07:46<9:04:51, 188.97s/it] 91%|█████████ | 1696/1868 [88:10:55<9:01:33, 188.91s/it]{'loss': 1.6451, 'grad_norm': 0.0, 'learning_rate': 1.8629550321199144e-06, 'epoch': 0.91}
? Step 1696  Epoch: 0.91  Loss: 1.5777
                                                          91%|█████████ | 1696/1868 [88:10:55<9:01:33, 188.91s/it] 91%|█████████ | 1697/1868 [88:14:04<8:58:30, 188.95s/it]{'loss': 1.5777, 'grad_norm': 0.0, 'learning_rate': 1.8522483940042828e-06, 'epoch': 0.91}
? Step 1697  Epoch: 0.91  Loss: 1.4960
                                                          91%|█████████ | 1697/1868 [88:14:04<8:58:30, 188.95s/it] 91%|█████████ | 1698/1868 [88:17:13<8:55:10, 188.89s/it]{'loss': 1.496, 'grad_norm': 0.0, 'learning_rate': 1.841541755888651e-06, 'epoch': 0.91}
? Step 1698  Epoch: 0.91  Loss: 1.4822
                                                          91%|█████████ | 1698/1868 [88:17:13<8:55:10, 188.89s/it] 91%|█████████ | 1699/1868 [88:20:22<8:52:15, 188.97s/it]{'loss': 1.4822, 'grad_norm': 0.0, 'learning_rate': 1.8308351177730193e-06, 'epoch': 0.91}
? Step 1699  Epoch: 0.91  Loss: 1.4545
                                                          91%|█████████ | 1699/1868 [88:20:22<8:52:15, 188.97s/it] 91%|█████████ | 1700/1868 [88:23:30<8:48:49, 188.87s/it]{'loss': 1.4545, 'grad_norm': 0.0, 'learning_rate': 1.8201284796573876e-06, 'epoch': 0.91}
? Step 1700  Epoch: 0.91  Loss: 1.5920
                                                          91%|█████████ | 1700/1868 [88:23:30<8:48:49, 188.87s/it]/home/dev25-01/mistral-env/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 91%|█████████ | 1701/1868 [88:26:40<8:45:59, 188.98s/it]{'loss': 1.592, 'grad_norm': 0.0, 'learning_rate': 1.809421841541756e-06, 'epoch': 0.91}
? Step 1701  Epoch: 0.91  Loss: 1.4637
                                                          91%|█████████ | 1701/1868 [88:26:40<8:45:59, 188.98s/it] 91%|█████████ | 1702/1868 [88:29:48<8:42:40, 188.92s/it]{'loss': 1.4637, 'grad_norm': 0.0, 'learning_rate': 1.7987152034261243e-06, 'epoch': 0.91}
? Step 1702  Epoch: 0.91  Loss: 1.5699
                                                          91%|█████████ | 1702/1868 [88:29:48<8:42:40, 188.92s/it] 91%|█████████ | 1703/1868 [88:32:57<8:39:38, 188.96s/it]{'loss': 1.5699, 'grad_norm': 0.0, 'learning_rate': 1.7880085653104925e-06, 'epoch': 0.91}
? Step 1703  Epoch: 0.91  Loss: 1.7179
                                                          91%|█████████ | 1703/1868 [88:32:57<8:39:38, 188.96s/it] 91%|█████████ | 1704/1868 [88:36:06<8:36:28, 188.95s/it]{'loss': 1.7179, 'grad_norm': 0.0, 'learning_rate': 1.777301927194861e-06, 'epoch': 0.91}
? Step 1704  Epoch: 0.91  Loss: 1.4634
                                                          91%|█████████ | 1704/1868 [88:36:06<8:36:28, 188.95s/it] 91%|█████████▏| 1705/1868 [88:39:16<8:33:31, 189.03s/it]{'loss': 1.4634, 'grad_norm': 0.0, 'learning_rate': 1.7665952890792293e-06, 'epoch': 0.91}
? Step 1705  Epoch: 0.91  Loss: 1.7257
                                                          91%|█████████▏| 1705/1868 [88:39:16<8:33:31, 189.03s/it] 91%|█████████▏| 1706/1868 [88:42:24<8:29:54, 188.86s/it]{'loss': 1.7257, 'grad_norm': 0.0, 'learning_rate': 1.7558886509635975e-06, 'epoch': 0.91}
? Step 1706  Epoch: 0.91  Loss: 2.0024
                                                          91%|█████████▏| 1706/1868 [88:42:24<8:29:54, 188.86s/it] 91%|█████████▏| 1707/1868 [88:45:33<8:26:34, 188.79s/it]{'loss': 2.0024, 'grad_norm': 0.0, 'learning_rate': 1.745182012847966e-06, 'epoch': 0.91}
? Step 1707  Epoch: 0.91  Loss: 1.6815
                                                          91%|█████████▏| 1707/1868 [88:45:33<8:26:34, 188.79s/it] 91%|█████████▏| 1708/1868 [88:48:42<8:23:34, 188.84s/it]{'loss': 1.6815, 'grad_norm': 0.0, 'learning_rate': 1.7344753747323342e-06, 'epoch': 0.91}
? Step 1708  Epoch: 0.91  Loss: 1.4343
                                                          91%|█████████▏| 1708/1868 [88:48:42<8:23:34, 188.84s/it] 91%|█████████▏| 1709/1868 [88:51:50<8:20:16, 188.79s/it]{'loss': 1.4343, 'grad_norm': 0.0, 'learning_rate': 1.7237687366167025e-06, 'epoch': 0.91}
? Step 1709  Epoch: 0.91  Loss: 1.7176
                                                          91%|█████████▏| 1709/1868 [88:51:50<8:20:16, 188.79s/it] 92%|█████████▏| 1710/1868 [88:54:59<8:16:53, 188.69s/it]{'loss': 1.7176, 'grad_norm': 0.0, 'learning_rate': 1.7130620985010707e-06, 'epoch': 0.91}
? Step 1710  Epoch: 0.92  Loss: 1.5141
                                                          92%|█████████▏| 1710/1868 [88:54:59<8:16:53, 188.69s/it] 92%|█████████▏| 1711/1868 [88:58:07<8:13:45, 188.70s/it]{'loss': 1.5141, 'grad_norm': 0.0, 'learning_rate': 1.7023554603854392e-06, 'epoch': 0.92}
? Step 1711  Epoch: 0.92  Loss: 1.4844
                                                          92%|█████████▏| 1711/1868 [88:58:07<8:13:45, 188.70s/it] 92%|█████████▏| 1712/1868 [89:01:16<8:10:50, 188.79s/it]{'loss': 1.4844, 'grad_norm': 0.0, 'learning_rate': 1.6916488222698074e-06, 'epoch': 0.92}
? Step 1712  Epoch: 0.92  Loss: 1.5240
                                                          92%|█████████▏| 1712/1868 [89:01:16<8:10:50, 188.79s/it] 92%|█████████▏| 1713/1868 [89:04:25<8:07:46, 188.82s/it]{'loss': 1.524, 'grad_norm': 0.0, 'learning_rate': 1.6809421841541757e-06, 'epoch': 0.92}
? Step 1713  Epoch: 0.92  Loss: 1.7281
                                                          92%|█████████▏| 1713/1868 [89:04:25<8:07:46, 188.82s/it] 92%|█████████▏| 1714/1868 [89:07:34<8:04:28, 188.76s/it]{'loss': 1.7281, 'grad_norm': 0.0, 'learning_rate': 1.6702355460385441e-06, 'epoch': 0.92}
? Step 1714  Epoch: 0.92  Loss: 1.8031
                                                          92%|█████████▏| 1714/1868 [89:07:34<8:04:28, 188.76s/it] 92%|█████████▏| 1715/1868 [89:10:43<8:01:12, 188.71s/it]{'loss': 1.8031, 'grad_norm': 0.0, 'learning_rate': 1.6595289079229124e-06, 'epoch': 0.92}
? Step 1715  Epoch: 0.92  Loss: 1.7111
                                                          92%|█████████▏| 1715/1868 [89:10:43<8:01:12, 188.71s/it] 92%|█████████▏| 1716/1868 [89:13:52<7:58:18, 188.81s/it]{'loss': 1.7111, 'grad_norm': 0.0, 'learning_rate': 1.6488222698072806e-06, 'epoch': 0.92}
? Step 1716  Epoch: 0.92  Loss: 1.4848
                                                          92%|█████████▏| 1716/1868 [89:13:52<7:58:18, 188.81s/it] 92%|█████████▏| 1717/1868 [89:17:00<7:54:50, 188.68s/it]{'loss': 1.4848, 'grad_norm': 0.0, 'learning_rate': 1.6381156316916489e-06, 'epoch': 0.92}
? Step 1717  Epoch: 0.92  Loss: 1.8578
                                                          92%|█████████▏| 1717/1868 [89:17:00<7:54:50, 188.68s/it] 92%|█████████▏| 1718/1868 [89:20:09<7:51:39, 188.67s/it]{'loss': 1.8578, 'grad_norm': 0.0, 'learning_rate': 1.6274089935760173e-06, 'epoch': 0.92}
? Step 1718  Epoch: 0.92  Loss: 1.7048
                                                          92%|█████████▏| 1718/1868 [89:20:09<7:51:39, 188.67s/it] 92%|█████████▏| 1719/1868 [89:23:17<7:48:12, 188.54s/it]{'loss': 1.7048, 'grad_norm': 0.0, 'learning_rate': 1.6167023554603856e-06, 'epoch': 0.92}
? Step 1719  Epoch: 0.92  Loss: 1.6392
                                                          92%|█████████▏| 1719/1868 [89:23:17<7:48:12, 188.54s/it] 92%|█████████▏| 1720/1868 [89:26:25<7:45:03, 188.54s/it]{'loss': 1.6392, 'grad_norm': 0.0, 'learning_rate': 1.6059957173447538e-06, 'epoch': 0.92}
? Step 1720  Epoch: 0.92  Loss: 1.5885
                                                          92%|█████████▏| 1720/1868 [89:26:25<7:45:03, 188.54s/it] 92%|█████████▏| 1721/1868 [89:29:34<7:41:50, 188.51s/it]{'loss': 1.5885, 'grad_norm': 0.0, 'learning_rate': 1.5952890792291223e-06, 'epoch': 0.92}
? Step 1721  Epoch: 0.92  Loss: 1.5073
                                                          92%|█████████▏| 1721/1868 [89:29:34<7:41:50, 188.51s/it] 92%|█████████▏| 1722/1868 [89:32:42<7:38:41, 188.51s/it]{'loss': 1.5073, 'grad_norm': 0.0, 'learning_rate': 1.5845824411134905e-06, 'epoch': 0.92}
? Step 1722  Epoch: 0.92  Loss: 1.5186
                                                          92%|█████████▏| 1722/1868 [89:32:42<7:38:41, 188.51s/it] 92%|█████████▏| 1723/1868 [89:35:51<7:35:27, 188.46s/it]{'loss': 1.5186, 'grad_norm': 0.0, 'learning_rate': 1.5738758029978588e-06, 'epoch': 0.92}
? Step 1723  Epoch: 0.92  Loss: 1.5777
                                                          92%|█████████▏| 1723/1868 [89:35:51<7:35:27, 188.46s/it] 92%|█████████▏| 1724/1868 [89:38:59<7:32:03, 188.36s/it]{'loss': 1.5777, 'grad_norm': 0.0, 'learning_rate': 1.563169164882227e-06, 'epoch': 0.92}
? Step 1724  Epoch: 0.92  Loss: 1.5382
                                                          92%|█████████▏| 1724/1868 [89:38:59<7:32:03, 188.36s/it] 92%|█████████▏| 1725/1868 [89:42:07<7:28:51, 188.33s/it]{'loss': 1.5382, 'grad_norm': 0.0, 'learning_rate': 1.5524625267665955e-06, 'epoch': 0.92}
? Step 1725  Epoch: 0.92  Loss: 1.6615
                                                          92%|█████████▏| 1725/1868 [89:42:07<7:28:51, 188.33s/it] 92%|█████████▏| 1726/1868 [89:45:15<7:25:34, 188.27s/it]{'loss': 1.6615, 'grad_norm': 0.0, 'learning_rate': 1.5417558886509637e-06, 'epoch': 0.92}
? Step 1726  Epoch: 0.92  Loss: 1.3531
                                                          92%|█████████▏| 1726/1868 [89:45:15<7:25:34, 188.27s/it] 92%|█████████▏| 1727/1868 [89:48:23<7:22:16, 188.20s/it]{'loss': 1.3531, 'grad_norm': 0.0, 'learning_rate': 1.531049250535332e-06, 'epoch': 0.92}
? Step 1727  Epoch: 0.92  Loss: 1.8088
                                                          92%|█████████▏| 1727/1868 [89:48:23<7:22:16, 188.20s/it] 93%|█████████▎| 1728/1868 [89:51:32<7:19:10, 188.22s/it]{'loss': 1.8088, 'grad_norm': 0.0, 'learning_rate': 1.5203426124197005e-06, 'epoch': 0.92}
? Step 1728  Epoch: 0.93  Loss: 1.5741
                                                          93%|█████████▎| 1728/1868 [89:51:32<7:19:10, 188.22s/it] 93%|█████████▎| 1729/1868 [89:54:39<7:15:52, 188.15s/it]{'loss': 1.5741, 'grad_norm': 0.0, 'learning_rate': 1.5096359743040687e-06, 'epoch': 0.93}
? Step 1729  Epoch: 0.93  Loss: 1.6222
                                                          93%|█████████▎| 1729/1868 [89:54:39<7:15:52, 188.15s/it] 93%|█████████▎| 1730/1868 [89:57:48<7:12:42, 188.13s/it]{'loss': 1.6222, 'grad_norm': 0.0, 'learning_rate': 1.498929336188437e-06, 'epoch': 0.93}
? Step 1730  Epoch: 0.93  Loss: 1.5044
                                                          93%|█████████▎| 1730/1868 [89:57:48<7:12:42, 188.13s/it] 93%|█████████▎| 1731/1868 [90:00:56<7:09:34, 188.13s/it]{'loss': 1.5044, 'grad_norm': 0.0, 'learning_rate': 1.4882226980728054e-06, 'epoch': 0.93}
? Step 1731  Epoch: 0.93  Loss: 1.6995
                                                          93%|█████████▎| 1731/1868 [90:00:56<7:09:34, 188.13s/it] 93%|█████████▎| 1732/1868 [90:04:04<7:06:17, 188.07s/it]{'loss': 1.6995, 'grad_norm': 0.0, 'learning_rate': 1.4775160599571737e-06, 'epoch': 0.93}
? Step 1732  Epoch: 0.93  Loss: 1.7818
                                                          93%|█████████▎| 1732/1868 [90:04:04<7:06:17, 188.07s/it] 93%|█████████▎| 1733/1868 [90:07:12<7:03:08, 188.07s/it]{'loss': 1.7818, 'grad_norm': 0.0, 'learning_rate': 1.466809421841542e-06, 'epoch': 0.93}
? Step 1733  Epoch: 0.93  Loss: 1.7484
                                                          93%|█████████▎| 1733/1868 [90:07:12<7:03:08, 188.07s/it] 93%|█████████▎| 1734/1868 [90:10:20<7:00:00, 188.07s/it]{'loss': 1.7484, 'grad_norm': 0.0, 'learning_rate': 1.4561027837259102e-06, 'epoch': 0.93}
? Step 1734  Epoch: 0.93  Loss: 1.7262
                                                          93%|█████████▎| 1734/1868 [90:10:20<7:00:00, 188.07s/it] 93%|█████████▎| 1735/1868 [90:13:28<6:57:01, 188.13s/it]{'loss': 1.7262, 'grad_norm': 0.0, 'learning_rate': 1.4453961456102786e-06, 'epoch': 0.93}
? Step 1735  Epoch: 0.93  Loss: 1.5343
                                                          93%|█████████▎| 1735/1868 [90:13:28<6:57:01, 188.13s/it] 93%|█████████▎| 1736/1868 [90:16:36<6:53:55, 188.15s/it]{'loss': 1.5343, 'grad_norm': 0.0, 'learning_rate': 1.4346895074946469e-06, 'epoch': 0.93}
? Step 1736  Epoch: 0.93  Loss: 1.6232
                                                          93%|█████████▎| 1736/1868 [90:16:36<6:53:55, 188.15s/it] 93%|█████████▎| 1737/1868 [90:19:44<6:50:40, 188.09s/it]{'loss': 1.6232, 'grad_norm': 0.0, 'learning_rate': 1.4239828693790151e-06, 'epoch': 0.93}
? Step 1737  Epoch: 0.93  Loss: 1.7168
                                                          93%|█████████▎| 1737/1868 [90:19:44<6:50:40, 188.09s/it] 93%|█████████▎| 1738/1868 [90:22:52<6:47:33, 188.10s/it]{'loss': 1.7168, 'grad_norm': 0.0, 'learning_rate': 1.4132762312633836e-06, 'epoch': 0.93}
? Step 1738  Epoch: 0.93  Loss: 1.6244
                                                          93%|█████████▎| 1738/1868 [90:22:52<6:47:33, 188.10s/it] 93%|█████████▎| 1739/1868 [90:26:00<6:44:27, 188.12s/it]{'loss': 1.6244, 'grad_norm': 0.0, 'learning_rate': 1.4025695931477518e-06, 'epoch': 0.93}
? Step 1739  Epoch: 0.93  Loss: 1.6100
                                                          93%|█████████▎| 1739/1868 [90:26:00<6:44:27, 188.12s/it] 93%|█████████▎| 1740/1868 [90:29:09<6:41:20, 188.13s/it]{'loss': 1.61, 'grad_norm': 0.0, 'learning_rate': 1.39186295503212e-06, 'epoch': 0.93}
? Step 1740  Epoch: 0.93  Loss: 1.5207
                                                          93%|█████████▎| 1740/1868 [90:29:09<6:41:20, 188.13s/it] 93%|█████████▎| 1741/1868 [90:32:17<6:38:09, 188.11s/it]{'loss': 1.5207, 'grad_norm': 0.0, 'learning_rate': 1.3811563169164883e-06, 'epoch': 0.93}
? Step 1741  Epoch: 0.93  Loss: 1.7098
                                                          93%|█████████▎| 1741/1868 [90:32:17<6:38:09, 188.11s/it] 93%|█████████▎| 1742/1868 [90:35:25<6:35:07, 188.16s/it]{'loss': 1.7098, 'grad_norm': 0.0, 'learning_rate': 1.3704496788008568e-06, 'epoch': 0.93}
? Step 1742  Epoch: 0.93  Loss: 1.6784
                                                          93%|█████████▎| 1742/1868 [90:35:25<6:35:07, 188.16s/it] 93%|█████████▎| 1743/1868 [90:38:33<6:31:44, 188.04s/it]{'loss': 1.6784, 'grad_norm': 0.0, 'learning_rate': 1.359743040685225e-06, 'epoch': 0.93}
? Step 1743  Epoch: 0.93  Loss: 1.8451
                                                          93%|█████████▎| 1743/1868 [90:38:33<6:31:44, 188.04s/it] 93%|█████████▎| 1744/1868 [90:41:41<6:28:33, 188.01s/it]{'loss': 1.8451, 'grad_norm': 0.0, 'learning_rate': 1.3490364025695933e-06, 'epoch': 0.93}
? Step 1744  Epoch: 0.93  Loss: 1.5605
                                                          93%|█████████▎| 1744/1868 [90:41:41<6:28:33, 188.01s/it] 93%|█████████▎| 1745/1868 [90:44:48<6:25:13, 187.92s/it]{'loss': 1.5605, 'grad_norm': 0.0, 'learning_rate': 1.3383297644539617e-06, 'epoch': 0.93}
? Step 1745  Epoch: 0.93  Loss: 1.6430
                                                          93%|█████████▎| 1745/1868 [90:44:48<6:25:13, 187.92s/it] 93%|█████████▎| 1746/1868 [90:47:56<6:22:03, 187.89s/it]{'loss': 1.643, 'grad_norm': 0.0, 'learning_rate': 1.32762312633833e-06, 'epoch': 0.93}
? Step 1746  Epoch: 0.93  Loss: 1.6646
                                                          93%|█████████▎| 1746/1868 [90:47:56<6:22:03, 187.89s/it] 94%|█████████▎| 1747/1868 [90:51:04<6:18:56, 187.91s/it]{'loss': 1.6646, 'grad_norm': 0.0, 'learning_rate': 1.3169164882226982e-06, 'epoch': 0.93}
? Step 1747  Epoch: 0.94  Loss: 1.8500
                                                          94%|█████████▎| 1747/1868 [90:51:04<6:18:56, 187.91s/it] 94%|█████████▎| 1748/1868 [90:54:12<6:15:55, 187.96s/it]{'loss': 1.85, 'grad_norm': 0.0, 'learning_rate': 1.3062098501070663e-06, 'epoch': 0.94}
? Step 1748  Epoch: 0.94  Loss: 2.0149
                                                          94%|█████████▎| 1748/1868 [90:54:12<6:15:55, 187.96s/it] 94%|█████████▎| 1749/1868 [90:57:20<6:12:44, 187.94s/it]{'loss': 2.0149, 'grad_norm': 0.0, 'learning_rate': 1.295503211991435e-06, 'epoch': 0.94}
? Step 1749  Epoch: 0.94  Loss: 1.8082
                                                          94%|█████████▎| 1749/1868 [90:57:20<6:12:44, 187.94s/it] 94%|█████████▎| 1750/1868 [91:00:28<6:09:31, 187.89s/it]{'loss': 1.8082, 'grad_norm': 0.0, 'learning_rate': 1.2847965738758032e-06, 'epoch': 0.94}
? Step 1750  Epoch: 0.94  Loss: 2.0948
                                                          94%|█████████▎| 1750/1868 [91:00:28<6:09:31, 187.89s/it] 94%|█████████▎| 1751/1868 [91:03:36<6:06:26, 187.92s/it]{'loss': 2.0948, 'grad_norm': 0.0, 'learning_rate': 1.2740899357601712e-06, 'epoch': 0.94}
? Step 1751  Epoch: 0.94  Loss: 1.5539
                                                          94%|█████████▎| 1751/1868 [91:03:36<6:06:26, 187.92s/it] 94%|█████████▍| 1752/1868 [91:06:44<6:03:08, 187.83s/it]{'loss': 1.5539, 'grad_norm': 0.0, 'learning_rate': 1.26338329764454e-06, 'epoch': 0.94}
? Step 1752  Epoch: 0.94  Loss: 2.0943
                                                          94%|█████████▍| 1752/1868 [91:06:44<6:03:08, 187.83s/it] 94%|█████████▍| 1753/1868 [91:09:52<6:00:06, 187.88s/it]{'loss': 2.0943, 'grad_norm': 0.0, 'learning_rate': 1.2526766595289081e-06, 'epoch': 0.94}
? Step 1753  Epoch: 0.94  Loss: 1.6483
                                                          94%|█████████▍| 1753/1868 [91:09:52<6:00:06, 187.88s/it] 94%|█████████▍| 1754/1868 [91:12:59<5:56:51, 187.82s/it]{'loss': 1.6483, 'grad_norm': 0.0, 'learning_rate': 1.2419700214132762e-06, 'epoch': 0.94}
? Step 1754  Epoch: 0.94  Loss: 1.9631
                                                          94%|█████████▍| 1754/1868 [91:12:59<5:56:51, 187.82s/it] 94%|█████████▍| 1755/1868 [91:16:07<5:53:55, 187.93s/it]{'loss': 1.9631, 'grad_norm': 0.0, 'learning_rate': 1.2312633832976446e-06, 'epoch': 0.94}
? Step 1755  Epoch: 0.94  Loss: 1.6744
                                                          94%|█████████▍| 1755/1868 [91:16:07<5:53:55, 187.93s/it] 94%|█████████▍| 1756/1868 [91:19:15<5:50:42, 187.88s/it]{'loss': 1.6744, 'grad_norm': 0.0, 'learning_rate': 1.220556745182013e-06, 'epoch': 0.94}
? Step 1756  Epoch: 0.94  Loss: 1.4310
                                                          94%|█████████▍| 1756/1868 [91:19:15<5:50:42, 187.88s/it] 94%|█████████▍| 1757/1868 [91:22:23<5:47:43, 187.96s/it]{'loss': 1.431, 'grad_norm': 0.0, 'learning_rate': 1.2098501070663811e-06, 'epoch': 0.94}
? Step 1757  Epoch: 0.94  Loss: 1.4625
                                                          94%|█████████▍| 1757/1868 [91:22:23<5:47:43, 187.96s/it] 94%|█████████▍| 1758/1868 [91:25:31<5:44:39, 187.99s/it]{'loss': 1.4625, 'grad_norm': 0.0, 'learning_rate': 1.1991434689507496e-06, 'epoch': 0.94}
? Step 1758  Epoch: 0.94  Loss: 1.5116
                                                          94%|█████████▍| 1758/1868 [91:25:31<5:44:39, 187.99s/it] 94%|█████████▍| 1759/1868 [91:28:39<5:41:17, 187.87s/it]{'loss': 1.5116, 'grad_norm': 0.0, 'learning_rate': 1.1884368308351178e-06, 'epoch': 0.94}
? Step 1759  Epoch: 0.94  Loss: 1.4248
                                                          94%|█████████▍| 1759/1868 [91:28:39<5:41:17, 187.87s/it] 94%|█████████▍| 1760/1868 [91:31:47<5:38:04, 187.82s/it]{'loss': 1.4248, 'grad_norm': 0.0, 'learning_rate': 1.177730192719486e-06, 'epoch': 0.94}
? Step 1760  Epoch: 0.94  Loss: 1.5061
                                                          94%|█████████▍| 1760/1868 [91:31:47<5:38:04, 187.82s/it] 94%|█████████▍| 1761/1868 [91:34:54<5:34:55, 187.81s/it]{'loss': 1.5061, 'grad_norm': 0.0, 'learning_rate': 1.1670235546038546e-06, 'epoch': 0.94}
? Step 1761  Epoch: 0.94  Loss: 1.4876
                                                          94%|█████████▍| 1761/1868 [91:34:54<5:34:55, 187.81s/it] 94%|█████████▍| 1762/1868 [91:38:02<5:31:50, 187.83s/it]{'loss': 1.4876, 'grad_norm': 0.0, 'learning_rate': 1.1563169164882228e-06, 'epoch': 0.94}
? Step 1762  Epoch: 0.94  Loss: 1.6216
                                                          94%|█████████▍| 1762/1868 [91:38:02<5:31:50, 187.83s/it] 94%|█████████▍| 1763/1868 [91:41:10<5:28:49, 187.90s/it]{'loss': 1.6216, 'grad_norm': 0.0, 'learning_rate': 1.145610278372591e-06, 'epoch': 0.94}
? Step 1763  Epoch: 0.94  Loss: 1.5535
                                                          94%|█████████▍| 1763/1868 [91:41:10<5:28:49, 187.90s/it] 94%|█████████▍| 1764/1868 [91:44:18<5:25:43, 187.92s/it]{'loss': 1.5535, 'grad_norm': 0.0, 'learning_rate': 1.1349036402569593e-06, 'epoch': 0.94}
? Step 1764  Epoch: 0.94  Loss: 1.5634
                                                          94%|█████████▍| 1764/1868 [91:44:18<5:25:43, 187.92s/it] 94%|█████████▍| 1765/1868 [91:47:26<5:22:29, 187.86s/it]{'loss': 1.5634, 'grad_norm': 0.0, 'learning_rate': 1.1241970021413278e-06, 'epoch': 0.94}
? Step 1765  Epoch: 0.94  Loss: 1.6762
                                                          94%|█████████▍| 1765/1868 [91:47:26<5:22:29, 187.86s/it] 95%|█████████▍| 1766/1868 [91:50:34<5:19:12, 187.77s/it]{'loss': 1.6762, 'grad_norm': 0.0, 'learning_rate': 1.113490364025696e-06, 'epoch': 0.94}
? Step 1766  Epoch: 0.95  Loss: 1.8063
                                                          95%|█████████▍| 1766/1868 [91:50:34<5:19:12, 187.77s/it] 95%|█████████▍| 1767/1868 [91:53:42<5:16:13, 187.85s/it]{'loss': 1.8063, 'grad_norm': 0.0, 'learning_rate': 1.1027837259100643e-06, 'epoch': 0.95}
? Step 1767  Epoch: 0.95  Loss: 1.5818
                                                          95%|█████████▍| 1767/1868 [91:53:42<5:16:13, 187.85s/it] 95%|█████████▍| 1768/1868 [91:56:49<5:13:00, 187.81s/it]{'loss': 1.5818, 'grad_norm': 0.0, 'learning_rate': 1.0920770877944327e-06, 'epoch': 0.95}
? Step 1768  Epoch: 0.95  Loss: 1.7372
                                                          95%|█████████▍| 1768/1868 [91:56:49<5:13:00, 187.81s/it] 95%|█████████▍| 1769/1868 [91:59:57<5:09:53, 187.82s/it]{'loss': 1.7372, 'grad_norm': 0.0, 'learning_rate': 1.081370449678801e-06, 'epoch': 0.95}
? Step 1769  Epoch: 0.95  Loss: 1.4213
                                                          95%|█████████▍| 1769/1868 [91:59:57<5:09:53, 187.82s/it] 95%|█████████▍| 1770/1868 [92:03:05<5:06:45, 187.81s/it]{'loss': 1.4213, 'grad_norm': 0.0, 'learning_rate': 1.0706638115631692e-06, 'epoch': 0.95}
? Step 1770  Epoch: 0.95  Loss: 1.5022
                                                          95%|█████████▍| 1770/1868 [92:03:05<5:06:45, 187.81s/it] 95%|█████████▍| 1771/1868 [92:06:13<5:03:30, 187.74s/it]{'loss': 1.5022, 'grad_norm': 0.0, 'learning_rate': 1.0599571734475375e-06, 'epoch': 0.95}
? Step 1771  Epoch: 0.95  Loss: 1.8159
                                                          95%|█████████▍| 1771/1868 [92:06:13<5:03:30, 187.74s/it] 95%|█████████▍| 1772/1868 [92:09:20<5:00:26, 187.77s/it]{'loss': 1.8159, 'grad_norm': 0.0, 'learning_rate': 1.049250535331906e-06, 'epoch': 0.95}
? Step 1772  Epoch: 0.95  Loss: 1.7954
                                                          95%|█████████▍| 1772/1868 [92:09:20<5:00:26, 187.77s/it] 95%|█████████▍| 1773/1868 [92:12:28<4:57:17, 187.77s/it]{'loss': 1.7954, 'grad_norm': 0.0, 'learning_rate': 1.0385438972162742e-06, 'epoch': 0.95}
? Step 1773  Epoch: 0.95  Loss: 1.8881
                                                          95%|█████████▍| 1773/1868 [92:12:28<4:57:17, 187.77s/it] 95%|█████████▍| 1774/1868 [92:15:36<4:54:20, 187.88s/it]{'loss': 1.8881, 'grad_norm': 0.0, 'learning_rate': 1.0278372591006424e-06, 'epoch': 0.95}
? Step 1774  Epoch: 0.95  Loss: 1.4063
                                                          95%|█████████▍| 1774/1868 [92:15:36<4:54:20, 187.88s/it] 95%|█████████▌| 1775/1868 [92:18:44<4:51:16, 187.92s/it]{'loss': 1.4063, 'grad_norm': 0.0, 'learning_rate': 1.0171306209850109e-06, 'epoch': 0.95}
? Step 1775  Epoch: 0.95  Loss: 1.5058
                                                          95%|█████████▌| 1775/1868 [92:18:44<4:51:16, 187.92s/it] 95%|█████████▌| 1776/1868 [92:21:52<4:48:08, 187.92s/it]{'loss': 1.5058, 'grad_norm': 0.0, 'learning_rate': 1.0064239828693791e-06, 'epoch': 0.95}
? Step 1776  Epoch: 0.95  Loss: 1.4530
                                                          95%|█████████▌| 1776/1868 [92:21:52<4:48:08, 187.92s/it] 95%|█████████▌| 1777/1868 [92:25:00<4:44:59, 187.91s/it]{'loss': 1.453, 'grad_norm': 0.0, 'learning_rate': 9.957173447537474e-07, 'epoch': 0.95}
? Step 1777  Epoch: 0.95  Loss: 1.7690
                                                          95%|█████████▌| 1777/1868 [92:25:00<4:44:59, 187.91s/it] 95%|█████████▌| 1778/1868 [92:28:08<4:41:57, 187.97s/it]{'loss': 1.769, 'grad_norm': 0.0, 'learning_rate': 9.850107066381156e-07, 'epoch': 0.95}
? Step 1778  Epoch: 0.95  Loss: 1.5467
                                                          95%|█████████▌| 1778/1868 [92:28:08<4:41:57, 187.97s/it] 95%|█████████▌| 1779/1868 [92:31:16<4:38:43, 187.90s/it]{'loss': 1.5467, 'grad_norm': 0.0, 'learning_rate': 9.74304068522484e-07, 'epoch': 0.95}
? Step 1779  Epoch: 0.95  Loss: 1.4902
                                                          95%|█████████▌| 1779/1868 [92:31:16<4:38:43, 187.90s/it] 95%|█████████▌| 1780/1868 [92:34:24<4:35:29, 187.83s/it]{'loss': 1.4902, 'grad_norm': 0.0, 'learning_rate': 9.635974304068523e-07, 'epoch': 0.95}
? Step 1780  Epoch: 0.95  Loss: 1.4412
                                                          95%|█████████▌| 1780/1868 [92:34:24<4:35:29, 187.83s/it] 95%|█████████▌| 1781/1868 [92:37:31<4:32:19, 187.81s/it]{'loss': 1.4412, 'grad_norm': 0.0, 'learning_rate': 9.528907922912206e-07, 'epoch': 0.95}
? Step 1781  Epoch: 0.95  Loss: 1.6042
                                                          95%|█████████▌| 1781/1868 [92:37:31<4:32:19, 187.81s/it] 95%|█████████▌| 1782/1868 [92:40:39<4:29:11, 187.81s/it]{'loss': 1.6042, 'grad_norm': 0.0, 'learning_rate': 9.421841541755889e-07, 'epoch': 0.95}
? Step 1782  Epoch: 0.95  Loss: 1.7244
                                                          95%|█████████▌| 1782/1868 [92:40:39<4:29:11, 187.81s/it] 95%|█████████▌| 1783/1868 [92:43:47<4:26:10, 187.88s/it]{'loss': 1.7244, 'grad_norm': 0.0, 'learning_rate': 9.314775160599572e-07, 'epoch': 0.95}
? Step 1783  Epoch: 0.95  Loss: 1.5647
                                                          95%|█████████▌| 1783/1868 [92:43:47<4:26:10, 187.88s/it] 96%|█████████▌| 1784/1868 [92:46:55<4:22:58, 187.84s/it]{'loss': 1.5647, 'grad_norm': 0.0, 'learning_rate': 9.207708779443255e-07, 'epoch': 0.95}
? Step 1784  Epoch: 0.96  Loss: 1.5813
                                                          96%|█████████▌| 1784/1868 [92:46:55<4:22:58, 187.84s/it] 96%|█████████▌| 1785/1868 [92:50:03<4:19:45, 187.78s/it]{'loss': 1.5813, 'grad_norm': 0.0, 'learning_rate': 9.100642398286938e-07, 'epoch': 0.96}
? Step 1785  Epoch: 0.96  Loss: 1.6058
                                                          96%|█████████▌| 1785/1868 [92:50:03<4:19:45, 187.78s/it] 96%|█████████▌| 1786/1868 [92:53:10<4:16:35, 187.75s/it]{'loss': 1.6058, 'grad_norm': 0.0, 'learning_rate': 8.993576017130621e-07, 'epoch': 0.96}
? Step 1786  Epoch: 0.96  Loss: 1.5656
                                                          96%|█████████▌| 1786/1868 [92:53:10<4:16:35, 187.75s/it] 96%|█████████▌| 1787/1868 [92:56:18<4:13:20, 187.66s/it]{'loss': 1.5656, 'grad_norm': 0.0, 'learning_rate': 8.886509635974305e-07, 'epoch': 0.96}
? Step 1787  Epoch: 0.96  Loss: 1.4945
                                                          96%|█████████▌| 1787/1868 [92:56:18<4:13:20, 187.66s/it] 96%|█████████▌| 1788/1868 [92:59:25<4:10:09, 187.62s/it]{'loss': 1.4945, 'grad_norm': 0.0, 'learning_rate': 8.779443254817988e-07, 'epoch': 0.96}
? Step 1788  Epoch: 0.96  Loss: 1.5655
                                                          96%|█████████▌| 1788/1868 [92:59:25<4:10:09, 187.62s/it] 96%|█████████▌| 1789/1868 [93:02:33<4:07:02, 187.62s/it]{'loss': 1.5655, 'grad_norm': 0.0, 'learning_rate': 8.672376873661671e-07, 'epoch': 0.96}
? Step 1789  Epoch: 0.96  Loss: 1.4754
                                                          96%|█████████▌| 1789/1868 [93:02:33<4:07:02, 187.62s/it] 96%|█████████▌| 1790/1868 [93:05:41<4:03:57, 187.66s/it]{'loss': 1.4754, 'grad_norm': 0.0, 'learning_rate': 8.565310492505354e-07, 'epoch': 0.96}
? Step 1790  Epoch: 0.96  Loss: 1.4110
                                                          96%|█████████▌| 1790/1868 [93:05:41<4:03:57, 187.66s/it] 96%|█████████▌| 1791/1868 [93:08:48<4:00:49, 187.65s/it]{'loss': 1.411, 'grad_norm': 0.0, 'learning_rate': 8.458244111349037e-07, 'epoch': 0.96}
? Step 1791  Epoch: 0.96  Loss: 1.6422
                                                          96%|█████████▌| 1791/1868 [93:08:48<4:00:49, 187.65s/it] 96%|█████████▌| 1792/1868 [93:11:56<3:57:36, 187.59s/it]{'loss': 1.6422, 'grad_norm': 0.0, 'learning_rate': 8.351177730192721e-07, 'epoch': 0.96}
? Step 1792  Epoch: 0.96  Loss: 1.5829
                                                          96%|█████████▌| 1792/1868 [93:11:56<3:57:36, 187.59s/it] 96%|█████████▌| 1793/1868 [93:15:04<3:54:36, 187.68s/it]{'loss': 1.5829, 'grad_norm': 0.0, 'learning_rate': 8.244111349036403e-07, 'epoch': 0.96}
? Step 1793  Epoch: 0.96  Loss: 1.5452
                                                          96%|█████████▌| 1793/1868 [93:15:04<3:54:36, 187.68s/it] 96%|█████████▌| 1794/1868 [93:18:11<3:51:23, 187.62s/it]{'loss': 1.5452, 'grad_norm': 0.0, 'learning_rate': 8.137044967880087e-07, 'epoch': 0.96}
? Step 1794  Epoch: 0.96  Loss: 1.6536
                                                          96%|█████████▌| 1794/1868 [93:18:11<3:51:23, 187.62s/it] 96%|█████████▌| 1795/1868 [93:21:19<3:48:10, 187.55s/it]{'loss': 1.6536, 'grad_norm': 0.0, 'learning_rate': 8.029978586723769e-07, 'epoch': 0.96}
? Step 1795  Epoch: 0.96  Loss: 1.4552
                                                          96%|█████████▌| 1795/1868 [93:21:19<3:48:10, 187.55s/it] 96%|█████████▌| 1796/1868 [93:24:26<3:45:08, 187.62s/it]{'loss': 1.4552, 'grad_norm': 0.0, 'learning_rate': 7.922912205567453e-07, 'epoch': 0.96}
? Step 1796  Epoch: 0.96  Loss: 1.5111
                                                          96%|█████████▌| 1796/1868 [93:24:26<3:45:08, 187.62s/it] 96%|█████████▌| 1797/1868 [93:27:34<3:42:07, 187.71s/it]{'loss': 1.5111, 'grad_norm': 0.0, 'learning_rate': 7.815845824411135e-07, 'epoch': 0.96}
? Step 1797  Epoch: 0.96  Loss: 1.5024
                                                          96%|█████████▌| 1797/1868 [93:27:34<3:42:07, 187.71s/it] 96%|█████████▋| 1798/1868 [93:30:42<3:39:01, 187.74s/it]{'loss': 1.5024, 'grad_norm': 0.0, 'learning_rate': 7.708779443254819e-07, 'epoch': 0.96}
? Step 1798  Epoch: 0.96  Loss: 1.6786
                                                          96%|█████████▋| 1798/1868 [93:30:42<3:39:01, 187.74s/it] 96%|█████████▋| 1799/1868 [93:33:50<3:35:50, 187.68s/it]{'loss': 1.6786, 'grad_norm': 0.0, 'learning_rate': 7.601713062098502e-07, 'epoch': 0.96}
? Step 1799  Epoch: 0.96  Loss: 1.7384
                                                          96%|█████████▋| 1799/1868 [93:33:50<3:35:50, 187.68s/it] 96%|█████████▋| 1800/1868 [93:36:57<3:32:46, 187.75s/it]{'loss': 1.7384, 'grad_norm': 0.0, 'learning_rate': 7.494646680942185e-07, 'epoch': 0.96}
? Step 1800  Epoch: 0.96  Loss: 1.6977
                                                          96%|█████████▋| 1800/1868 [93:36:57<3:32:46, 187.75s/it]/home/dev25-01/mistral-env/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 96%|█████████▋| 1801/1868 [93:40:05<3:29:38, 187.74s/it]{'loss': 1.6977, 'grad_norm': 0.0, 'learning_rate': 7.387580299785868e-07, 'epoch': 0.96}
? Step 1801  Epoch: 0.96  Loss: 1.6393
                                                          96%|█████████▋| 1801/1868 [93:40:05<3:29:38, 187.74s/it] 96%|█████████▋| 1802/1868 [93:43:13<3:26:28, 187.70s/it]{'loss': 1.6393, 'grad_norm': 0.0, 'learning_rate': 7.280513918629551e-07, 'epoch': 0.96}
? Step 1802  Epoch: 0.96  Loss: 1.6052
                                                          96%|█████████▋| 1802/1868 [93:43:13<3:26:28, 187.70s/it] 97%|█████████▋| 1803/1868 [93:46:21<3:23:23, 187.74s/it]{'loss': 1.6052, 'grad_norm': 0.0, 'learning_rate': 7.173447537473234e-07, 'epoch': 0.96}
? Step 1803  Epoch: 0.97  Loss: 1.5388
                                                          97%|█████████▋| 1803/1868 [93:46:21<3:23:23, 187.74s/it] 97%|█████████▋| 1804/1868 [93:49:28<3:20:13, 187.71s/it]{'loss': 1.5388, 'grad_norm': 0.0, 'learning_rate': 7.066381156316918e-07, 'epoch': 0.97}
? Step 1804  Epoch: 0.97  Loss: 1.4484
                                                          97%|█████████▋| 1804/1868 [93:49:28<3:20:13, 187.71s/it] 97%|█████████▋| 1805/1868 [93:52:36<3:17:07, 187.73s/it]{'loss': 1.4484, 'grad_norm': 0.0, 'learning_rate': 6.9593147751606e-07, 'epoch': 0.97}
? Step 1805  Epoch: 0.97  Loss: 1.5132
                                                          97%|█████████▋| 1805/1868 [93:52:36<3:17:07, 187.73s/it] 97%|█████████▋| 1806/1868 [93:55:44<3:13:59, 187.73s/it]{'loss': 1.5132, 'grad_norm': 0.0, 'learning_rate': 6.852248394004284e-07, 'epoch': 0.97}
? Step 1806  Epoch: 0.97  Loss: 1.6904
                                                          97%|█████████▋| 1806/1868 [93:55:44<3:13:59, 187.73s/it] 97%|█████████▋| 1807/1868 [93:58:51<3:10:49, 187.70s/it]{'loss': 1.6904, 'grad_norm': 0.0, 'learning_rate': 6.745182012847966e-07, 'epoch': 0.97}
? Step 1807  Epoch: 0.97  Loss: 1.6619
                                                          97%|█████████▋| 1807/1868 [93:58:51<3:10:49, 187.70s/it] 97%|█████████▋| 1808/1868 [94:01:59<3:07:43, 187.73s/it]{'loss': 1.6619, 'grad_norm': 0.0, 'learning_rate': 6.63811563169165e-07, 'epoch': 0.97}
? Step 1808  Epoch: 0.97  Loss: 1.3951
                                                          97%|█████████▋| 1808/1868 [94:01:59<3:07:43, 187.73s/it] 97%|█████████▋| 1809/1868 [94:05:07<3:04:37, 187.75s/it]{'loss': 1.3951, 'grad_norm': 0.0, 'learning_rate': 6.531049250535331e-07, 'epoch': 0.97}
? Step 1809  Epoch: 0.97  Loss: 1.6813
                                                          97%|█████████▋| 1809/1868 [94:05:07<3:04:37, 187.75s/it] 97%|█████████▋| 1810/1868 [94:08:15<3:01:25, 187.67s/it]{'loss': 1.6813, 'grad_norm': 0.0, 'learning_rate': 6.423982869379016e-07, 'epoch': 0.97}
? Step 1810  Epoch: 0.97  Loss: 1.5752
                                                          97%|█████████▋| 1810/1868 [94:08:15<3:01:25, 187.67s/it] 97%|█████████▋| 1811/1868 [94:11:22<2:58:19, 187.70s/it]{'loss': 1.5752, 'grad_norm': 0.0, 'learning_rate': 6.3169164882227e-07, 'epoch': 0.97}
? Step 1811  Epoch: 0.97  Loss: 1.5337
                                                          97%|█████████▋| 1811/1868 [94:11:22<2:58:19, 187.70s/it] 97%|█████████▋| 1812/1868 [94:14:30<2:55:07, 187.64s/it]{'loss': 1.5337, 'grad_norm': 0.0, 'learning_rate': 6.209850107066381e-07, 'epoch': 0.97}
? Step 1812  Epoch: 0.97  Loss: 1.5521
                                                          97%|█████████▋| 1812/1868 [94:14:30<2:55:07, 187.64s/it] 97%|█████████▋| 1813/1868 [94:17:37<2:51:59, 187.63s/it]{'loss': 1.5521, 'grad_norm': 0.0, 'learning_rate': 6.102783725910066e-07, 'epoch': 0.97}
? Step 1813  Epoch: 0.97  Loss: 1.5611
                                                          97%|█████████▋| 1813/1868 [94:17:37<2:51:59, 187.63s/it] 97%|█████████▋| 1814/1868 [94:20:45<2:48:52, 187.64s/it]{'loss': 1.5611, 'grad_norm': 0.0, 'learning_rate': 5.995717344753748e-07, 'epoch': 0.97}
? Step 1814  Epoch: 0.97  Loss: 1.7572
                                                          97%|█████████▋| 1814/1868 [94:20:45<2:48:52, 187.64s/it] 97%|█████████▋| 1815/1868 [94:23:53<2:45:48, 187.70s/it]{'loss': 1.7572, 'grad_norm': 0.0, 'learning_rate': 5.88865096359743e-07, 'epoch': 0.97}
? Step 1815  Epoch: 0.97  Loss: 1.7680
                                                          97%|█████████▋| 1815/1868 [94:23:53<2:45:48, 187.70s/it] 97%|█████████▋| 1816/1868 [94:27:01<2:42:41, 187.72s/it]{'loss': 1.768, 'grad_norm': 0.0, 'learning_rate': 5.781584582441114e-07, 'epoch': 0.97}
? Step 1816  Epoch: 0.97  Loss: 1.5012
                                                          97%|█████████▋| 1816/1868 [94:27:01<2:42:41, 187.72s/it] 97%|█████████▋| 1817/1868 [94:30:08<2:39:32, 187.70s/it]{'loss': 1.5012, 'grad_norm': 0.0, 'learning_rate': 5.674518201284797e-07, 'epoch': 0.97}
? Step 1817  Epoch: 0.97  Loss: 1.2822
                                                          97%|█████████▋| 1817/1868 [94:30:08<2:39:32, 187.70s/it] 97%|█████████▋| 1818/1868 [94:33:16<2:36:24, 187.69s/it]{'loss': 1.2822, 'grad_norm': 0.0, 'learning_rate': 5.56745182012848e-07, 'epoch': 0.97}
? Step 1818  Epoch: 0.97  Loss: 1.6673
                                                          97%|█████████▋| 1818/1868 [94:33:16<2:36:24, 187.69s/it] 97%|█████████▋| 1819/1868 [94:36:24<2:33:18, 187.73s/it]{'loss': 1.6673, 'grad_norm': 0.0, 'learning_rate': 5.460385438972164e-07, 'epoch': 0.97}
? Step 1819  Epoch: 0.97  Loss: 1.2770
                                                          97%|█████████▋| 1819/1868 [94:36:24<2:33:18, 187.73s/it] 97%|█████████▋| 1820/1868 [94:39:31<2:30:07, 187.65s/it]{'loss': 1.277, 'grad_norm': 0.0, 'learning_rate': 5.353319057815846e-07, 'epoch': 0.97}
? Step 1820  Epoch: 0.97  Loss: 1.4832
                                                          97%|█████████▋| 1820/1868 [94:39:31<2:30:07, 187.65s/it] 97%|█████████▋| 1821/1868 [94:42:39<2:26:59, 187.64s/it]{'loss': 1.4832, 'grad_norm': 0.0, 'learning_rate': 5.24625267665953e-07, 'epoch': 0.97}
? Step 1821  Epoch: 0.97  Loss: 1.6033
                                                          97%|█████████▋| 1821/1868 [94:42:39<2:26:59, 187.64s/it] 98%|█████████▊| 1822/1868 [94:45:46<2:23:49, 187.61s/it]{'loss': 1.6033, 'grad_norm': 0.0, 'learning_rate': 5.139186295503212e-07, 'epoch': 0.97}
? Step 1822  Epoch: 0.98  Loss: 1.5541
                                                          98%|█████████▊| 1822/1868 [94:45:46<2:23:49, 187.61s/it] 98%|█████████▊| 1823/1868 [94:48:54<2:20:44, 187.66s/it]{'loss': 1.5541, 'grad_norm': 0.0, 'learning_rate': 5.032119914346896e-07, 'epoch': 0.98}
? Step 1823  Epoch: 0.98  Loss: 1.6333
                                                          98%|█████████▊| 1823/1868 [94:48:54<2:20:44, 187.66s/it] 98%|█████████▊| 1824/1868 [94:52:02<2:17:39, 187.71s/it]{'loss': 1.6333, 'grad_norm': 0.0, 'learning_rate': 4.925053533190578e-07, 'epoch': 0.98}
? Step 1824  Epoch: 0.98  Loss: 1.3544
                                                          98%|█████████▊| 1824/1868 [94:52:02<2:17:39, 187.71s/it] 98%|█████████▊| 1825/1868 [94:55:10<2:14:29, 187.67s/it]{'loss': 1.3544, 'grad_norm': 0.0, 'learning_rate': 4.817987152034262e-07, 'epoch': 0.98}
? Step 1825  Epoch: 0.98  Loss: 1.4563
                                                          98%|█████████▊| 1825/1868 [94:55:10<2:14:29, 187.67s/it] 98%|█████████▊| 1826/1868 [94:58:17<2:11:22, 187.68s/it]{'loss': 1.4563, 'grad_norm': 0.0, 'learning_rate': 4.7109207708779447e-07, 'epoch': 0.98}
? Step 1826  Epoch: 0.98  Loss: 1.5006
                                                          98%|█████████▊| 1826/1868 [94:58:17<2:11:22, 187.68s/it] 98%|█████████▊| 1827/1868 [95:01:25<2:08:17, 187.74s/it]{'loss': 1.5006, 'grad_norm': 0.0, 'learning_rate': 4.6038543897216277e-07, 'epoch': 0.98}
? Step 1827  Epoch: 0.98  Loss: 1.4789
                                                          98%|█████████▊| 1827/1868 [95:01:25<2:08:17, 187.74s/it] 98%|█████████▊| 1828/1868 [95:04:33<2:05:10, 187.76s/it]{'loss': 1.4789, 'grad_norm': 0.0, 'learning_rate': 4.496788008565311e-07, 'epoch': 0.98}
? Step 1828  Epoch: 0.98  Loss: 1.5909
                                                          98%|█████████▊| 1828/1868 [95:04:33<2:05:10, 187.76s/it] 98%|█████████▊| 1829/1868 [95:07:41<2:02:03, 187.79s/it]{'loss': 1.5909, 'grad_norm': 0.0, 'learning_rate': 4.389721627408994e-07, 'epoch': 0.98}
? Step 1829  Epoch: 0.98  Loss: 1.8383
                                                          98%|█████████▊| 1829/1868 [95:07:41<2:02:03, 187.79s/it] 98%|█████████▊| 1830/1868 [95:10:48<1:58:53, 187.72s/it]{'loss': 1.8383, 'grad_norm': 0.0, 'learning_rate': 4.282655246252677e-07, 'epoch': 0.98}
? Step 1830  Epoch: 0.98  Loss: 1.6404
                                                          98%|█████████▊| 1830/1868 [95:10:48<1:58:53, 187.72s/it] 98%|█████████▊| 1831/1868 [95:13:56<1:55:43, 187.67s/it]{'loss': 1.6404, 'grad_norm': 0.0, 'learning_rate': 4.1755888650963603e-07, 'epoch': 0.98}
? Step 1831  Epoch: 0.98  Loss: 1.6164
                                                          98%|█████████▊| 1831/1868 [95:13:56<1:55:43, 187.67s/it] 98%|█████████▊| 1832/1868 [95:17:04<1:52:35, 187.65s/it]{'loss': 1.6164, 'grad_norm': 0.0, 'learning_rate': 4.0685224839400433e-07, 'epoch': 0.98}
? Step 1832  Epoch: 0.98  Loss: 1.8189
                                                          98%|█████████▊| 1832/1868 [95:17:04<1:52:35, 187.65s/it] 98%|█████████▊| 1833/1868 [95:20:11<1:49:28, 187.66s/it]{'loss': 1.8189, 'grad_norm': 0.0, 'learning_rate': 3.9614561027837263e-07, 'epoch': 0.98}
? Step 1833  Epoch: 0.98  Loss: 1.9137
                                                          98%|█████████▊| 1833/1868 [95:20:11<1:49:28, 187.66s/it] 98%|█████████▊| 1834/1868 [95:23:19<1:46:18, 187.60s/it]{'loss': 1.9137, 'grad_norm': 0.0, 'learning_rate': 3.8543897216274094e-07, 'epoch': 0.98}
? Step 1834  Epoch: 0.98  Loss: 1.6166
                                                          98%|█████████▊| 1834/1868 [95:23:19<1:46:18, 187.60s/it] 98%|█████████▊| 1835/1868 [95:26:26<1:43:10, 187.59s/it]{'loss': 1.6166, 'grad_norm': 0.0, 'learning_rate': 3.7473233404710924e-07, 'epoch': 0.98}
? Step 1835  Epoch: 0.98  Loss: 1.5344
                                                          98%|█████████▊| 1835/1868 [95:26:26<1:43:10, 187.59s/it] 98%|█████████▊| 1836/1868 [95:29:34<1:40:03, 187.60s/it]{'loss': 1.5344, 'grad_norm': 0.0, 'learning_rate': 3.6402569593147754e-07, 'epoch': 0.98}
? Step 1836  Epoch: 0.98  Loss: 1.6448
                                                          98%|█████████▊| 1836/1868 [95:29:34<1:40:03, 187.60s/it] 98%|█████████▊| 1837/1868 [95:32:42<1:36:58, 187.68s/it]{'loss': 1.6448, 'grad_norm': 0.0, 'learning_rate': 3.533190578158459e-07, 'epoch': 0.98}
? Step 1837  Epoch: 0.98  Loss: 1.4637
                                                          98%|█████████▊| 1837/1868 [95:32:42<1:36:58, 187.68s/it] 98%|█████████▊| 1838/1868 [95:35:49<1:33:49, 187.65s/it]{'loss': 1.4637, 'grad_norm': 0.0, 'learning_rate': 3.426124197002142e-07, 'epoch': 0.98}
? Step 1838  Epoch: 0.98  Loss: 1.6043
                                                          98%|█████████▊| 1838/1868 [95:35:49<1:33:49, 187.65s/it] 98%|█████████▊| 1839/1868 [95:38:57<1:30:41, 187.64s/it]{'loss': 1.6043, 'grad_norm': 0.0, 'learning_rate': 3.319057815845825e-07, 'epoch': 0.98}
? Step 1839  Epoch: 0.98  Loss: 1.4876
                                                          98%|█████████▊| 1839/1868 [95:38:57<1:30:41, 187.64s/it] 99%|█████████▊| 1840/1868 [95:42:05<1:27:34, 187.67s/it]{'loss': 1.4876, 'grad_norm': 0.0, 'learning_rate': 3.211991434689508e-07, 'epoch': 0.98}
? Step 1840  Epoch: 0.99  Loss: 1.6664
                                                          99%|█████████▊| 1840/1868 [95:42:05<1:27:34, 187.67s/it] 99%|█████████▊| 1841/1868 [95:45:12<1:24:26, 187.63s/it]{'loss': 1.6664, 'grad_norm': 0.0, 'learning_rate': 3.1049250535331905e-07, 'epoch': 0.99}
? Step 1841  Epoch: 0.99  Loss: 1.3522
                                                          99%|█████████▊| 1841/1868 [95:45:12<1:24:26, 187.63s/it] 99%|█████████▊| 1842/1868 [95:48:20<1:21:17, 187.60s/it]{'loss': 1.3522, 'grad_norm': 0.0, 'learning_rate': 2.997858672376874e-07, 'epoch': 0.99}
? Step 1842  Epoch: 0.99  Loss: 1.5727
                                                          99%|█████████▊| 1842/1868 [95:48:20<1:21:17, 187.60s/it] 99%|█████████▊| 1843/1868 [95:51:27<1:18:09, 187.60s/it]{'loss': 1.5727, 'grad_norm': 0.0, 'learning_rate': 2.890792291220557e-07, 'epoch': 0.99}
? Step 1843  Epoch: 0.99  Loss: 1.4294
                                                          99%|█████████▊| 1843/1868 [95:51:27<1:18:09, 187.60s/it] 99%|█████████▊| 1844/1868 [95:54:35<1:15:04, 187.71s/it]{'loss': 1.4294, 'grad_norm': 0.0, 'learning_rate': 2.78372591006424e-07, 'epoch': 0.99}
? Step 1844  Epoch: 0.99  Loss: 1.4826
                                                          99%|█████████▊| 1844/1868 [95:54:35<1:15:04, 187.71s/it] 99%|█████████▉| 1845/1868 [95:57:43<1:11:56, 187.69s/it]{'loss': 1.4826, 'grad_norm': 0.0, 'learning_rate': 2.676659528907923e-07, 'epoch': 0.99}
? Step 1845  Epoch: 0.99  Loss: 1.7325
                                                          99%|█████████▉| 1845/1868 [95:57:43<1:11:56, 187.69s/it] 99%|█████████▉| 1846/1868 [96:00:51<1:08:48, 187.65s/it]{'loss': 1.7325, 'grad_norm': 0.0, 'learning_rate': 2.569593147751606e-07, 'epoch': 0.99}
? Step 1846  Epoch: 0.99  Loss: 1.6481
                                                          99%|█████████▉| 1846/1868 [96:00:51<1:08:48, 187.65s/it] 99%|█████████▉| 1847/1868 [96:03:58<1:05:41, 187.70s/it]{'loss': 1.6481, 'grad_norm': 0.0, 'learning_rate': 2.462526766595289e-07, 'epoch': 0.99}
? Step 1847  Epoch: 0.99  Loss: 1.4014
                                                          99%|█████████▉| 1847/1868 [96:03:58<1:05:41, 187.70s/it] 99%|█████████▉| 1848/1868 [96:07:06<1:02:34, 187.70s/it]{'loss': 1.4014, 'grad_norm': 0.0, 'learning_rate': 2.3554603854389724e-07, 'epoch': 0.99}
? Step 1848  Epoch: 0.99  Loss: 1.5502
                                                          99%|█████████▉| 1848/1868 [96:07:06<1:02:34, 187.70s/it] 99%|█████████▉| 1849/1868 [96:10:14<59:25, 187.64s/it]  {'loss': 1.5502, 'grad_norm': 0.0, 'learning_rate': 2.2483940042826554e-07, 'epoch': 0.99}
? Step 1849  Epoch: 0.99  Loss: 1.4851
                                                        99%|█████████▉| 1849/1868 [96:10:14<59:25, 187.64s/it] 99%|█████████▉| 1850/1868 [96:13:21<56:16, 187.57s/it]{'loss': 1.4851, 'grad_norm': 0.0, 'learning_rate': 2.1413276231263384e-07, 'epoch': 0.99}
? Step 1850  Epoch: 0.99  Loss: 1.5165
                                                        99%|█████████▉| 1850/1868 [96:13:21<56:16, 187.57s/it] 99%|█████████▉| 1851/1868 [96:16:29<53:10, 187.68s/it]{'loss': 1.5165, 'grad_norm': 0.0, 'learning_rate': 2.0342612419700217e-07, 'epoch': 0.99}
? Step 1851  Epoch: 0.99  Loss: 1.3797
                                                        99%|█████████▉| 1851/1868 [96:16:29<53:10, 187.68s/it] 99%|█████████▉| 1852/1868 [96:19:36<50:01, 187.62s/it]{'loss': 1.3797, 'grad_norm': 0.0, 'learning_rate': 1.9271948608137047e-07, 'epoch': 0.99}
? Step 1852  Epoch: 0.99  Loss: 1.5640
                                                        99%|█████████▉| 1852/1868 [96:19:36<50:01, 187.62s/it] 99%|█████████▉| 1853/1868 [96:22:44<46:53, 187.54s/it]{'loss': 1.564, 'grad_norm': 0.0, 'learning_rate': 1.8201284796573877e-07, 'epoch': 0.99}
? Step 1853  Epoch: 0.99  Loss: 1.8473
                                                        99%|█████████▉| 1853/1868 [96:22:44<46:53, 187.54s/it] 99%|█████████▉| 1854/1868 [96:25:51<43:45, 187.53s/it]{'loss': 1.8473, 'grad_norm': 0.0, 'learning_rate': 1.713062098501071e-07, 'epoch': 0.99}
? Step 1854  Epoch: 0.99  Loss: 1.4235
                                                        99%|█████████▉| 1854/1868 [96:25:51<43:45, 187.53s/it] 99%|█████████▉| 1855/1868 [96:28:59<40:37, 187.51s/it]{'loss': 1.4235, 'grad_norm': 0.0, 'learning_rate': 1.605995717344754e-07, 'epoch': 0.99}
? Step 1855  Epoch: 0.99  Loss: 1.6450
                                                        99%|█████████▉| 1855/1868 [96:28:59<40:37, 187.51s/it] 99%|█████████▉| 1856/1868 [96:32:07<37:31, 187.65s/it]{'loss': 1.645, 'grad_norm': 0.0, 'learning_rate': 1.498929336188437e-07, 'epoch': 0.99}
? Step 1856  Epoch: 0.99  Loss: 1.6090
                                                        99%|█████████▉| 1856/1868 [96:32:07<37:31, 187.65s/it] 99%|█████████▉| 1857/1868 [96:35:14<34:23, 187.59s/it]{'loss': 1.609, 'grad_norm': 0.0, 'learning_rate': 1.39186295503212e-07, 'epoch': 0.99}
? Step 1857  Epoch: 0.99  Loss: 1.7356
                                                        99%|█████████▉| 1857/1868 [96:35:14<34:23, 187.59s/it] 99%|█████████▉| 1858/1868 [96:38:22<31:15, 187.56s/it]{'loss': 1.7356, 'grad_norm': 0.0, 'learning_rate': 1.284796573875803e-07, 'epoch': 0.99}
? Step 1858  Epoch: 0.99  Loss: 1.7876
                                                        99%|█████████▉| 1858/1868 [96:38:22<31:15, 187.56s/it]100%|█████████▉| 1859/1868 [96:41:29<28:08, 187.57s/it]{'loss': 1.7876, 'grad_norm': 0.0, 'learning_rate': 1.1777301927194862e-07, 'epoch': 0.99}
? Step 1859  Epoch: 1.00  Loss: 1.5338
                                                       100%|█████████▉| 1859/1868 [96:41:29<28:08, 187.57s/it]100%|█████████▉| 1860/1868 [96:44:37<24:59, 187.49s/it]{'loss': 1.5338, 'grad_norm': 0.0, 'learning_rate': 1.0706638115631692e-07, 'epoch': 1.0}
? Step 1860  Epoch: 1.00  Loss: 1.4631
                                                       100%|█████████▉| 1860/1868 [96:44:37<24:59, 187.49s/it]100%|█████████▉| 1861/1868 [96:47:44<21:52, 187.51s/it]{'loss': 1.4631, 'grad_norm': 0.0, 'learning_rate': 9.635974304068523e-08, 'epoch': 1.0}
? Step 1861  Epoch: 1.00  Loss: 1.4488
                                                       100%|█████████▉| 1861/1868 [96:47:44<21:52, 187.51s/it]100%|█████████▉| 1862/1868 [96:50:52<18:44, 187.49s/it]{'loss': 1.4488, 'grad_norm': 0.0, 'learning_rate': 8.565310492505355e-08, 'epoch': 1.0}
? Step 1862  Epoch: 1.00  Loss: 1.3402
                                                       100%|█████████▉| 1862/1868 [96:50:52<18:44, 187.49s/it]100%|█████████▉| 1863/1868 [96:53:59<15:37, 187.53s/it]{'loss': 1.3402, 'grad_norm': 0.0, 'learning_rate': 7.494646680942185e-08, 'epoch': 1.0}
? Step 1863  Epoch: 1.00  Loss: 1.8062
                                                       100%|█████████▉| 1863/1868 [96:53:59<15:37, 187.53s/it]100%|█████████▉| 1864/1868 [96:57:07<12:30, 187.56s/it]{'loss': 1.8062, 'grad_norm': 0.0, 'learning_rate': 6.423982869379015e-08, 'epoch': 1.0}
? Step 1864  Epoch: 1.00  Loss: 1.3653
                                                       100%|█████████▉| 1864/1868 [96:57:07<12:30, 187.56s/it]100%|█████████▉| 1865/1868 [97:00:14<09:22, 187.50s/it]{'loss': 1.3653, 'grad_norm': 0.0, 'learning_rate': 5.353319057815846e-08, 'epoch': 1.0}
? Step 1865  Epoch: 1.00  Loss: 1.6171
                                                       100%|█████████▉| 1865/1868 [97:00:14<09:22, 187.50s/it]100%|█████████▉| 1866/1868 [97:03:21<06:14, 187.40s/it]{'loss': 1.6171, 'grad_norm': 0.0, 'learning_rate': 4.2826552462526774e-08, 'epoch': 1.0}
? Step 1866  Epoch: 1.00  Loss: 1.8533
                                                       100%|█████████▉| 1866/1868 [97:03:21<06:14, 187.40s/it]100%|█████████▉| 1867/1868 [97:06:29<03:07, 187.40s/it]{'loss': 1.8533, 'grad_norm': 0.0, 'learning_rate': 3.2119914346895076e-08, 'epoch': 1.0}
? Step 1867  Epoch: 1.00  Loss: 1.4238
                                                       100%|█████████▉| 1867/1868 [97:06:29<03:07, 187.40s/it]100%|██████████| 1868/1868 [97:08:49<00:00, 173.40s/it]{'loss': 1.4238, 'grad_norm': 0.0, 'learning_rate': 2.1413276231263387e-08, 'epoch': 1.0}
? Step 1868  Epoch: 1.00  Loss: 1.6977
                                                       100%|██████████| 1868/1868 [97:08:49<00:00, 173.40s/it]                                                       100%|██████████| 1868/1868 [97:08:50<00:00, 173.40s/it]100%|██████████| 1868/1868 [97:08:50<00:00, 187.22s/it]
{'loss': 1.6977, 'grad_norm': 0.0, 'learning_rate': 1.0706638115631694e-08, 'epoch': 1.0}
{'train_runtime': 349730.247, 'train_samples_per_second': 0.171, 'train_steps_per_second': 0.005, 'train_loss': 1.6007552402815932, 'epoch': 1.0}
? Continued LoRA fine-tuning complete.
