{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 4194,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004768717215069146,
      "grad_norm": 1.0597742795944214,
      "learning_rate": 4.977348593228422e-05,
      "loss": 1.2947,
      "step": 20
    },
    {
      "epoch": 0.009537434430138292,
      "grad_norm": 1.2947362661361694,
      "learning_rate": 4.9535050071530756e-05,
      "loss": 1.2884,
      "step": 40
    },
    {
      "epoch": 0.01430615164520744,
      "grad_norm": 1.7191916704177856,
      "learning_rate": 4.92966142107773e-05,
      "loss": 1.1494,
      "step": 60
    },
    {
      "epoch": 0.019074868860276584,
      "grad_norm": 1.4722106456756592,
      "learning_rate": 4.905817835002385e-05,
      "loss": 1.0561,
      "step": 80
    },
    {
      "epoch": 0.023843586075345733,
      "grad_norm": 1.3879005908966064,
      "learning_rate": 4.8819742489270385e-05,
      "loss": 1.0707,
      "step": 100
    },
    {
      "epoch": 0.02861230329041488,
      "grad_norm": 1.0952510833740234,
      "learning_rate": 4.858130662851693e-05,
      "loss": 0.9605,
      "step": 120
    },
    {
      "epoch": 0.03338102050548403,
      "grad_norm": 0.9833084344863892,
      "learning_rate": 4.8342870767763476e-05,
      "loss": 1.1009,
      "step": 140
    },
    {
      "epoch": 0.03814973772055317,
      "grad_norm": 1.317730188369751,
      "learning_rate": 4.8104434907010015e-05,
      "loss": 0.9947,
      "step": 160
    },
    {
      "epoch": 0.04291845493562232,
      "grad_norm": 1.583817720413208,
      "learning_rate": 4.786599904625656e-05,
      "loss": 1.0013,
      "step": 180
    },
    {
      "epoch": 0.047687172150691466,
      "grad_norm": 1.8073344230651855,
      "learning_rate": 4.7627563185503105e-05,
      "loss": 1.0513,
      "step": 200
    },
    {
      "epoch": 0.05245588936576061,
      "grad_norm": 1.4739515781402588,
      "learning_rate": 4.7389127324749644e-05,
      "loss": 1.0139,
      "step": 220
    },
    {
      "epoch": 0.05722460658082976,
      "grad_norm": 1.268761396408081,
      "learning_rate": 4.715069146399619e-05,
      "loss": 1.0088,
      "step": 240
    },
    {
      "epoch": 0.061993323795898905,
      "grad_norm": 1.1705139875411987,
      "learning_rate": 4.691225560324273e-05,
      "loss": 0.9954,
      "step": 260
    },
    {
      "epoch": 0.06676204101096805,
      "grad_norm": 1.0941624641418457,
      "learning_rate": 4.6673819742489273e-05,
      "loss": 0.9433,
      "step": 280
    },
    {
      "epoch": 0.0715307582260372,
      "grad_norm": 1.0886039733886719,
      "learning_rate": 4.643538388173582e-05,
      "loss": 1.195,
      "step": 300
    },
    {
      "epoch": 0.07629947544110634,
      "grad_norm": 1.957524299621582,
      "learning_rate": 4.619694802098236e-05,
      "loss": 0.9985,
      "step": 320
    },
    {
      "epoch": 0.08106819265617549,
      "grad_norm": 1.278721809387207,
      "learning_rate": 4.59585121602289e-05,
      "loss": 1.0599,
      "step": 340
    },
    {
      "epoch": 0.08583690987124463,
      "grad_norm": 1.3714358806610107,
      "learning_rate": 4.572007629947545e-05,
      "loss": 0.913,
      "step": 360
    },
    {
      "epoch": 0.09060562708631378,
      "grad_norm": 1.4288010597229004,
      "learning_rate": 4.548164043872199e-05,
      "loss": 1.0598,
      "step": 380
    },
    {
      "epoch": 0.09537434430138293,
      "grad_norm": 1.7826805114746094,
      "learning_rate": 4.524320457796853e-05,
      "loss": 0.9521,
      "step": 400
    },
    {
      "epoch": 0.10014306151645208,
      "grad_norm": 1.124695897102356,
      "learning_rate": 4.500476871721507e-05,
      "loss": 0.9388,
      "step": 420
    },
    {
      "epoch": 0.10491177873152122,
      "grad_norm": 1.1629503965377808,
      "learning_rate": 4.476633285646161e-05,
      "loss": 1.0048,
      "step": 440
    },
    {
      "epoch": 0.10968049594659036,
      "grad_norm": 1.4127851724624634,
      "learning_rate": 4.4527896995708155e-05,
      "loss": 0.9892,
      "step": 460
    },
    {
      "epoch": 0.11444921316165951,
      "grad_norm": 1.3109830617904663,
      "learning_rate": 4.42894611349547e-05,
      "loss": 1.0283,
      "step": 480
    },
    {
      "epoch": 0.11921793037672866,
      "grad_norm": 1.723090410232544,
      "learning_rate": 4.405102527420124e-05,
      "loss": 0.9555,
      "step": 500
    },
    {
      "epoch": 0.12398664759179781,
      "grad_norm": 1.6538074016571045,
      "learning_rate": 4.3812589413447784e-05,
      "loss": 1.1051,
      "step": 520
    },
    {
      "epoch": 0.12875536480686695,
      "grad_norm": 1.3932290077209473,
      "learning_rate": 4.357415355269433e-05,
      "loss": 0.9858,
      "step": 540
    },
    {
      "epoch": 0.1335240820219361,
      "grad_norm": 1.1359041929244995,
      "learning_rate": 4.333571769194087e-05,
      "loss": 1.0672,
      "step": 560
    },
    {
      "epoch": 0.13829279923700524,
      "grad_norm": 1.1233707666397095,
      "learning_rate": 4.309728183118741e-05,
      "loss": 0.926,
      "step": 580
    },
    {
      "epoch": 0.1430615164520744,
      "grad_norm": 1.361912727355957,
      "learning_rate": 4.285884597043395e-05,
      "loss": 1.1137,
      "step": 600
    },
    {
      "epoch": 0.14783023366714354,
      "grad_norm": 1.360790491104126,
      "learning_rate": 4.26204101096805e-05,
      "loss": 0.95,
      "step": 620
    },
    {
      "epoch": 0.15259895088221268,
      "grad_norm": 1.3459925651550293,
      "learning_rate": 4.238197424892704e-05,
      "loss": 0.9314,
      "step": 640
    },
    {
      "epoch": 0.15736766809728184,
      "grad_norm": 1.0633941888809204,
      "learning_rate": 4.214353838817358e-05,
      "loss": 0.9383,
      "step": 660
    },
    {
      "epoch": 0.16213638531235097,
      "grad_norm": 1.3596978187561035,
      "learning_rate": 4.190510252742013e-05,
      "loss": 1.0702,
      "step": 680
    },
    {
      "epoch": 0.16690510252742013,
      "grad_norm": 1.5300402641296387,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.9504,
      "step": 700
    },
    {
      "epoch": 0.17167381974248927,
      "grad_norm": 1.6920222043991089,
      "learning_rate": 4.142823080591321e-05,
      "loss": 0.9043,
      "step": 720
    },
    {
      "epoch": 0.1764425369575584,
      "grad_norm": 1.2439645528793335,
      "learning_rate": 4.1189794945159756e-05,
      "loss": 1.0698,
      "step": 740
    },
    {
      "epoch": 0.18121125417262757,
      "grad_norm": 1.3480240106582642,
      "learning_rate": 4.09513590844063e-05,
      "loss": 0.9614,
      "step": 760
    },
    {
      "epoch": 0.1859799713876967,
      "grad_norm": 1.1544166803359985,
      "learning_rate": 4.071292322365284e-05,
      "loss": 0.9454,
      "step": 780
    },
    {
      "epoch": 0.19074868860276586,
      "grad_norm": 1.2736202478408813,
      "learning_rate": 4.0474487362899385e-05,
      "loss": 0.9771,
      "step": 800
    },
    {
      "epoch": 0.195517405817835,
      "grad_norm": 1.2910155057907104,
      "learning_rate": 4.0236051502145924e-05,
      "loss": 0.9237,
      "step": 820
    },
    {
      "epoch": 0.20028612303290416,
      "grad_norm": 1.533652663230896,
      "learning_rate": 3.999761564139247e-05,
      "loss": 0.9816,
      "step": 840
    },
    {
      "epoch": 0.2050548402479733,
      "grad_norm": 1.4791055917739868,
      "learning_rate": 3.975917978063901e-05,
      "loss": 0.8763,
      "step": 860
    },
    {
      "epoch": 0.20982355746304243,
      "grad_norm": 1.978102684020996,
      "learning_rate": 3.952074391988555e-05,
      "loss": 0.903,
      "step": 880
    },
    {
      "epoch": 0.2145922746781116,
      "grad_norm": 1.4871009588241577,
      "learning_rate": 3.928230805913209e-05,
      "loss": 1.1103,
      "step": 900
    },
    {
      "epoch": 0.21936099189318073,
      "grad_norm": 1.2535284757614136,
      "learning_rate": 3.904387219837864e-05,
      "loss": 0.9165,
      "step": 920
    },
    {
      "epoch": 0.2241297091082499,
      "grad_norm": 1.5051355361938477,
      "learning_rate": 3.8805436337625176e-05,
      "loss": 0.9185,
      "step": 940
    },
    {
      "epoch": 0.22889842632331903,
      "grad_norm": 1.548619270324707,
      "learning_rate": 3.856700047687172e-05,
      "loss": 0.8485,
      "step": 960
    },
    {
      "epoch": 0.23366714353838816,
      "grad_norm": 1.507490873336792,
      "learning_rate": 3.832856461611827e-05,
      "loss": 0.9613,
      "step": 980
    },
    {
      "epoch": 0.23843586075345732,
      "grad_norm": 1.3467366695404053,
      "learning_rate": 3.8090128755364805e-05,
      "loss": 1.0047,
      "step": 1000
    },
    {
      "epoch": 0.24320457796852646,
      "grad_norm": 1.2666115760803223,
      "learning_rate": 3.785169289461135e-05,
      "loss": 1.0646,
      "step": 1020
    },
    {
      "epoch": 0.24797329518359562,
      "grad_norm": 1.3762949705123901,
      "learning_rate": 3.7613257033857896e-05,
      "loss": 0.8526,
      "step": 1040
    },
    {
      "epoch": 0.2527420123986648,
      "grad_norm": 1.2429423332214355,
      "learning_rate": 3.7374821173104435e-05,
      "loss": 0.985,
      "step": 1060
    },
    {
      "epoch": 0.2575107296137339,
      "grad_norm": 1.4324678182601929,
      "learning_rate": 3.713638531235098e-05,
      "loss": 0.9553,
      "step": 1080
    },
    {
      "epoch": 0.26227944682880305,
      "grad_norm": 1.471815586090088,
      "learning_rate": 3.6897949451597525e-05,
      "loss": 1.0411,
      "step": 1100
    },
    {
      "epoch": 0.2670481640438722,
      "grad_norm": 1.0281531810760498,
      "learning_rate": 3.6659513590844064e-05,
      "loss": 0.8465,
      "step": 1120
    },
    {
      "epoch": 0.2718168812589413,
      "grad_norm": 1.2049260139465332,
      "learning_rate": 3.642107773009061e-05,
      "loss": 0.9576,
      "step": 1140
    },
    {
      "epoch": 0.2765855984740105,
      "grad_norm": 1.4600142240524292,
      "learning_rate": 3.6182641869337155e-05,
      "loss": 1.0122,
      "step": 1160
    },
    {
      "epoch": 0.28135431568907965,
      "grad_norm": 1.3119523525238037,
      "learning_rate": 3.594420600858369e-05,
      "loss": 0.8998,
      "step": 1180
    },
    {
      "epoch": 0.2861230329041488,
      "grad_norm": 1.2690001726150513,
      "learning_rate": 3.570577014783024e-05,
      "loss": 0.8846,
      "step": 1200
    },
    {
      "epoch": 0.2908917501192179,
      "grad_norm": 1.245191216468811,
      "learning_rate": 3.546733428707678e-05,
      "loss": 1.0742,
      "step": 1220
    },
    {
      "epoch": 0.2956604673342871,
      "grad_norm": 1.3532249927520752,
      "learning_rate": 3.522889842632332e-05,
      "loss": 1.006,
      "step": 1240
    },
    {
      "epoch": 0.30042918454935624,
      "grad_norm": 1.4516853094100952,
      "learning_rate": 3.499046256556987e-05,
      "loss": 0.9308,
      "step": 1260
    },
    {
      "epoch": 0.30519790176442535,
      "grad_norm": 1.3931989669799805,
      "learning_rate": 3.4752026704816407e-05,
      "loss": 1.0013,
      "step": 1280
    },
    {
      "epoch": 0.3099666189794945,
      "grad_norm": 1.2548660039901733,
      "learning_rate": 3.4513590844062945e-05,
      "loss": 0.974,
      "step": 1300
    },
    {
      "epoch": 0.3147353361945637,
      "grad_norm": 1.326783537864685,
      "learning_rate": 3.427515498330949e-05,
      "loss": 0.9108,
      "step": 1320
    },
    {
      "epoch": 0.3195040534096328,
      "grad_norm": 1.2453583478927612,
      "learning_rate": 3.403671912255603e-05,
      "loss": 0.9407,
      "step": 1340
    },
    {
      "epoch": 0.32427277062470194,
      "grad_norm": 1.3844345808029175,
      "learning_rate": 3.3798283261802575e-05,
      "loss": 0.926,
      "step": 1360
    },
    {
      "epoch": 0.3290414878397711,
      "grad_norm": 1.1765111684799194,
      "learning_rate": 3.355984740104912e-05,
      "loss": 0.9649,
      "step": 1380
    },
    {
      "epoch": 0.33381020505484027,
      "grad_norm": 1.236985683441162,
      "learning_rate": 3.332141154029566e-05,
      "loss": 0.938,
      "step": 1400
    },
    {
      "epoch": 0.3385789222699094,
      "grad_norm": 1.1481375694274902,
      "learning_rate": 3.3082975679542204e-05,
      "loss": 0.8841,
      "step": 1420
    },
    {
      "epoch": 0.34334763948497854,
      "grad_norm": 1.341557264328003,
      "learning_rate": 3.284453981878875e-05,
      "loss": 1.0086,
      "step": 1440
    },
    {
      "epoch": 0.3481163567000477,
      "grad_norm": 1.283595323562622,
      "learning_rate": 3.260610395803529e-05,
      "loss": 0.9107,
      "step": 1460
    },
    {
      "epoch": 0.3528850739151168,
      "grad_norm": 0.9025381207466125,
      "learning_rate": 3.236766809728183e-05,
      "loss": 1.0508,
      "step": 1480
    },
    {
      "epoch": 0.35765379113018597,
      "grad_norm": 1.6411161422729492,
      "learning_rate": 3.212923223652837e-05,
      "loss": 0.9608,
      "step": 1500
    },
    {
      "epoch": 0.36242250834525513,
      "grad_norm": 1.5789679288864136,
      "learning_rate": 3.189079637577492e-05,
      "loss": 1.0312,
      "step": 1520
    },
    {
      "epoch": 0.3671912255603243,
      "grad_norm": 1.253656029701233,
      "learning_rate": 3.165236051502146e-05,
      "loss": 1.1127,
      "step": 1540
    },
    {
      "epoch": 0.3719599427753934,
      "grad_norm": 1.073884129524231,
      "learning_rate": 3.1413924654268e-05,
      "loss": 0.9464,
      "step": 1560
    },
    {
      "epoch": 0.37672865999046257,
      "grad_norm": 1.5194674730300903,
      "learning_rate": 3.1175488793514547e-05,
      "loss": 0.9252,
      "step": 1580
    },
    {
      "epoch": 0.38149737720553173,
      "grad_norm": 1.4375606775283813,
      "learning_rate": 3.093705293276109e-05,
      "loss": 1.0064,
      "step": 1600
    },
    {
      "epoch": 0.38626609442060084,
      "grad_norm": 1.183712363243103,
      "learning_rate": 3.069861707200763e-05,
      "loss": 0.942,
      "step": 1620
    },
    {
      "epoch": 0.39103481163567,
      "grad_norm": 1.2413294315338135,
      "learning_rate": 3.0460181211254172e-05,
      "loss": 0.9697,
      "step": 1640
    },
    {
      "epoch": 0.39580352885073916,
      "grad_norm": 1.2456539869308472,
      "learning_rate": 3.0221745350500718e-05,
      "loss": 0.9258,
      "step": 1660
    },
    {
      "epoch": 0.4005722460658083,
      "grad_norm": 1.1191842555999756,
      "learning_rate": 2.9983309489747256e-05,
      "loss": 0.9189,
      "step": 1680
    },
    {
      "epoch": 0.40534096328087743,
      "grad_norm": 1.1491806507110596,
      "learning_rate": 2.9744873628993802e-05,
      "loss": 0.9212,
      "step": 1700
    },
    {
      "epoch": 0.4101096804959466,
      "grad_norm": 1.8228310346603394,
      "learning_rate": 2.9506437768240347e-05,
      "loss": 0.9356,
      "step": 1720
    },
    {
      "epoch": 0.41487839771101576,
      "grad_norm": 1.128524661064148,
      "learning_rate": 2.9268001907486886e-05,
      "loss": 1.1798,
      "step": 1740
    },
    {
      "epoch": 0.41964711492608486,
      "grad_norm": 1.48799729347229,
      "learning_rate": 2.902956604673343e-05,
      "loss": 0.9121,
      "step": 1760
    },
    {
      "epoch": 0.424415832141154,
      "grad_norm": 1.0991872549057007,
      "learning_rate": 2.8791130185979977e-05,
      "loss": 0.8171,
      "step": 1780
    },
    {
      "epoch": 0.4291845493562232,
      "grad_norm": 1.1420670747756958,
      "learning_rate": 2.8552694325226515e-05,
      "loss": 0.9794,
      "step": 1800
    },
    {
      "epoch": 0.4339532665712923,
      "grad_norm": 1.6459310054779053,
      "learning_rate": 2.831425846447306e-05,
      "loss": 1.0969,
      "step": 1820
    },
    {
      "epoch": 0.43872198378636146,
      "grad_norm": 1.1337288618087769,
      "learning_rate": 2.80758226037196e-05,
      "loss": 0.8845,
      "step": 1840
    },
    {
      "epoch": 0.4434907010014306,
      "grad_norm": 1.5769482851028442,
      "learning_rate": 2.7837386742966144e-05,
      "loss": 0.8542,
      "step": 1860
    },
    {
      "epoch": 0.4482594182164998,
      "grad_norm": 1.1669659614562988,
      "learning_rate": 2.7598950882212686e-05,
      "loss": 0.953,
      "step": 1880
    },
    {
      "epoch": 0.4530281354315689,
      "grad_norm": 1.6590200662612915,
      "learning_rate": 2.7360515021459225e-05,
      "loss": 0.9009,
      "step": 1900
    },
    {
      "epoch": 0.45779685264663805,
      "grad_norm": 1.5319013595581055,
      "learning_rate": 2.712207916070577e-05,
      "loss": 0.9962,
      "step": 1920
    },
    {
      "epoch": 0.4625655698617072,
      "grad_norm": 1.3673611879348755,
      "learning_rate": 2.6883643299952316e-05,
      "loss": 0.9236,
      "step": 1940
    },
    {
      "epoch": 0.4673342870767763,
      "grad_norm": 1.1691057682037354,
      "learning_rate": 2.6645207439198854e-05,
      "loss": 0.9263,
      "step": 1960
    },
    {
      "epoch": 0.4721030042918455,
      "grad_norm": 1.3918895721435547,
      "learning_rate": 2.64067715784454e-05,
      "loss": 1.0202,
      "step": 1980
    },
    {
      "epoch": 0.47687172150691465,
      "grad_norm": 1.4609272480010986,
      "learning_rate": 2.6168335717691945e-05,
      "loss": 1.0024,
      "step": 2000
    },
    {
      "epoch": 0.4816404387219838,
      "grad_norm": 1.2983264923095703,
      "learning_rate": 2.5929899856938484e-05,
      "loss": 0.927,
      "step": 2020
    },
    {
      "epoch": 0.4864091559370529,
      "grad_norm": 1.3916524648666382,
      "learning_rate": 2.569146399618503e-05,
      "loss": 0.8901,
      "step": 2040
    },
    {
      "epoch": 0.4911778731521221,
      "grad_norm": 1.1478402614593506,
      "learning_rate": 2.545302813543157e-05,
      "loss": 0.9864,
      "step": 2060
    },
    {
      "epoch": 0.49594659036719124,
      "grad_norm": 2.109710216522217,
      "learning_rate": 2.5214592274678113e-05,
      "loss": 0.9753,
      "step": 2080
    },
    {
      "epoch": 0.5007153075822603,
      "grad_norm": 1.4530889987945557,
      "learning_rate": 2.4976156413924655e-05,
      "loss": 0.9085,
      "step": 2100
    },
    {
      "epoch": 0.5054840247973296,
      "grad_norm": 1.4124199151992798,
      "learning_rate": 2.4737720553171197e-05,
      "loss": 0.899,
      "step": 2120
    },
    {
      "epoch": 0.5102527420123987,
      "grad_norm": 1.8190959692001343,
      "learning_rate": 2.449928469241774e-05,
      "loss": 0.8911,
      "step": 2140
    },
    {
      "epoch": 0.5150214592274678,
      "grad_norm": 1.3624554872512817,
      "learning_rate": 2.4260848831664284e-05,
      "loss": 0.9199,
      "step": 2160
    },
    {
      "epoch": 0.519790176442537,
      "grad_norm": 1.328049659729004,
      "learning_rate": 2.4022412970910826e-05,
      "loss": 0.9239,
      "step": 2180
    },
    {
      "epoch": 0.5245588936576061,
      "grad_norm": 1.330856204032898,
      "learning_rate": 2.378397711015737e-05,
      "loss": 0.9489,
      "step": 2200
    },
    {
      "epoch": 0.5293276108726752,
      "grad_norm": 1.1876801252365112,
      "learning_rate": 2.354554124940391e-05,
      "loss": 0.9618,
      "step": 2220
    },
    {
      "epoch": 0.5340963280877444,
      "grad_norm": 1.2491744756698608,
      "learning_rate": 2.3307105388650456e-05,
      "loss": 0.948,
      "step": 2240
    },
    {
      "epoch": 0.5388650453028135,
      "grad_norm": 1.3610022068023682,
      "learning_rate": 2.3068669527896998e-05,
      "loss": 0.9109,
      "step": 2260
    },
    {
      "epoch": 0.5436337625178826,
      "grad_norm": 1.5486723184585571,
      "learning_rate": 2.283023366714354e-05,
      "loss": 0.9453,
      "step": 2280
    },
    {
      "epoch": 0.5484024797329519,
      "grad_norm": 1.318285346031189,
      "learning_rate": 2.259179780639008e-05,
      "loss": 0.97,
      "step": 2300
    },
    {
      "epoch": 0.553171196948021,
      "grad_norm": 1.1549203395843506,
      "learning_rate": 2.2353361945636624e-05,
      "loss": 0.8917,
      "step": 2320
    },
    {
      "epoch": 0.5579399141630901,
      "grad_norm": 1.607796549797058,
      "learning_rate": 2.2114926084883166e-05,
      "loss": 0.9854,
      "step": 2340
    },
    {
      "epoch": 0.5627086313781593,
      "grad_norm": 2.0379793643951416,
      "learning_rate": 2.187649022412971e-05,
      "loss": 0.9843,
      "step": 2360
    },
    {
      "epoch": 0.5674773485932284,
      "grad_norm": 1.3971434831619263,
      "learning_rate": 2.1638054363376253e-05,
      "loss": 0.9165,
      "step": 2380
    },
    {
      "epoch": 0.5722460658082976,
      "grad_norm": 1.4532968997955322,
      "learning_rate": 2.1399618502622795e-05,
      "loss": 1.0151,
      "step": 2400
    },
    {
      "epoch": 0.5770147830233667,
      "grad_norm": 1.3726704120635986,
      "learning_rate": 2.1161182641869337e-05,
      "loss": 0.9836,
      "step": 2420
    },
    {
      "epoch": 0.5817835002384358,
      "grad_norm": 1.2176868915557861,
      "learning_rate": 2.0922746781115882e-05,
      "loss": 1.0015,
      "step": 2440
    },
    {
      "epoch": 0.586552217453505,
      "grad_norm": 1.8452644348144531,
      "learning_rate": 2.0684310920362424e-05,
      "loss": 0.9865,
      "step": 2460
    },
    {
      "epoch": 0.5913209346685742,
      "grad_norm": 1.4712800979614258,
      "learning_rate": 2.0445875059608966e-05,
      "loss": 0.9445,
      "step": 2480
    },
    {
      "epoch": 0.5960896518836433,
      "grad_norm": 1.3475956916809082,
      "learning_rate": 2.0207439198855512e-05,
      "loss": 0.8874,
      "step": 2500
    },
    {
      "epoch": 0.6008583690987125,
      "grad_norm": 1.4040330648422241,
      "learning_rate": 1.996900333810205e-05,
      "loss": 0.964,
      "step": 2520
    },
    {
      "epoch": 0.6056270863137816,
      "grad_norm": 1.5036418437957764,
      "learning_rate": 1.9730567477348592e-05,
      "loss": 0.8817,
      "step": 2540
    },
    {
      "epoch": 0.6103958035288507,
      "grad_norm": 1.7951703071594238,
      "learning_rate": 1.9492131616595134e-05,
      "loss": 0.9693,
      "step": 2560
    },
    {
      "epoch": 0.6151645207439199,
      "grad_norm": 4.627903938293457,
      "learning_rate": 1.925369575584168e-05,
      "loss": 1.0408,
      "step": 2580
    },
    {
      "epoch": 0.619933237958989,
      "grad_norm": 1.4453176259994507,
      "learning_rate": 1.901525989508822e-05,
      "loss": 0.8654,
      "step": 2600
    },
    {
      "epoch": 0.6247019551740581,
      "grad_norm": 1.4390984773635864,
      "learning_rate": 1.8776824034334764e-05,
      "loss": 0.8789,
      "step": 2620
    },
    {
      "epoch": 0.6294706723891274,
      "grad_norm": 1.3638828992843628,
      "learning_rate": 1.853838817358131e-05,
      "loss": 0.8611,
      "step": 2640
    },
    {
      "epoch": 0.6342393896041965,
      "grad_norm": 1.1782294511795044,
      "learning_rate": 1.829995231282785e-05,
      "loss": 0.8635,
      "step": 2660
    },
    {
      "epoch": 0.6390081068192656,
      "grad_norm": 1.5472058057785034,
      "learning_rate": 1.8061516452074393e-05,
      "loss": 0.8448,
      "step": 2680
    },
    {
      "epoch": 0.6437768240343348,
      "grad_norm": 1.4039324522018433,
      "learning_rate": 1.7823080591320935e-05,
      "loss": 0.813,
      "step": 2700
    },
    {
      "epoch": 0.6485455412494039,
      "grad_norm": 1.4133095741271973,
      "learning_rate": 1.758464473056748e-05,
      "loss": 0.8281,
      "step": 2720
    },
    {
      "epoch": 0.6533142584644731,
      "grad_norm": 1.556391716003418,
      "learning_rate": 1.7346208869814022e-05,
      "loss": 0.9034,
      "step": 2740
    },
    {
      "epoch": 0.6580829756795422,
      "grad_norm": 1.101553201675415,
      "learning_rate": 1.710777300906056e-05,
      "loss": 0.9177,
      "step": 2760
    },
    {
      "epoch": 0.6628516928946113,
      "grad_norm": 1.2489521503448486,
      "learning_rate": 1.6869337148307106e-05,
      "loss": 1.0057,
      "step": 2780
    },
    {
      "epoch": 0.6676204101096805,
      "grad_norm": 1.2102469205856323,
      "learning_rate": 1.6630901287553648e-05,
      "loss": 0.9344,
      "step": 2800
    },
    {
      "epoch": 0.6723891273247496,
      "grad_norm": 1.5468286275863647,
      "learning_rate": 1.639246542680019e-05,
      "loss": 0.8997,
      "step": 2820
    },
    {
      "epoch": 0.6771578445398188,
      "grad_norm": 1.3351550102233887,
      "learning_rate": 1.6154029566046736e-05,
      "loss": 0.9234,
      "step": 2840
    },
    {
      "epoch": 0.681926561754888,
      "grad_norm": 1.2363255023956299,
      "learning_rate": 1.5915593705293278e-05,
      "loss": 0.9053,
      "step": 2860
    },
    {
      "epoch": 0.6866952789699571,
      "grad_norm": 1.2730854749679565,
      "learning_rate": 1.567715784453982e-05,
      "loss": 0.9157,
      "step": 2880
    },
    {
      "epoch": 0.6914639961850262,
      "grad_norm": 1.4184443950653076,
      "learning_rate": 1.543872198378636e-05,
      "loss": 0.8519,
      "step": 2900
    },
    {
      "epoch": 0.6962327134000954,
      "grad_norm": 1.5118052959442139,
      "learning_rate": 1.5200286123032905e-05,
      "loss": 0.8494,
      "step": 2920
    },
    {
      "epoch": 0.7010014306151645,
      "grad_norm": 1.5344632863998413,
      "learning_rate": 1.4961850262279447e-05,
      "loss": 0.8749,
      "step": 2940
    },
    {
      "epoch": 0.7057701478302336,
      "grad_norm": 1.455815315246582,
      "learning_rate": 1.472341440152599e-05,
      "loss": 0.8858,
      "step": 2960
    },
    {
      "epoch": 0.7105388650453028,
      "grad_norm": 1.2186567783355713,
      "learning_rate": 1.4484978540772535e-05,
      "loss": 0.9809,
      "step": 2980
    },
    {
      "epoch": 0.7153075822603719,
      "grad_norm": 2.130155324935913,
      "learning_rate": 1.4246542680019077e-05,
      "loss": 1.0534,
      "step": 3000
    },
    {
      "epoch": 0.7200762994754412,
      "grad_norm": 1.306131362915039,
      "learning_rate": 1.4008106819265619e-05,
      "loss": 0.852,
      "step": 3020
    },
    {
      "epoch": 0.7248450166905103,
      "grad_norm": 1.1204150915145874,
      "learning_rate": 1.376967095851216e-05,
      "loss": 0.8773,
      "step": 3040
    },
    {
      "epoch": 0.7296137339055794,
      "grad_norm": 1.2963199615478516,
      "learning_rate": 1.3531235097758704e-05,
      "loss": 0.9295,
      "step": 3060
    },
    {
      "epoch": 0.7343824511206486,
      "grad_norm": 1.8810690641403198,
      "learning_rate": 1.3292799237005246e-05,
      "loss": 0.9398,
      "step": 3080
    },
    {
      "epoch": 0.7391511683357177,
      "grad_norm": 1.7814133167266846,
      "learning_rate": 1.3054363376251788e-05,
      "loss": 0.8411,
      "step": 3100
    },
    {
      "epoch": 0.7439198855507868,
      "grad_norm": 1.4396029710769653,
      "learning_rate": 1.2815927515498332e-05,
      "loss": 0.9099,
      "step": 3120
    },
    {
      "epoch": 0.748688602765856,
      "grad_norm": 1.7095965147018433,
      "learning_rate": 1.2577491654744874e-05,
      "loss": 0.8794,
      "step": 3140
    },
    {
      "epoch": 0.7534573199809251,
      "grad_norm": 1.25852632522583,
      "learning_rate": 1.2339055793991416e-05,
      "loss": 0.9703,
      "step": 3160
    },
    {
      "epoch": 0.7582260371959942,
      "grad_norm": 1.1802326440811157,
      "learning_rate": 1.210061993323796e-05,
      "loss": 0.8109,
      "step": 3180
    },
    {
      "epoch": 0.7629947544110635,
      "grad_norm": 1.3079819679260254,
      "learning_rate": 1.1862184072484503e-05,
      "loss": 0.859,
      "step": 3200
    },
    {
      "epoch": 0.7677634716261326,
      "grad_norm": 1.3016670942306519,
      "learning_rate": 1.1623748211731045e-05,
      "loss": 1.0913,
      "step": 3220
    },
    {
      "epoch": 0.7725321888412017,
      "grad_norm": 1.0512570142745972,
      "learning_rate": 1.1385312350977589e-05,
      "loss": 0.8844,
      "step": 3240
    },
    {
      "epoch": 0.7773009060562709,
      "grad_norm": 1.4565324783325195,
      "learning_rate": 1.114687649022413e-05,
      "loss": 0.9411,
      "step": 3260
    },
    {
      "epoch": 0.78206962327134,
      "grad_norm": 1.4627190828323364,
      "learning_rate": 1.0908440629470673e-05,
      "loss": 0.9982,
      "step": 3280
    },
    {
      "epoch": 0.7868383404864091,
      "grad_norm": 1.4300286769866943,
      "learning_rate": 1.0670004768717215e-05,
      "loss": 0.8864,
      "step": 3300
    },
    {
      "epoch": 0.7916070577014783,
      "grad_norm": 1.2167309522628784,
      "learning_rate": 1.0431568907963759e-05,
      "loss": 0.9059,
      "step": 3320
    },
    {
      "epoch": 0.7963757749165474,
      "grad_norm": 1.6252484321594238,
      "learning_rate": 1.0193133047210302e-05,
      "loss": 1.0322,
      "step": 3340
    },
    {
      "epoch": 0.8011444921316166,
      "grad_norm": 1.4298536777496338,
      "learning_rate": 9.954697186456844e-06,
      "loss": 1.0541,
      "step": 3360
    },
    {
      "epoch": 0.8059132093466858,
      "grad_norm": 1.14928138256073,
      "learning_rate": 9.716261325703386e-06,
      "loss": 0.8739,
      "step": 3380
    },
    {
      "epoch": 0.8106819265617549,
      "grad_norm": 1.4548890590667725,
      "learning_rate": 9.477825464949928e-06,
      "loss": 0.8066,
      "step": 3400
    },
    {
      "epoch": 0.8154506437768241,
      "grad_norm": 1.2869699001312256,
      "learning_rate": 9.239389604196472e-06,
      "loss": 0.8906,
      "step": 3420
    },
    {
      "epoch": 0.8202193609918932,
      "grad_norm": 1.1042087078094482,
      "learning_rate": 9.000953743443015e-06,
      "loss": 0.8003,
      "step": 3440
    },
    {
      "epoch": 0.8249880782069623,
      "grad_norm": 2.0802507400512695,
      "learning_rate": 8.762517882689557e-06,
      "loss": 0.9532,
      "step": 3460
    },
    {
      "epoch": 0.8297567954220315,
      "grad_norm": 1.355757236480713,
      "learning_rate": 8.5240820219361e-06,
      "loss": 0.9947,
      "step": 3480
    },
    {
      "epoch": 0.8345255126371006,
      "grad_norm": 1.725433111190796,
      "learning_rate": 8.285646161182641e-06,
      "loss": 0.879,
      "step": 3500
    },
    {
      "epoch": 0.8392942298521697,
      "grad_norm": 1.621289610862732,
      "learning_rate": 8.047210300429185e-06,
      "loss": 0.9845,
      "step": 3520
    },
    {
      "epoch": 0.844062947067239,
      "grad_norm": 1.5374184846878052,
      "learning_rate": 7.808774439675727e-06,
      "loss": 0.9596,
      "step": 3540
    },
    {
      "epoch": 0.848831664282308,
      "grad_norm": 1.5177509784698486,
      "learning_rate": 7.57033857892227e-06,
      "loss": 0.8766,
      "step": 3560
    },
    {
      "epoch": 0.8536003814973772,
      "grad_norm": 1.2995351552963257,
      "learning_rate": 7.331902718168814e-06,
      "loss": 0.9605,
      "step": 3580
    },
    {
      "epoch": 0.8583690987124464,
      "grad_norm": 1.5667448043823242,
      "learning_rate": 7.093466857415356e-06,
      "loss": 0.9551,
      "step": 3600
    },
    {
      "epoch": 0.8631378159275155,
      "grad_norm": 1.2681479454040527,
      "learning_rate": 6.8550309966618984e-06,
      "loss": 0.9419,
      "step": 3620
    },
    {
      "epoch": 0.8679065331425846,
      "grad_norm": 1.2720385789871216,
      "learning_rate": 6.6165951359084404e-06,
      "loss": 0.8572,
      "step": 3640
    },
    {
      "epoch": 0.8726752503576538,
      "grad_norm": 1.2967969179153442,
      "learning_rate": 6.378159275154984e-06,
      "loss": 0.943,
      "step": 3660
    },
    {
      "epoch": 0.8774439675727229,
      "grad_norm": 1.3998456001281738,
      "learning_rate": 6.139723414401526e-06,
      "loss": 0.9859,
      "step": 3680
    },
    {
      "epoch": 0.8822126847877921,
      "grad_norm": 1.276596188545227,
      "learning_rate": 5.901287553648069e-06,
      "loss": 0.8261,
      "step": 3700
    },
    {
      "epoch": 0.8869814020028612,
      "grad_norm": 1.5894662141799927,
      "learning_rate": 5.662851692894612e-06,
      "loss": 0.9945,
      "step": 3720
    },
    {
      "epoch": 0.8917501192179303,
      "grad_norm": 1.8278568983078003,
      "learning_rate": 5.424415832141154e-06,
      "loss": 0.8027,
      "step": 3740
    },
    {
      "epoch": 0.8965188364329996,
      "grad_norm": 1.5279995203018188,
      "learning_rate": 5.185979971387697e-06,
      "loss": 0.8526,
      "step": 3760
    },
    {
      "epoch": 0.9012875536480687,
      "grad_norm": 1.4728868007659912,
      "learning_rate": 4.947544110634239e-06,
      "loss": 0.946,
      "step": 3780
    },
    {
      "epoch": 0.9060562708631378,
      "grad_norm": 1.6694302558898926,
      "learning_rate": 4.709108249880782e-06,
      "loss": 0.8975,
      "step": 3800
    },
    {
      "epoch": 0.910824988078207,
      "grad_norm": 1.5636152029037476,
      "learning_rate": 4.470672389127325e-06,
      "loss": 0.8963,
      "step": 3820
    },
    {
      "epoch": 0.9155937052932761,
      "grad_norm": 1.4841067790985107,
      "learning_rate": 4.232236528373867e-06,
      "loss": 0.8309,
      "step": 3840
    },
    {
      "epoch": 0.9203624225083452,
      "grad_norm": 1.2376110553741455,
      "learning_rate": 3.99380066762041e-06,
      "loss": 0.9143,
      "step": 3860
    },
    {
      "epoch": 0.9251311397234144,
      "grad_norm": 1.1185015439987183,
      "learning_rate": 3.755364806866953e-06,
      "loss": 0.9638,
      "step": 3880
    },
    {
      "epoch": 0.9298998569384835,
      "grad_norm": 1.5274699926376343,
      "learning_rate": 3.516928946113496e-06,
      "loss": 0.9749,
      "step": 3900
    },
    {
      "epoch": 0.9346685741535526,
      "grad_norm": 1.3688127994537354,
      "learning_rate": 3.2784930853600384e-06,
      "loss": 0.9649,
      "step": 3920
    },
    {
      "epoch": 0.9394372913686219,
      "grad_norm": 1.4450637102127075,
      "learning_rate": 3.040057224606581e-06,
      "loss": 0.9466,
      "step": 3940
    },
    {
      "epoch": 0.944206008583691,
      "grad_norm": 1.4875627756118774,
      "learning_rate": 2.8016213638531236e-06,
      "loss": 1.0572,
      "step": 3960
    },
    {
      "epoch": 0.9489747257987602,
      "grad_norm": 1.3681576251983643,
      "learning_rate": 2.5631855030996665e-06,
      "loss": 0.8909,
      "step": 3980
    },
    {
      "epoch": 0.9537434430138293,
      "grad_norm": 1.5435686111450195,
      "learning_rate": 2.324749642346209e-06,
      "loss": 0.9451,
      "step": 4000
    },
    {
      "epoch": 0.9585121602288984,
      "grad_norm": 1.1848077774047852,
      "learning_rate": 2.0863137815927517e-06,
      "loss": 0.9694,
      "step": 4020
    },
    {
      "epoch": 0.9632808774439676,
      "grad_norm": 1.2967153787612915,
      "learning_rate": 1.8478779208392943e-06,
      "loss": 0.9518,
      "step": 4040
    },
    {
      "epoch": 0.9680495946590367,
      "grad_norm": 1.6107815504074097,
      "learning_rate": 1.609442060085837e-06,
      "loss": 0.9864,
      "step": 4060
    },
    {
      "epoch": 0.9728183118741058,
      "grad_norm": 1.5682792663574219,
      "learning_rate": 1.3710061993323796e-06,
      "loss": 0.9233,
      "step": 4080
    },
    {
      "epoch": 0.977587029089175,
      "grad_norm": 1.285284399986267,
      "learning_rate": 1.1325703385789224e-06,
      "loss": 0.857,
      "step": 4100
    },
    {
      "epoch": 0.9823557463042442,
      "grad_norm": 1.4252442121505737,
      "learning_rate": 8.941344778254651e-07,
      "loss": 0.9886,
      "step": 4120
    },
    {
      "epoch": 0.9871244635193133,
      "grad_norm": 1.1579716205596924,
      "learning_rate": 6.556986170720076e-07,
      "loss": 0.9314,
      "step": 4140
    },
    {
      "epoch": 0.9918931807343825,
      "grad_norm": 1.356528878211975,
      "learning_rate": 4.1726275631855036e-07,
      "loss": 0.8413,
      "step": 4160
    },
    {
      "epoch": 0.9966618979494516,
      "grad_norm": 1.6150778532028198,
      "learning_rate": 1.78826895565093e-07,
      "loss": 0.9564,
      "step": 4180
    }
  ],
  "logging_steps": 20,
  "max_steps": 4194,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.668053238533325e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
